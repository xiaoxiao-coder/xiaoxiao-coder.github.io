<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>校园网自动连接</title>
    <url>/2022/03/13/%E6%A0%A1%E5%9B%AD%E7%BD%91%E8%87%AA%E5%8A%A8%E8%BF%9E%E6%8E%A5/</url>
    <content><![CDATA[<p>算校园网每天凌晨三点会自动断掉，疫情封校就会导致很麻烦，所以就想着写个脚本自动进行连网。</p>
<a id="more"></a>
<h3 id="Ubuntu"><a href="#Ubuntu" class="headerlink" title="Ubuntu"></a>Ubuntu</h3><p>将如下py文件放到linux路径a下,起名为lll.py（示例，名称随便），然后username是学号，passoword是密码，ip代表你ubuntu主机的ip。然后在a路径下开启一个terminal，python lll.py运行即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">URL=<span class="string">&quot;https://10.108.255.249/include/auth_action.php&quot;</span> <span class="comment"># not change</span></span><br><span class="line">username= <span class="string">&quot;21210720&quot;</span> <span class="comment">#change: there please use your student number</span></span><br><span class="line">password=<span class="string">&quot;123456&quot;</span> <span class="comment">#change: use your password</span></span><br><span class="line">ip= <span class="string">&quot;10.176.54.102&quot;</span> <span class="comment">#change: use your ubuntu ip</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">    os.system(<span class="string">fr&quot;curl $URL --insecure --data \&quot;action=login&amp;username=<span class="subst">&#123;username&#125;</span>&amp;password=<span class="subst">&#123;password&#125;</span>&amp;ac_id=1&amp;user_ip=<span class="subst">&#123;ip&#125;</span>&amp;nas_ip=&amp;user_mac=&amp;save_me=1&amp;ajax=1\&quot; &gt; /dev/null 2&gt;&amp;1&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><p>首先浏览器输入如下ip，进入联网的网站。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">10.108.255.249</span><br></pre></td></tr></table></figure>
<p>然后左下角有个windows的图标，点击下载windows客户端，然后安装，第一次需要输入个人学号和密码登录，记得点上自动登录等等的四个选项。同时点击exe，运行exe的时候可能会跳出一个界面，需要手动点确认后exe才能真的执行，将其取消，否则脚本调用的时候会卡在那一步。</p>
<p>然后输入创建如下py文件，命名为lll.py，Desk_path代表你的桌面路径，即按装的windows客户端Srun3K.exe所在的路径。install_path是安装路径，可以运行Srun3K.exe,然后在任务管理器中右键打开文件位置看，install_name就是在安装路径install_path下的真实名称。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;please run py in cmd by administrator&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line">Desk_path = <span class="string">r&quot;C:\Users\21311\Desktop&quot;</span> <span class="comment">#your desk path</span></span><br><span class="line">install_path = <span class="string">r&quot;C:\Users\21311\AppData\Local&quot;</span> <span class="comment">#your installed path</span></span><br><span class="line">install_name = <span class="string">&quot;9194922e84f9cbf8db275b28a386f84b.exe&quot;</span> <span class="comment">#the name in your installed path</span></span><br><span class="line">os.system(<span class="string">rf&quot;cd <span class="subst">&#123;install_path&#125;</span> &amp; taskkill /im <span class="subst">&#123;install_name&#125;</span> /f&quot;</span>)</span><br><span class="line"><span class="keyword">while</span> <span class="number">1</span> :</span><br><span class="line">    t = time.localtime()</span><br><span class="line">    <span class="keyword">if</span> (t.tm_hour == <span class="number">3</span> <span class="keyword">and</span> t.tm_min == <span class="number">0</span>) :</span><br><span class="line">        os.system(<span class="string">rf&quot;cd C:\Users\21311\AppData\Local &amp; taskkill /im <span class="subst">&#123;install_name&#125;</span> /f&quot;</span>)</span><br><span class="line">        os.system(<span class="string">fr&quot;cd <span class="subst">&#123;Desk_path&#125;</span> &amp; SRun3K.exe&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">70</span>)</span><br></pre></td></tr></table></figure>
<p>注意，windows下以管理权权限运行cmd，然后python lll.py,不是administrator权限的话会导致kill程序权限不足。</p>
<p>如有问题，可以咨询~</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <tags>
        <tag>校园</tag>
      </tags>
  </entry>
  <entry>
    <title>算法精粹(一)</title>
    <url>/2021/12/20/%E7%AE%97%E6%B3%95%E7%B2%BE%E7%B2%B9-%E4%B8%80/</url>
    <content><![CDATA[<p>算法精粹第一章的几个小问题，挺有意思的，写一写纪录一下。(迭代器、生成器、装饰器、元组解包、字符串直接对比、sum原理)</p>
<a id="more"></a>
<h2 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h2><p>其数列是0,1,1,2,3,5,8,13….模样，除了前两个数字，后面的数字都是其前两个之和，即a(n)=a(n-1)+a(n-2).此种方程式显然可以用递归求解，那么如下看最初始的递归实现.</p>
<h3 id="初始版递归求解："><a href="#初始版递归求解：" class="headerlink" title="初始版递归求解："></a>初始版递归求解：</h3><p>如下，递归会不断的调用自身，直至达到基线条件(a0和a1)，但是递归的算法复杂度特别高，调用自身的次数近似和2^n呈正相关，输入0/1的时候只调用1次，2的时候调用1+2,3的时候调用1+2+2次，4的时候9次….用树状图可以不断的通过节点来表示，可以看出复杂度异常的高。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci</span>(<span class="params">n:<span class="built_in">int</span></span>)-&gt;int:</span></span><br><span class="line">    <span class="keyword">if</span> n&lt;<span class="number">2</span> :</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> Fibonacci(n-<span class="number">1</span>)+Fibonacci(n-<span class="number">2</span>)</span><br><span class="line">Fibonacci(<span class="number">20</span>)    </span><br></pre></td></tr></table></figure>
<p>注:这里说一下n:int和-&gt;是什么意思，由于python中定义变量的时候是不需要指定类型的，所以可能会对别人造成勿扰，所以这里的n:int 代表n是int类型的，如a:int = 3，这一句其实是等价于a=3,int是告诉别这是int型。-&gt;代表函数返回值类型。<strong>值得注意的是，这种指定类型的方式没有强制性作用，a：int=3时候，解释器运行时，会自动把:int优化掉，-&gt;同理，所以其存在意义仅仅是加大代码可读性，你若不按规矩来，别人也没办法，比如a:int = “sagjsa”,完全可以</strong>，注意,冒号后和-&gt;是需要合法的数据类型，对于一些复杂的类型，python提供了typing包，举例如下，如下tup和lis就类似于int一样是一种数据类型了，如果不写前三行，那么a:lis=[(0,0,0)]会报错。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Tuple,List</span><br><span class="line">tup=Tuple[<span class="built_in">int</span>,<span class="built_in">int</span>,<span class="built_in">int</span>]</span><br><span class="line">lis=List[tup]</span><br><span class="line">a:lis = [(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)]</span><br></pre></td></tr></table></figure>
<h3 id="递归求解-结果缓存"><a href="#递归求解-结果缓存" class="headerlink" title="递归求解+结果缓存"></a>递归求解+结果缓存</h3><p>当n为5的时候，树状图如下，我们知道加法运算，会从左往右算的，所以第一个递归完成的必然是如下图粉色笔圈出的函数块，其实观察一下就可以知道，这个粉色块已经计算出了整个树状图需要的所有结果，由于局部函数的数据是在栈中(先入后出)，所以会被系统自动释放，无法保存，所以这个树状图进行了很多很多的重复运算，如果加入一个全局堆区数据，保存下这些结果，用到时候直接读出，那么这样就会大大降低算法复杂度。</p>
<p><a href="https://imgtu.com/i/TeucgH"><img src="https://s4.ax1x.com/2021/12/19/TeucgH.md.jpg" alt="TeucgH.md.jpg"></a></p>
<p>设计如下，就是定义了一个memo来存储计算得到的结果，就是为了重复利用这些值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict</span><br><span class="line">memo:Dict[<span class="built_in">int</span>,<span class="built_in">int</span>] = &#123;<span class="number">0</span>:<span class="number">0</span>,<span class="number">1</span>:<span class="number">1</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci2</span>(<span class="params">n:<span class="built_in">int</span></span>)-&gt;int:</span></span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> memo:</span><br><span class="line">        memo[n]=Fibonacci2(n-<span class="number">1</span>)+Fibonacci2(n-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> memo[n]</span><br><span class="line">Fibonacci2(<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注：n in memo,这个式子是判断n是否是memo的key的，即键值，而不是用于判断value的。在python中，几乎所有的合法数据类型，都可以进行简单的in，==，&gt;,&lt;等，比如“abc”&lt;”cds”就是True，其会自动从第一个开始比较，若相当则比较第二个，比如”abc”&lt;”aac”就是False，之所以可以这样感觉&lt;这个符号底层也是用了python的特色：迭代器，从第一个开始比较一直到最后一个，和for似的，不知道什么时候停止，反正next取不到值了，raiseerror就退出，而sum的本质也是在用迭代器，所以输入的数据不需要是完整的数据，完全可以只输入一个迭代器or生成器</strong></p>
<h3 id="递归求解-自动化结果缓存"><a href="#递归求解-自动化结果缓存" class="headerlink" title="递归求解+自动化结果缓存"></a>递归求解+自动化结果缓存</h3><p>上面的加入缓存后，极大的减少了运算复杂度，但还可以进一步简化，<strong>Python中自带了一个内置的装饰器(decorator),可以自动为如何函数缓存结果，如下，@functools.lru_caches()就是会把函数的返回值缓存起来</strong>，和上面的memo作用一致，当遇到相同的输入参数时，就可以直接用缓存中的数据。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"><span class="meta">@lru_cache(<span class="params">maxsize=<span class="literal">None</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci2</span>(<span class="params">n:<span class="built_in">int</span></span>)-&gt;int:</span></span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> Fibonacci2(n-<span class="number">1</span>)+Fibonacci2(n-<span class="number">2</span>)</span><br><span class="line">Fibonacci2(<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<h3 id="迭代求解"><a href="#迭代求解" class="headerlink" title="迭代求解"></a>迭代求解</h3><p>递归求解本质是反向进行，然后得到结果，然而除了反向求解，我们还可以正向求解，即迭代，只要是递归可以解决的问题，迭代也都可以解决。此处迭代求解复杂度比前面的几种方法都要低。</p>
<p>代码如下，就是从两个值推出下一步需要的两个值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci2</span>(<span class="params">n:<span class="built_in">int</span></span>)-&gt;int:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    last_data = <span class="number">0</span></span><br><span class="line">    next_data = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n):</span><br><span class="line">        last_data,next_data=next_data,last_data+next_data</span><br><span class="line">    <span class="keyword">return</span> next_data</span><br><span class="line">Fibonacci2(<span class="number">50</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注：这里用到了元组解包的操作，就是last_data,next_data=next_data,last_data+next_data这个赋值，一般来说，需要交换数据时候，往往需要引入中间变量，但是元组解包作为一个整体，可以在功能上展示出类似于verilog一样的并行结构，就是相当于last_data=next_data和next_data=last_data+next_data是并行完成的，所以不需要人为引入中间变量了，这样操作相当便利，十分常用。</strong></p>
<h3 id="生成器求解"><a href="#生成器求解" class="headerlink" title="生成器求解"></a>生成器求解</h3><p>首先介绍一下生成器的概念，看如下链接，讲的十分简洁明了。</p>
<p><a href="https://blog.csdn.net/mieleizhi0522/article/details/82142856">(31条消息) python中yield的用法详解——最简单，最清晰的解释</a></p>
<p><strong>所以说通过yield代替return就可以把函数变成一个生成器，生成器的本质还是迭代器，用next取值，仔细向来，现在pytorch中很多封装的读取batch数据的函数返回的一般都是生成器or迭代器or可迭代对象，这样的好处就是节约内存，而for中平时用的特别多的range也可以理解为一个迭代器/生成器。之前用过for循环实现列表生成式，其实还可以实现生成器，因为for本质也用到迭代器，如下:(i for i in range(10))就会得到一个生成器，sum的本质用的也是生成器进行累和的所以完全可以送一个生成器进去然后进行累和，可以节约内存，如下，两句，第一句在小内存的电脑上就容易卡死，而第二句就不会。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sum</span>([i <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">10000000000</span>)])</span><br><span class="line"><span class="built_in">sum</span>(i <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">10000000000</span>))</span><br></pre></td></tr></table></figure>
<p>也正是依赖于迭代器和生成器这一类巧妙的方式，可以使得文件打开+for可以很好的读取文本，但值得注意的是yield和return有所区别，yield一旦执行完毕了，就再也不会输出结果了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read</span>(<span class="params">filename</span>):</span></span><br><span class="line">	<span class="keyword">with</span> <span class="built_in">open</span>(filename) <span class="keyword">as</span> f:</span><br><span class="line">		<span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">			<span class="keyword">yield</span> <span class="built_in">int</span>(line)</span><br><span class="line">file=read(<span class="string">&quot;hhh.txt&quot;</span>)</span><br><span class="line">sum_file = <span class="built_in">sum</span>(file)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> file:</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
<p><strong>如上代码，传给sum一个生成器，sum求和本质是next，所以会遍历完yield，没有下一个yield了，所以下面的for i in file不会有任何输出，所以需要注意，生成器只会遍历完一次就停止。</strong></p>
<p>回到正题，此处的斐波那契数列代码编写如下，结合for自动使用next，可以得到每一步的斐波那契数列结果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Generator</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Fibonacci2</span>(<span class="params">n:<span class="built_in">int</span></span>)-&gt;int:</span></span><br><span class="line">    <span class="keyword">yield</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">    last_data = <span class="number">0</span></span><br><span class="line">    next_data = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,n):</span><br><span class="line">        last_data,next_data=next_data,last_data+next_data</span><br><span class="line">        <span class="keyword">yield</span> next_data</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> Fibonacci2(<span class="number">50</span>):</span><br><span class="line">        print(i)</span><br></pre></td></tr></table></figure>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>

]]></content>
      <categories>
        <category>算法精髓</category>
      </categories>
      <tags>
        <tag>-算法精髓 -python</tag>
      </tags>
  </entry>
  <entry>
    <title>Torch类具体说明--以剪枝模型为例</title>
    <url>/2021/11/28/Torch%E7%B1%BB%E5%85%B7%E4%BD%93%E8%AF%B4%E6%98%8E-%E4%BB%A5%E5%89%AA%E6%9E%9D%E6%A8%A1%E5%9E%8B%E4%B8%BA%E4%BE%8B/</url>
    <content><![CDATA[<p>本节主要讲解剪枝模型，以此模型为基础顺带讲解一下torch中的一些常用方法，参数导出，梯度回溯，以及可视化</p>
<a id="more"></a>
<h1 id="Torch类说明—以剪枝模型为例"><a href="#Torch类说明—以剪枝模型为例" class="headerlink" title="Torch类说明—以剪枝模型为例"></a>Torch类说明—以剪枝模型为例</h1><h2 id="Troch标准类格式01"><a href="#Troch标准类格式01" class="headerlink" title="Troch标准类格式01"></a>Troch标准类格式01</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_var</span>(<span class="params">x, requires_grad=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Automatically choose cpu or cuda</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x = x.cuda()</span><br><span class="line">    <span class="keyword">return</span> x.clone().detach().requires_grad_(requires_grad)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedLinear</span>(<span class="params">nn.Linear</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MaskedLinear, self).__init__(in_features, out_features, bias)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_mask</span>(<span class="params">self, mask</span>):</span></span><br><span class="line">        self.mask = to_var(mask, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        self.weight.data = self.weight.data * self.mask.data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_mask</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.mask</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="keyword">if</span> self.mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            weight = self.weight * self.mask</span><br><span class="line">            <span class="keyword">return</span> F.linear(x, weight, self.bias)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> F.linear(x, self.weight, self.bias)</span><br></pre></td></tr></table></figure>
<p>torch中的主类一般如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear1 = MaskedLinear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">200</span>)</span><br><span class="line">        self.relu1 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.linear2 = MaskedLinear(<span class="number">200</span>, <span class="number">200</span>)</span><br><span class="line">        self.relu2 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.linear3 = MaskedLinear(<span class="number">200</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        out = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">        out = self.relu1(self.linear1(out))</span><br><span class="line">        out = self.relu2(self.linear2(out))</span><br><span class="line">        out = self.linear3(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_masks</span>(<span class="params">self, masks</span>):</span></span><br><span class="line">        <span class="comment"># Should be a less manual way to set masks</span></span><br><span class="line">        <span class="comment"># Leave it for the future</span></span><br><span class="line">        self.linear1.set_mask(masks[<span class="number">0</span>])</span><br><span class="line">        self.linear2.set_mask(masks[<span class="number">1</span>])</span><br><span class="line">        self.linear3.set_mask(masks[<span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_state_dict</span>(<span class="params">self, state_dict, strict=<span class="literal">True</span></span>):</span>        </span><br><span class="line">        <span class="built_in">super</span>().load_state_dict(state_dict, strict=<span class="literal">False</span>)     </span><br></pre></td></tr></table></figure>
<p>其实上述只是我们常用的一些方法重写，但是还有nn.Module还有很多可以提供自由使用的方法，不过正常不需要修改重写而已。我们可以看一下nn.Module中的各个类,输入如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">dir</span>(MLP())</span><br></pre></td></tr></table></figure>
<p>得到结果如下：，可以看到其实dir自身也是类的方法，其可以显示类中的所有方法名，可以看到<strong>init</strong>中的liner1、2、3也作为了其独立的方法，其中还有常见的<strong>forward</strong>，其主要用于拼接一些方法进行前向推理，还有<strong>load_state_dict</strong>方法，其是用于参数加载的，以备模型迁移时候，直接读入权重系数，需要改方法时候，重写一下就好。还有比较常用的train和eval方法，因为有些内容，比如dropout，在训练时用，但测试时不用，所以需要通过一些方法来使一些方法(比如在init中定义的dropout可以理解为一个方法，像linear1就会加入到该类的方法里了)无效(理解为跳过)，或有效，所以需要train方法和eval方法，代表训练和推理模式，使得一些方法有效或无效。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">&#x27;T_destination&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__annotations__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__call__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__class__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__delattr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__dict__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__dir__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__doc__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__eq__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__format__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__ge__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__getattr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__getattribute__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__gt__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__hash__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__init__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__init_subclass__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__le__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__lt__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__module__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__ne__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__new__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__reduce__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__reduce_ex__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__repr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__setattr__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__setstate__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__sizeof__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__slotnames__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__str__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__subclasshook__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;__weakref__&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_apply&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_backward_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_buffers&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_call_impl&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_forward_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_forward_pre_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_get_name&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_load_from_state_dict&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_load_state_dict_pre_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_modules&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_named_members&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_non_persistent_buffers_set&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_parameters&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_register_load_state_dict_pre_hook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_register_state_dict_hook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_replicate_for_data_parallel&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_save_to_state_dict&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_slow_forward&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_state_dict_hooks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;_version&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;add_module&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;apply&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;bfloat16&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;buffers&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;children&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;cpu&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;cuda&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;double&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;dump_patches&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;eval&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;extra_repr&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;float&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;forward&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;half&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;linear1&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;linear2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;linear3&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;load_state_dict&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;modules&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;named_buffers&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;named_children&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;named_modules&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;named_parameters&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;parameters&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;register_backward_hook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;register_buffer&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;register_forward_hook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;register_forward_pre_hook&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;register_parameter&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;relu1&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;relu2&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;requires_grad_&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;set_masks&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;share_memory&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;state_dict&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;to&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;train&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;training&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;type&#x27;</span>,</span><br><span class="line"> <span class="string">&#x27;zero_grad&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>下面仔细分析一下类:其中类中的self是认为实例自身的意思，__init__则是初始化方法的意思，实例创建的时候自动调用__init__方法，而super的意思可以理解为父类的自身，因为MLP继承了父类的所有方法，而初始化你重写了，但是你的初始化中并未对所有的方法初始化(有些方法要初始化)，所以需要调用父类的__init__进行初始化，super().__init__()就是调用父类的初始化函数，但是需要指定对象，即父类的初始化方法是对谁进行初始化的，所以就super(MLP, self).__init__()，前面的MLP代表子类，self代表实例自身，即对该子类的实例进行初始化。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span>        </span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br></pre></td></tr></table></figure>
<p>调用父类初始化函数初始化对象举例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,gender</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.gender = gender</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printinfo</span>(<span class="params">self</span>):</span></span><br><span class="line">        print(self.name,self.gender)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Stu</span>(<span class="params">Person</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,name,gender,school</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Stu, self).__init__(name,gender) <span class="comment"># 使用父类的初始化方法来初始化子类</span></span><br><span class="line">        self.school = school</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printinfo</span>(<span class="params">self</span>):</span> <span class="comment"># 对父类的printinfo方法进行重写</span></span><br><span class="line">        print(self.name,self.gender,self.school)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    stu = Stu(<span class="string">&#x27;djk&#x27;</span>,<span class="string">&#x27;man&#x27;</span>,<span class="string">&#x27;nwnu&#x27;</span>)</span><br><span class="line">    stu.printinfo()</span><br></pre></td></tr></table></figure>
<p>参考链接：<a href="https://blog.csdn.net/dongjinkun/article/details/114575998">(29条消息) 解惑（一） ——- super(XXX, self).<strong>init</strong>()到底是代表什么含义_奋斗の博客-CSDN博客</a></p>
<p>再来看一下后续代码,为什么又用MaskedLinear这个类呢？__init__中，一般我们希望只定义方法或者变量，所以我们写了个类作为方法linear1。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLP</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MLP, self).__init__()</span><br><span class="line">        self.linear1 = MaskedLinear(<span class="number">28</span> * <span class="number">28</span>, <span class="number">200</span>)</span><br></pre></td></tr></table></figure>
<h3 id="MaskedLinear子类01"><a href="#MaskedLinear子类01" class="headerlink" title="MaskedLinear子类01"></a>MaskedLinear子类01</h3><p>下面看一看MaskedLinear这个类，其是继承了Liner类，通过supe(MaskdLinear,self).__init__进行了初始化，注意这里初始化传入了输入单元数和输出单元数。然后<strong>定义了个变量叫mask</strong>，这里需要注意，其不是用self.mask定义变量的，而是self.register_buffer()定义的，self.register_buffer本质上定义的不是参数，而只是留出了一个缓冲区，缓冲区名为mask，初始值是None，这个在之前博客总结说过，即尚未定义的意思。torch中，一般参数存储都是存储成orderedDict形式的(所以模型的参数往往都是在一个字典里)，而orderedDict包括模型的各类module的参数，即nn.Parameter,另一种就是buffer，buffer很特殊的特征就是不会得到更新，即视为一个常数了，而不是变量。参考链接：</p>
<p><a href="https://blog.csdn.net/weixin_38145317/article/details/104917218">(29条消息) pytorch 中register_buffer（）_shuijinghua的博客-CSDN博客_register_buffer</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MaskedLinear</span>(<span class="params">nn.Linear</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_features, out_features, bias=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MaskedLinear, self).__init__(in_features, out_features, bias)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask&#x27;</span>, <span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>再看如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_mask</span>(<span class="params">self, mask</span>):</span></span><br><span class="line">       self.mask = to_var(mask, requires_grad=<span class="literal">False</span>)</span><br><span class="line">       self.weight.data = self.weight.data * self.mask.data</span><br></pre></td></tr></table></figure>
<h4 id="to-var类"><a href="#to-var类" class="headerlink" title="to_var类"></a>to_var类</h4><p>又嵌套了to_var类，来看一下to_var类，代码如下,如果GPU可以使用的话，那么将数据放到GPU上，返回的时候，不是直接返回x，而是使用了一系列方法。在np中，如果直接a=b的话，其实a和b是共享内存的，但如果进行切片的话，则是两个内存空间，由于np是为了数据处理，复杂度远不如神经网络大，所以复用性不需要那么高，但是torch中，由于数据太多，所以复用就很关键，所以不管是直接赋值还是切片，都是指向同一个内存单元，所以需要开辟新空间的话，就需要用clone这个方法了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_var</span>(<span class="params">x, requires_grad=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x = x.cuda()</span><br><span class="line">    <span class="keyword">return</span> x.clone().detach().requires_grad_(requires_grad)</span><br></pre></td></tr></table></figure>
<p>下面来说明一下clone和detach方法，我们可以在jupyter中一下clone和detach的含义,创建一个torch.tensor变量a，然后a.clone?和a.detach？即可查看，detach说明结果如下，可以从描述文档中看出，detach是从来不用梯度的，即无梯度这个属性，即数据共享，但脱离了计算图；clone其实是可以追溯梯度的，后面再说。值得注意的是：<strong>clone是开辟了一个新内存空间，保存梯度信息，但是detach其实是共享了a的data内存，但是丢弃了梯度(为什么不说完全共享内存呢？因为torch中需要梯度等等信息，所以一个tensor变量并不仅仅只有存储数据的空间，还会有梯度空间等等(所以tensor变量有data方法，用于只取出data)，而此处detach只是共享了data空间，即a.data，所以a.detach的指针和a未必相同，即id（a) 和id(a.detach())可能不一样 </strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">detach--Docstring:</span><br><span class="line">Returns a new Tensor, detached from the current graph.</span><br><span class="line">The result will never require gradient.</span><br><span class="line"></span><br><span class="line">clone--Docstring:</span><br><span class="line">clone(*, memory_format&#x3D;torch.preserve_format) -&gt; Tensor</span><br><span class="line">See :func:&#96;torch.clone&#96;</span><br></pre></td></tr></table></figure>
<p>如下代码测试：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">3</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a</span><br><span class="line">c=a.clone()</span><br><span class="line">d=a.detach()</span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<p>得到结果如下，可以看出，require_grad=True即可追溯计算梯度，clone的结果保留了梯度结果，而detach却没有。<code>grad_fn=&lt;CloneBackward&gt;</code>，表示clone后的返回值是个中间变量，<strong>因此支持梯度的回溯</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>], grad_fn=&lt;CloneBackward&gt;)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
<p>如下可以看出detach是共享data内存的，而clone不是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=torch.tensor([<span class="number">1.0</span>,<span class="number">2</span>,<span class="number">3</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">b=a</span><br><span class="line">c=a.clone()</span><br><span class="line">d=a.detach()</span><br><span class="line">d[<span class="number">0</span>]=<span class="number">5</span></span><br><span class="line">c[<span class="number">0</span>]=<span class="number">100</span></span><br><span class="line">print(a)</span><br><span class="line">print(b)</span><br><span class="line">print(c)</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([<span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line">tensor([<span class="number">100.</span>,   <span class="number">2.</span>,   <span class="number">3.</span>], grad_fn=&lt;CopySlices&gt;)</span><br><span class="line">tensor([<span class="number">5.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
<p>上述clone的梯度回溯是什么意思呢？就是对该clone的值的求导结果，值是累加在原值a上的,如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">a_ = a.clone()</span><br><span class="line">y = a**<span class="number">2</span></span><br><span class="line">z = a ** <span class="number">2</span>+a_ * <span class="number">3</span></span><br><span class="line">y.backward()</span><br><span class="line">print(a.grad)</span><br><span class="line">z.backward()</span><br><span class="line">print(a_.grad)</span><br><span class="line">print(a.grad)   </span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出y.backward()方法后，就会往回追溯梯度，得到的2结果存入在a的梯度中，但是z追溯梯度的时候，a_无梯度，是因为其可以梯度结果放在了原值上，此时累计2+2+3得到7，如果没有y.backward()这个句，结果就是5<strong>(由于梯度会进行累加，所以卷积神经网络搭建的时候，要进行清零操作)</strong>。<strong>即如果原数据的requires_grad的属性是True，则clone后也是True，只是梯度信息累加在了原数据上，可以通过a.requires_grad来或得是否进行梯度运算</strong>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line"><span class="literal">None</span></span><br><span class="line">tensor(<span class="number">7.</span>)</span><br></pre></td></tr></table></figure>
<p><strong>如果原数据的requires_grad的属性是False，则clone后设置为True，那么梯度信息只能留在clone后的数据上</strong>，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">a = torch.tensor(<span class="number">1.0</span>)</span><br><span class="line">a_ = a.clone()</span><br><span class="line">a_.requires_grad_() <span class="comment">#require_grad=True</span></span><br><span class="line">y = a_ ** <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line">print(a.grad)   <span class="comment"># None</span></span><br><span class="line">print(a_.grad) </span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="literal">None</span></span><br><span class="line">tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>
<p><strong>注意：其实view其本质只是将数据拉成一个维度的，而torch中只是可视化不同，内存依旧是共享的原数据data的。</strong></p>
<p>参考链接：<a href="https://blog.csdn.net/weixin_43199584/article/details/106876679">(29条消息) PyTorch中的clone(),detach()及相关扩展_Breeze-CSDN博客</a></p>
<p>而a.requires_grad_()是将梯度设置为True，也可以传入True/False,比如：a.requires_grad_(True)，不写默认为True，返回的依旧是a，不过梯度是否纪录这个属性被修改了。所以回到to_var类，其返回的就是clone后的一个新空间然后又detach使其脱离计算图，其实此时不需要再设置require_grad_了，因为其已经没有梯度了，再False没有意义。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_var</span>(<span class="params">x, requires_grad=<span class="literal">False</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">        x = x.cuda()</span><br><span class="line">    <span class="keyword">return</span> x.clone().detach().requires_grad_(requires_grad)</span><br></pre></td></tr></table></figure>
<h3 id="MaskedLinear子类02"><a href="#MaskedLinear子类02" class="headerlink" title="MaskedLinear子类02"></a>MaskedLinear子类02</h3><p>继续MaskedLinear类，所以如下就是传入一个mask，但是clone一个获取新内存同时通过detach使其脱离计算图。（因为剪枝的mask我们是希望不随梯度进行改变的）。<strong>如下self.weight就是得到权重，但是这个权重是加上了梯度属性的，self.weight则得到了简单的数据，其和self.state_dict()[“weight”],state_dict()由上述知道其实类的一个方法，主要是用于获取训练好的模型后的参数，用于保存的，所以self.state_dict()[“weight”]直接就是不带梯度的。所以self.state_dict()[“weight”].data这样是否有这个.data都无所谓，因为梯度属性为Fasle，就不需要空间存储梯度。</strong>如下就是将参数乘上掩码的过程，为了得到稀疏化的参数。<strong>值得注意的是：可以看到nn.Module是weight方法的，weight方法是nn.Linear的方法，因为线性模型只有weight和bias，所以还有bias属性。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_mask</span>(<span class="params">self, mask</span>):</span></span><br><span class="line">       self.mask = to_var(mask, requires_grad=<span class="literal">False</span>)</span><br><span class="line">       self.weight.data = self.weight.data * self.mask.data</span><br></pre></td></tr></table></figure>
<p>该类的forwar也是判断是否进行掩码操作的。</p>
<h2 id="Troch标准类格式02"><a href="#Troch标准类格式02" class="headerlink" title="Troch标准类格式02"></a>Troch标准类格式02</h2><p>继续回到MLP类，如下，这个inplace我们似乎不太常用，这个属性作用是什么呢？其默认是False，即输入一个值时候，默认为是值传递，即返回的数值是一个新的内存空间，而True的话，则是地址传递，可以通过打印<strong>id(a)</strong>来查看，发现其输入输出是同一个地址，即共享内存了，会修改输入的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.relu1 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>然后下面如下，其实forward函数是类实例化后，输入参数自动执行的方法，其实本质是自动执行了__call__,然后该魔法函数中调用了forward。在该函数中，上述clone那写了，view只是改变可视化，内存依旧与之前的共享。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">    out = x.view(x.size(<span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">    out = self.relu1(self.linear1(out))</span><br><span class="line">    out = self.relu2(self.linear2(out))</span><br><span class="line">    out = self.linear3(out)</span><br><span class="line">    <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>然后看下一个方法，这个方法显然是给linear方法设置参数的，linear方法即上面的MaskLinear类，其通过set_mask来设置掩码。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_masks</span>(<span class="params">self, masks</span>):</span></span><br><span class="line">    <span class="comment"># Should be a less manual way to set masks</span></span><br><span class="line">    <span class="comment"># Leave it for the future</span></span><br><span class="line">    self.linear1.set_mask(masks[<span class="number">0</span>])</span><br><span class="line">    self.linear2.set_mask(masks[<span class="number">1</span>])</span><br><span class="line">    self.linear3.set_mask(masks[<span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<p>最后是加载权重的方法，如下。strcit的参数意思是，如果之前的网络时两层的，但是现在是3层的，那么直接加载会报错，因为两个模型不一致，但是将这个参数改为False，即表示能用多少用多少，即取两层作为当前模型的前两层的权重。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_state_dict</span>(<span class="params">self, state_dict, strict=<span class="literal">True</span></span>):</span>        </span><br><span class="line">    <span class="built_in">super</span>().load_state_dict(state_dict, strict=<span class="literal">False</span>)     </span><br></pre></td></tr></table></figure>
<p>模型搭建好了后，下面是训练函数的定义,首先得到一个实例化的model后，调用方法model.train(),这个方法是使得一些训练的方法生效(比如dropout)，使得一些推理的方法失效，与之对应的是model.eval()，其代表推理模式。然后for循环给批量数据，每次需要把数据送到GPU里(这里实例化的模型传入之前应该就是已放在GPU上了)，然后梯度归零，因为梯度会进行累加，所以要进行清零。后面就是正向传播得到输出，计算损失，再对损失进行求反向梯度，然后优化器优化（各类梯度下降，SGD等等）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">model, device, train_loader, optimizer, epoch</span>):</span></span><br><span class="line">    model.train()</span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (data, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        data, target = data.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(data)</span><br><span class="line">        loss = F.cross_entropy(output, target)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        total += <span class="built_in">len</span>(data)</span><br><span class="line">        progress = math.ceil(batch_idx / <span class="built_in">len</span>(train_loader) * <span class="number">50</span>)</span><br><span class="line">        print(<span class="string">&quot;\rTrain epoch %d: %d/%d, [%-51s] %d%%&quot;</span> %</span><br><span class="line">              (epoch, total, <span class="built_in">len</span>(train_loader.dataset),</span><br><span class="line">               <span class="string">&#x27;-&#x27;</span> * progress + <span class="string">&#x27;&gt;&#x27;</span>, progress * <span class="number">2</span>), end=<span class="string">&#x27;&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>推理函数如下，较为简单，讲解略：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span>(<span class="params">model, device, test_loader</span>):</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    test_loss = <span class="number">0</span></span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> data, target <span class="keyword">in</span> test_loader:</span><br><span class="line">            data, target = data.to(device), target.to(device)</span><br><span class="line">            output = model(data)</span><br><span class="line">            test_loss += F.cross_entropy(output, target, reduction=<span class="string">&#x27;sum&#x27;</span>).item()  <span class="comment"># sum up batch loss</span></span><br><span class="line">            pred = output.argmax(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># get the index of the max log-probability</span></span><br><span class="line">            correct += pred.eq(target.view_as(pred)).<span class="built_in">sum</span>().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= <span class="built_in">len</span>(test_loader.dataset)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;\nTest: average loss: &#123;:.4f&#125;, accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)&#x27;</span>.<span class="built_in">format</span>(</span><br><span class="line">        test_loss, correct, <span class="built_in">len</span>(test_loader.dataset),</span><br><span class="line">        <span class="number">100.</span> * correct / <span class="built_in">len</span>(test_loader.dataset)))</span><br><span class="line">    <span class="keyword">return</span> test_loss, correct / <span class="built_in">len</span>(test_loader.dataset)</span><br></pre></td></tr></table></figure>
<p>训练完成后，要考虑剪枝,如下，其中第一个参数是模型，第二个参数是需要剪枝的比例，比如0.9，即剪百分之九十的weight，即使得百分之90的weight为0。首先遍历参数，如果size长度不是1则需要裁剪(因为bias是一维的，即shape一维，所以bias不需要裁剪)，<strong>np.percentile(weight, pruning_perc)这个第一个参数传入一个数组，第二个参数传入0-100(代表0%-100%)的数a,返回的是一个值，比如传入60，返回的这个值超过了数组中百分之60的数，来举个例子，比如传入的数组是[[1,4,6],[2,5,7]],百分数参数是70，则把数组拉平从小到大排序，即1,2,4,5,6,7，那么百分之70在哪呢？numpy中下标是0开始到5，所以5<em>0.7=3.5,所以得到的下标应该是在3.5这，那么3.5不是整数怎么办？进行线性插值，所以结果就是5+(6-5)\</em>0.5=5.5。得到这个值即可通过判断data&gt;该值，得到0-1变量，即掩码。</strong>然后第二个for即是生成0-1变量的掩码，记得得变成float类型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_prune</span>(<span class="params">model, pruning_perc</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Prune pruning_perc % weights layer-wise</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    threshold_list = []</span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(p.data.size()) != <span class="number">1</span>: <span class="comment"># bias</span></span><br><span class="line">            weight = p.cpu().data.<span class="built_in">abs</span>().numpy().flatten()</span><br><span class="line">            threshold = np.percentile(weight, pruning_perc)</span><br><span class="line">            threshold_list.append(threshold)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># generate mask</span></span><br><span class="line">    masks = []</span><br><span class="line">    idx = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(p.data.size()) != <span class="number">1</span>:</span><br><span class="line">            pruned_inds = p.data.<span class="built_in">abs</span>() &gt; threshold_list[idx]</span><br><span class="line">            masks.append(pruned_inds.<span class="built_in">float</span>())</span><br><span class="line">            idx += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> masks</span><br></pre></td></tr></table></figure>
<p>所有函数都定义好了，下面看一下主函数，torch.manual_seed(0)函数是设定随机数种子，后面生成的随机数都是固定的，即不会变化(ramdom.seed类似)，然后使用torch.utils.data.DataLoader读取MNIST数据集，如果没有该数据集，则将download设置为True，’../data/MNIST’即为下载的路径，同时也是MNIST数据集的路径，..和linux中的cd ..一样，代表路径回退一级的意思。在’../data/MNIST’下，还会有一个交MNIST的文件夹，里面有raw、processed文件夹，datasets.MNIST这个函数会自动识别的，transform参数代表数据预处理，即变为tensor然后还进行归一化（均值和方差）。torch.utils.data.DataLoader的batch_size是设定batch大小，shuffle代表是否打乱顺序。训练完成后测试，<strong>然后保存模型参数：torch.save(model.state_dict(), ‘not-pruned.ckpt’)</strong>，然后通过copy库的deepcopy来深度拷贝这个模型，作为剪枝，同时测试剪枝后的准确率。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    epochs = <span class="number">1</span></span><br><span class="line">    batch_size = <span class="number">64</span></span><br><span class="line">    torch.manual_seed(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    train_loader = torch.utils.data.DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&#x27;../data/MNIST&#x27;</span>, train=<span class="literal">True</span>, download=<span class="literal">False</span>,</span><br><span class="line">                       transform=transforms.Compose([</span><br><span class="line">                           transforms.ToTensor(),</span><br><span class="line">                           transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">                       ])),</span><br><span class="line">        batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line">    test_loader = torch.utils.data.DataLoader(</span><br><span class="line">        datasets.MNIST(<span class="string">&#x27;../data/MNIST&#x27;</span>, train=<span class="literal">False</span>, download=<span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">            transforms.ToTensor(),</span><br><span class="line">            transforms.Normalize((<span class="number">0.1307</span>,), (<span class="number">0.3081</span>,))</span><br><span class="line">        ])),</span><br><span class="line">        batch_size=<span class="number">1000</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    model = MLP().to(device)</span><br><span class="line">    optimizer = torch.optim.Adadelta(model.parameters())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, epochs + <span class="number">1</span>):</span><br><span class="line">        train(model, device, train_loader, optimizer, epoch)</span><br><span class="line">        _, acc = test(model, device, test_loader)</span><br><span class="line">    </span><br><span class="line">    torch.save(model.state_dict(), <span class="string">&#x27;not-pruned.ckpt&#x27;</span>)</span><br><span class="line">    print(<span class="string">&quot;\n=====Pruning 60%=======\n&quot;</span>)</span><br><span class="line">    pruned_model = deepcopy(model)</span><br><span class="line">    mask = weight_prune(pruned_model, <span class="number">60</span>)</span><br><span class="line">    pruned_model.set_masks(mask)</span><br><span class="line">    test(pruned_model, device, test_loader)</span><br><span class="line">    torch.save(pruned_model.state_dict(), <span class="string">&#x27;pruned.ckpt&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> model, pruned_model</span><br></pre></td></tr></table></figure>
<p>剪枝完成后，需要考虑可视化问题，如下，其中model.modules是一个迭代器，可以遍历所有模型的所有子层(init中定义的层)，然后再通过hasattr函数来判断该层是否有weight属性来判断是否是全连接层，进行绘图，plt.subplot(131)代表1行3列的绘图板，在第一个格子里绘图，plt.subplot(132)，表示在第二个格子里绘图。结果可以发现剪枝百分之60后，准确率只掉了一个点，但是参数化绝对稀疏了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_weights</span>(<span class="params">model</span>):</span></span><br><span class="line">    modules = [module <span class="keyword">for</span> module <span class="keyword">in</span> model.modules()]</span><br><span class="line">    num_sub_plot = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(modules):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(layer, <span class="string">&#x27;weight&#x27;</span>):</span><br><span class="line">            plt.subplot(<span class="number">131</span>+num_sub_plot)</span><br><span class="line">            w = layer.weight.data</span><br><span class="line">            w_one_dim = w.cpu().numpy().flatten()</span><br><span class="line">            plt.hist(w_one_dim[w_one_dim!=<span class="number">0</span>], bins=<span class="number">50</span>)</span><br><span class="line">            num_sub_plot += <span class="number">1</span></span><br><span class="line">    plt.show()</span><br><span class="line">plot_weights(model)</span><br><span class="line">plot_weights(pruned_model)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<p><a href="https://imgtu.com/i/oehbND"><img src="https://z3.ax1x.com/2021/11/27/oehbND.png" alt="oehbND.png"></a><br><a href="https://imgtu.com/i/oehHAO"><img src="https://z3.ax1x.com/2021/11/27/oehHAO.png" alt="oehHAO.png"></a></p>
<p>下面具体说一下modules方法,测试代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">modules = [module <span class="keyword">for</span> module <span class="keyword">in</span> model.modules()]</span><br><span class="line"><span class="comment">#print(modules)</span></span><br><span class="line">count = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> module <span class="keyword">in</span> model.modules():</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">hasattr</span>(module, <span class="string">&#x27;weight&#x27;</span>):</span><br><span class="line">        print(count,<span class="string">&quot;有weight属性&quot;</span>)</span><br><span class="line">    print(count,module)</span><br><span class="line">    count = count+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出，迭代器返回的第一个是MLP大类的所有方法层，后续开始返回单独子层，上面也说了MLP是继承nn.Module的，dir可以看出其是没有weight方法的，但是Linear是有的，所以通过判断是否有weight属性来判断是否是全连接or卷积层。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span> MLP(</span><br><span class="line">  (linear1): MaskedLinear(in_features=<span class="number">784</span>, out_features=<span class="number">200</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (relu1): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  (linear2): MaskedLinear(in_features=<span class="number">200</span>, out_features=<span class="number">200</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  (relu2): ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">  (linear3): MaskedLinear(in_features=<span class="number">200</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br><span class="line">)</span><br><span class="line"><span class="number">2</span> 有weight属性</span><br><span class="line"><span class="number">2</span> MaskedLinear(in_features=<span class="number">784</span>, out_features=<span class="number">200</span>, bias=<span class="literal">True</span>)</span><br><span class="line"><span class="number">3</span> ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="number">4</span> 有weight属性</span><br><span class="line"><span class="number">4</span> MaskedLinear(in_features=<span class="number">200</span>, out_features=<span class="number">200</span>, bias=<span class="literal">True</span>)</span><br><span class="line"><span class="number">5</span> ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="number">6</span> 有weight属性</span><br><span class="line"><span class="number">6</span> MaskedLinear(in_features=<span class="number">200</span>, out_features=<span class="number">10</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>总结:a代表tensor变量，C代表一个类</p>
<p>a.clone机制</p>
<p>a.detach机制</p>
<p>a.requires_grad_</p>
<p>a.data</p>
<p>id(a)#打印指针</p>
<p>nn.Relu的inplace参数</p>
<p>C.state_dict()#获得含所有参数的字典，无梯度信息，且不是迭代器，其他parameters等都是可迭代对象</p>
<p>C.parameters()#获得所有参数，可以list(C.parameter)变成迭代器，也可以for，得到的参数含梯度信息。</p>
<p>C.weight#获得网络权重，nn.Linear有这个方法，还有bias方法，nn.Module没有，</p>
<p>np.percentile#返回超过百分之a的数。</p>
<p>注：torch中梯度等需要数据是float，数据生成的时候，最好都是以float存储，至少tf中都不会自动类型转换，可能怕存储爆炸？torch测试了是会自动数据转换，但是还是最好float一下。</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>

]]></content>
      <categories>
        <category>深度学习</category>
        <category>剪枝</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>剪枝</tag>
        <tag>Pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>xml转Ms CoCo格式的json</title>
    <url>/2021/08/20/xml%E8%BD%ACMs-CoCo%E6%A0%BC%E5%BC%8F%E7%9A%84json/</url>
    <content><![CDATA[<p>上次写过xml转json格式的代码，那个是一个xml对应一个json，这个json后来又写到了tfrecord格式的文件，模型加载这个tfrecord文件即可，最近又遇到了Ms CoCo格式的json格式，这种格式是把所有的xml文件转成json的，模型应该是直接加载这个json就可以（不确定）。下面就来描述一下如何把批量的xml转成一个Ms CoCo格式的json。</p>
<a id="more"></a>
<h2 id="1、xml格式"><a href="#1、xml格式" class="headerlink" title="1、xml格式"></a>1、xml格式</h2>]]></content>
  </entry>
  <entry>
    <title>深脑云平台服务器使用说明</title>
    <url>/2021/08/20/%E6%B7%B1%E8%84%91%E4%BA%91%E5%B9%B3%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<p>服务器开始逐渐使用<strong>DBCloud Lab</strong> 进行管理，此篇用于在开通账号后，给各位实验室人的使用说明指南，在此感谢lwq借给的演示账号。</p>
<a id="more"></a>
<h2 id="1、first-step"><a href="#1、first-step" class="headerlink" title="1、first step"></a>1、first step</h2><p>登录下面的网站：</p>
<blockquote class="blockquote-center">
            <p><a href="http://222.73.22.185:47688/login">DBCloud Lab</a></p>

          </blockquote>
<p>输入账号和密码，进入如下界面：</p>
<p><a href="https://imgtu.com/i/fLyCse"><img src="https://z3.ax1x.com/2021/08/20/fLyCse.png" alt="fLyCse.png"></a></p>
<h2 id="2、second-step"><a href="#2、second-step" class="headerlink" title="2、second step"></a>2、second step</h2><p>点击资源管理—服务器列表，可以看到自己组别内的GPU型号、数量和使用情况等等，如下：</p>
<p><a href="https://imgtu.com/i/fLyBw9"><img src="https://z3.ax1x.com/2021/08/20/fLyBw9.png" alt="fLyBw9.png"></a></p>
<p>点击如下位置：</p>
<p><a href="https://imgtu.com/i/fLyL6S"><img src="https://z3.ax1x.com/2021/08/20/fLyL6S.png" alt="fLyL6S.png"></a></p>
<p>进入的界面如下：</p>
<p><a href="https://imgtu.com/i/fL69f0"><img src="https://z3.ax1x.com/2021/08/20/fL69f0.png" alt="fL69f0.png"></a></p>
<h2 id="3、third-step"><a href="#3、third-step" class="headerlink" title="3、third step"></a>3、third step</h2><p>点击创建实例，如下图，注意点击创建实例时候需要选中GPU数量，组内可自行分配。</p>
<p><a href="https://imgtu.com/i/fLcsxK"><img src="https://z3.ax1x.com/2021/08/20/fLcsxK.png" alt="fLcsxK.png"></a></p>
<p>点击创建实例后得到如下界面，点击下图的确认界面即创建成功：</p>
<p><a href="https://imgtu.com/i/fLcIRP"><img src="https://z3.ax1x.com/2021/08/20/fLcIRP.png" alt="fLcIRP.png"></a></p>
<p>创建好的实例列表都可以在当前界面下面看到，如下图：</p>
<p><a href="https://imgtu.com/i/fLgViR"><img src="https://z3.ax1x.com/2021/08/20/fLgViR.png" alt="fLgViR.png"></a></p>
<p>点击实例访问信息，如下图：</p>
<p><a href="https://imgtu.com/i/fLgJJI"><img src="https://z3.ax1x.com/2021/08/20/fLgJJI.png" alt="fLgJJI.png"></a></p>
<p>可以看到的信息如下：</p>
<p><a href="https://imgtu.com/i/fLgRyT"><img src="https://z3.ax1x.com/2021/08/20/fLgRyT.png" alt="fLgRyT.png"></a></p>
<p>可以点击打开终端按键进行打开终端：</p>
<p><a href="https://imgtu.com/i/fLgv0e"><img src="https://z3.ax1x.com/2021/08/20/fLgv0e.png" alt="fLgv0e.png"></a></p>
<h2 id="4、forth-step"><a href="#4、forth-step" class="headerlink" title="4、forth step"></a>4、forth step</h2><p>DBCloud中使用的终端有点卡，所以下面使用ssh进行连接，ssh信息在实例访问信息中，如下图：</p>
<p><a href="https://imgtu.com/i/fL2NN9"><img src="https://z3.ax1x.com/2021/08/20/fL2NN9.png" alt="fL2NN9.png"></a></p>
<p>记住ssh服务—代理访问的信息，即下图，该ip地址和端口号在ssh连接时候需要使用.(更新：深脑云更新后，直接使用主机的ip地址和端口号即可，代理的网络带宽严重受限，传输速度极慢，不建议使用代理ip和端口号。即建议使用下图红框左边的ip和端口)。记住ip和端口号以后，后面可以进行ssh连接，此时需要开启实例的ssh连接权限，点击“ssh服务”的红色感叹号，可以看到启动ssh连接的命令，在深脑云网页端终端内输入即可</p>
<p><a href="https://imgtu.com/i/fL2q4s"><img src="https://z3.ax1x.com/2021/08/20/fL2q4s.png" alt="fL2q4s.png"></a></p>
<h2 id="5、fifth-step"><a href="#5、fifth-step" class="headerlink" title="5、fifth step"></a>5、fifth step</h2><p>知道了ssh的ip和端口号，我们需要一个远程连接的软件，这里推荐使用mobaXterm(感谢雷正鑫师兄的推荐)，软件链接如下，下载免费版即可：</p>
<blockquote class="blockquote-center">
            <p><a href="https://mobaxterm.mobatek.net/">mobaxterm</a></p>

          </blockquote>
<p>软件安装后，得到如下界面，然后点击左上角的session：</p>
<p><a href="https://imgtu.com/i/fLWjmT"><img src="https://z3.ax1x.com/2021/08/20/fLWjmT.png" alt="fLWjmT.png"></a></p>
<p>得到如下界面：</p>
<p><a href="https://imgtu.com/i/fLfQ1I"><img src="https://z3.ax1x.com/2021/08/20/fLfQ1I.png" alt="fLfQ1I.png"></a></p>
<p>点击最左侧的SSH按键，得到如下界面：</p>
<p><a href="https://imgtu.com/i/fLfhg1"><img src="https://z3.ax1x.com/2021/08/20/fLfhg1.png" alt="fLfhg1.png"></a></p>
<p>将forth step中记下的ssh的ip和端口号填在此处，如下图所示：</p>
<p><a href="https://imgtu.com/i/fL4PL6"><img src="https://z3.ax1x.com/2021/08/20/fL4PL6.png" alt="fL4PL6.png"></a></p>
<p>到此步即完成添加，点击OK即可，此时软件界面右侧出现我们添加的ip，如下图</p>
<p><a href="https://imgtu.com/i/fL4GFg"><img src="https://z3.ax1x.com/2021/08/20/fL4GFg.md.png" alt="fL4GFg.md.png"></a></p>
<p>双击上图的ip，得到如下界面，输入个人账号然后回车，然后会提醒输入密码，输入密码即可</p>
<p><a href="https://imgtu.com/i/fL4ayq"><img src="https://z3.ax1x.com/2021/08/20/fL4ayq.md.png" alt="fL4ayq.md.png"></a></p>
<p>注：如果在校外使用或在校内未连接校园网使用，需要连接VPN才能使用</p>
<h2 id="更新3：Vscode的配置"><a href="#更新3：Vscode的配置" class="headerlink" title="更新3：Vscode的配置"></a>更新3：Vscode的配置</h2><p>自行下载一个vscode，在扩展商店中下载插件remote-ssh，，如下图，推荐下载0.65.1版本，亲测最新版本有坑</p>
<p><a href="https://imgtu.com/i/hFRwE4"><img src="https://z3.ax1x.com/2021/08/24/hFRwE4.md.png" alt="hFRwE4.md.png"></a></p>
<p>然后在vscode左侧就会出现一个远程连接的按钮，如下，先点击加号，得到第二步的输入口，在输入口中输入服务器的ip(初始教程那个代理ip，不是真实ip)，如下图</p>
<p><a href="https://imgtu.com/i/hFWmGR"><img src="https://z3.ax1x.com/2021/08/24/hFWmGR.md.png" alt="hFWmGR.md.png"></a></p>
<p>然后出现如下界面：</p>
<p><a href="https://imgtu.com/i/hFWWQ0"><img src="https://z3.ax1x.com/2021/08/24/hFWWQ0.md.png" alt="hFWWQ0.md.png"></a></p>
<p>点击进入，进入如下界面</p>
<p><a href="https://imgtu.com/i/hFfCYd"><img src="https://z3.ax1x.com/2021/08/24/hFfCYd.md.png" alt="hFfCYd.md.png"></a></p>
<p>添加如下语句，其中hhh改为自己的用户名，Port后的改为自己的端口号。</p>
<p><a href="https://imgtu.com/i/hFfdh9"><img src="https://z3.ax1x.com/2021/08/24/hFfdh9.md.png" alt="hFfdh9.md.png"></a></p>
<p>此时已完成操作，点击如下图处即可连接服务器并选择打开的文件夹路径</p>
<p><a href="https://imgtu.com/i/hFfOhj"><img src="https://z3.ax1x.com/2021/08/24/hFfOhj.md.png" alt="hFfOhj.md.png"></a></p>
<p>注：若无法调试，插件商城重新安装python即可，若依旧不可以，降低ssh的版本。</p>
<h2 id="第四次更新"><a href="#第四次更新" class="headerlink" title="第四次更新"></a>第四次更新</h2><p>服务器创建账号好，需要自己安装anaconda。</p>
<p>首先通过如下文件下载linux下的anaconda</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-<span class="number">2020.07</span>-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>通过如下语句执行该文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bash Anaconda3-<span class="number">2020.07</span>-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>
<p>记得cd转入到该目录下再输入上述命令，中途会要求选择yes或者enter，输入enter和yes即可。</p>
<p>最后即可安装完毕。</p>
<p>此时conda依旧是不能用的，需要配置环境变量，root的环境变量没有权限添加，所以需要在.bashrc中添加，输入如下命令，用vim打开bashrc</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure>
<p>在该文件最后添加如下语句</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">export PATH=$PATH:/home/lll/anaconda3/<span class="built_in">bin</span></span><br></pre></td></tr></table></figure>
<p>其中$PATH：后的路径记得改为自己anaconda的路径。</p>
<p>然后ctr+:的按键即可退出编辑，输入wq!(lwq的wq，哈哈，其实是write+quit的意思)，即可保存退出，此时安装完毕，conda可以正常使用</p>
<p>参考链接：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/anpwu/article/details/109788684">给远程服务器(Linux) 安装Anaconda</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/qq_42154813/article/details/114691906">LINUX服务器下安装Anaconda并配置环境变量</a></p>

          </blockquote>
<h2 id="第一次更新"><a href="#第一次更新" class="headerlink" title="第一次更新"></a>第一次更新</h2><p>在mobaxterm双击左侧ip后，输入用户名和密码，进入终端，若发现未进入base环境，但如下命令可用：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda env <span class="built_in">list</span></span><br></pre></td></tr></table></figure>
<p>但如下的激活无效，且报错Your shell has not been properly configured to use ‘conda activate’</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate base</span><br></pre></td></tr></table></figure>
<p>出现此报错的原因是因为之前的虚拟环境没有退出，终端输入<code>source deactivate</code>，然后需要重新进入虚拟环境： <code>source activate</code>。如此即可conda activate 环境名。</p>
<p>注：好像每次都需要重新这样操作，即使conda config —set auto_activate_base true了也没有用，欢迎解决此问题的人进行补充操作说明。</p>
<h2 id="第二次更新"><a href="#第二次更新" class="headerlink" title="第二次更新"></a>第二次更新</h2><p>vscode更新后，可能会导致ssh远程连接失败，报错：_workbench.downloadResource‘ failed。具体解决方案如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/ibless/article/details/118610776">VS Code的Error: Running the contributed command: ‘_workbench.downloadResource‘ failed解决</a></p>

          </blockquote>
<p>如果按照这个连上了，可能还会出现无法调试的现象，但是给远程安装python编译器就安装不上，这是ssh插件的问题，最新版本有bug存在，降低到0.65.1即可。</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>

]]></content>
      <tags>
        <tag>-服务器 -深脑云</tag>
      </tags>
  </entry>
  <entry>
    <title>xml文件的数据分析</title>
    <url>/2021/08/18/xml%E6%96%87%E4%BB%B6%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</url>
    <content><![CDATA[<p>上篇得到了飞机的tif和对应的xml文件后(xml文件中的object只留下了飞机)，我们需要对飞机的信息进行分析汇总，此处对每一类飞机主要想统计的信息有如下：</p>
<a id="more"></a>
<ul>
<li>该类别的图片数量</li>
<li>飞机的最小宽度</li>
<li>飞机的最小高度</li>
<li>飞机最小的宽高比</li>
<li>飞机的最大宽度</li>
<li>飞机的最大高度</li>
<li>飞机最大的宽高比</li>
<li>图片的最小宽度</li>
<li>图片的最小高度</li>
<li>图片的最小深度</li>
<li>图片的最大宽度</li>
<li>图片的最大高度</li>
<li>图片的最大深度</li>
</ul>
<p>在之前的xml转json格式的博客中，那个read_xml_gtbox_and_label函数中加入如下的信息统计，即多读取一个size，size中三个数分别代表宽度、高度和深度，并将size数据return返回，注意其是字符串格式的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> child_of_root.tag == <span class="string">&#x27;size&#x27;</span>:</span><br><span class="line">    size = [-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>]<span class="comment">#[width,height,depth]</span></span><br><span class="line">    <span class="keyword">for</span> child_item <span class="keyword">in</span> child_of_root:</span><br><span class="line">        <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;width&#x27;</span>: </span><br><span class="line">            size[<span class="number">0</span>] = child_item.text</span><br><span class="line">        <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;height&#x27;</span>: </span><br><span class="line">            size[<span class="number">1</span>] = child_item.text</span><br><span class="line">        <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;depth&#x27;</span>: </span><br><span class="line">            size[<span class="number">2</span>] = child_item.text</span><br></pre></td></tr></table></figure>
<p>read_xml_gtbox_and_label函数的博客链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="http://gezhilai.com/2021/08/16/xml%E6%A0%BC%E5%BC%8F%E8%BD%ACjson/">xml格式转json</a></p>

          </blockquote>
<p>然后得到返回后，进行信息统计，统计得到的numpy数组进行保存。飞机的长宽以及长宽比，其实是按照矩形框来的，由于给的数据集是四边形框，我将这个框变成了xy方向上的水平矩形框进行计算高和宽。值得注意的是，飞机的尺寸竟然出现了浮点数，不知道是组委会给的xml文件有问题还是有浮点数是正常的，不过坐标是标记像素的位置的，出现浮点数有点不太正常。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    统计gf2021中目标大小(影响滑动窗口进行切片)，统计飞机的目标大小，统计含括飞机的图片尺寸，把</span></span><br><span class="line"><span class="string">    有飞机的图片另存为，同时另存xml，把xml也修改一下，把非飞机的目标丢弃掉。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.misc <span class="keyword">as</span> misc</span><br><span class="line"><span class="keyword">from</span> xml.dom.minidom <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy, cv2</span><br><span class="line"><span class="comment"># import imageio</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> osgeo <span class="keyword">import</span> gdal, gdalconst</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">information_statistics_save</span>(<span class="params">image_path, xml_path</span>):</span></span><br><span class="line">    plane_data = np.zeros((<span class="number">10</span>,<span class="number">13</span>))<span class="comment">#num,min_width,min_height,min_ratio,max_width,max_height,max_ratio。min_pic_width,min_pic_height,min_pic_depth,max_pic_width,max_pic_height,max_pic_depth</span></span><br><span class="line">    initialization=np.array([<span class="number">0</span>,<span class="number">10000</span>,<span class="number">10000</span>,<span class="number">10000</span>,-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>,<span class="number">10000</span>,<span class="number">10000</span>,<span class="number">10000</span>,-<span class="number">1</span>,-<span class="number">1</span>,-<span class="number">1</span>])</span><br><span class="line">    plane_data  = plane_data    +   initialization <span class="comment">#初始化，广播机制</span></span><br><span class="line">    plane_label = [<span class="string">&quot;Boeing737&quot;</span>,<span class="string">&quot;Boeing747&quot;</span>,<span class="string">&quot;Boeing777&quot;</span>,<span class="string">&quot;C919&quot;</span>,<span class="string">&quot;A220&quot;</span>,<span class="string">&quot;A321&quot;</span>,<span class="string">&quot;A330&quot;</span>,<span class="string">&quot;A350&quot;</span>,<span class="string">&quot;ARJ21&quot;</span>,<span class="string">&quot;other-airplane&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> count, xml <span class="keyword">in</span> <span class="built_in">enumerate</span>(glob.glob(xml_path + <span class="string">&#x27;/*.xml&#x27;</span>)):<span class="comment">#进行索引，*代表任意匹配，得到可迭代对象</span></span><br><span class="line">        <span class="comment"># to avoid path error in different development platform</span></span><br><span class="line">        xml = xml.replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>)<span class="comment">#用/替换\\,后面的替换前面的</span></span><br><span class="line">        (_, xml_name) = os.path.split(xml)<span class="comment">#分割。返回路径和文件名。若本身就是一个文件夹路径，则返回路径和空</span></span><br><span class="line">        <span class="comment">#print(xml)</span></span><br><span class="line">        img_name, box , size, difficult_list, tmp_score_list = read_xml_gtbox_and_label(xml)</span><br><span class="line">        img_name=xml_name.replace(<span class="string">&#x27;.xml&#x27;</span>, <span class="string">&#x27;.tif&#x27;</span>)</span><br><span class="line">        img_path = image_path + <span class="string">&#x27;/&#x27;</span> + img_name</span><br><span class="line"></span><br><span class="line">        size = np.float32(size)</span><br><span class="line"> </span><br><span class="line">        <span class="keyword">for</span> box_one_object <span class="keyword">in</span> box:</span><br><span class="line">            <span class="keyword">if</span> box_one_object[<span class="number">8</span>] <span class="keyword">in</span> plane_label:</span><br><span class="line">                index_label=plane_label.index(box_one_object[<span class="number">8</span>])</span><br><span class="line">                <span class="comment">#print(index_label)</span></span><br><span class="line">                plane_data[index_label,<span class="number">0</span>]=plane_data[index_label,<span class="number">0</span>]+<span class="number">1</span><span class="comment">#该类型飞机数量加1</span></span><br><span class="line">                box_float=np.float32(box_one_object[<span class="number">0</span>:<span class="number">8</span>])<span class="comment">#float只能对单个数使用，数组类的用np.float</span></span><br><span class="line">                x_box=box_float[[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>]]</span><br><span class="line">                y_box=box_float[[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>]]</span><br><span class="line">                <span class="comment">#print(x_box,y_box)</span></span><br><span class="line">                width = np.<span class="built_in">max</span>(x_box)-np.<span class="built_in">min</span>(x_box)</span><br><span class="line">                height = np.<span class="built_in">max</span>(y_box)-np.<span class="built_in">min</span>(y_box)</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">int</span>(width*<span class="number">100</span>) ==<span class="number">758</span>:</span><br><span class="line">                    print(xml)</span><br><span class="line">                <span class="keyword">if</span> width &lt; plane_data[index_label,<span class="number">1</span>]:</span><br><span class="line">                    plane_data[index_label,<span class="number">1</span>]=width</span><br><span class="line">                <span class="keyword">if</span> height &lt; plane_data[index_label,<span class="number">2</span>]:                    </span><br><span class="line">                    plane_data[index_label,<span class="number">2</span>]=height </span><br><span class="line">                <span class="keyword">if</span> width/height &lt; plane_data[index_label,<span class="number">3</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">3</span>]=width/height   </span><br><span class="line">                <span class="keyword">if</span> width &gt; plane_data[index_label,<span class="number">4</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">4</span>]=width </span><br><span class="line">                <span class="keyword">if</span> height &gt; plane_data[index_label,<span class="number">5</span>]:</span><br><span class="line">                    plane_data[index_label,<span class="number">5</span>]=height</span><br><span class="line">                <span class="keyword">if</span> width/height &gt; plane_data[index_label,<span class="number">6</span>]:                    </span><br><span class="line">                    plane_data[index_label,<span class="number">6</span>]=width/height </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">0</span>] &lt; plane_data[index_label,<span class="number">7</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">7</span>]=size[<span class="number">0</span>]   </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">1</span>] &lt; plane_data[index_label,<span class="number">8</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">8</span>]=size[<span class="number">1</span>]                       </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">2</span>] &lt; plane_data[index_label,<span class="number">9</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">9</span>]=size[<span class="number">2</span>]   </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">0</span>] &gt; plane_data[index_label,<span class="number">10</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">10</span>]=size[<span class="number">0</span>]   </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">1</span>] &gt; plane_data[index_label,<span class="number">11</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">11</span>]=size[<span class="number">1</span>]                       </span><br><span class="line">                <span class="keyword">if</span> size[<span class="number">2</span>] &gt; plane_data[index_label,<span class="number">12</span>]:               </span><br><span class="line">                    plane_data[index_label,<span class="number">12</span>]=size[<span class="number">2</span>] </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                print(<span class="string">&quot;出现错误&quot;</span>) </span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(img_path):</span><br><span class="line">            print(<span class="string">&#x27;&#123;&#125; is not exist!&#x27;</span>.<span class="built_in">format</span>(img_path))</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="comment"># view_bar(&#x27;Conversion progress&#x27;, count + 1,</span></span><br><span class="line">        <span class="comment">#          len(glob.glob(xml_path + &#x27;/*.xml&#x27;)))</span></span><br><span class="line">    print(plane_data)</span><br><span class="line">    np.save(<span class="string">&quot;plane_data.npy&quot;</span>,plane_data)</span><br><span class="line">    <span class="comment">#b = np.load(&quot;filename.npy&quot;)</span></span><br><span class="line">    print(<span class="string">&#x27;\nStatistics is complete!&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> plane_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    image_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1_plane/images&quot;</span></span><br><span class="line">    xml_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1_plane/labelXmls&quot;</span></span><br><span class="line"></span><br><span class="line">    information_statistics_save(image_path,xml_path)</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
  <entry>
    <title>数据集筛选及其xml格式的label修改</title>
    <url>/2021/08/18/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%AD%9B%E9%80%89%E5%8F%8A%E5%85%B6xml%E6%A0%BC%E5%BC%8F%E7%9A%84label%E4%BF%AE%E6%94%B9/</url>
    <content><![CDATA[<p>组委会先发布了一部分的数据，去年星图杯是用于飞机目标追踪的，由于今年的飞机种类和去年几乎一致（多了C919,不过后续的数据信息统计可以发现就几张，先扔掉），所以想先选出飞机的图片和xml的label来，我就写了个文件来选出飞机和对应的修改后的xml，需要考虑的时候，通过xml的objects/object/possibleresult/name来进行判别是否是飞机。由于只想留下飞机，所以需要做如下两点：</p>
<a id="more"></a>
<div class="table-container">
<table>
<thead>
<tr>
<th>1、通过name来看是否是飞机，不是则通过一些方法删除该object</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>2、检测删除完后的objects是否为空，非空说明xml有飞机，保存该xml和对应的tif</strong></td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    把gf2021的数据集中的飞机都选出来，即一张图片中若含有飞机模型的，则选出来，同时需要修改xml文件，将</span></span><br><span class="line"><span class="string">    objects属性中的非飞机的object去掉，由于删除的非annotation直系节点，需要转换一下才能删除。</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.misc <span class="keyword">as</span> misc</span><br><span class="line"><span class="keyword">from</span> xml.dom.minidom <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy, cv2</span><br><span class="line"><span class="comment"># import imageio</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">from</span> xml.etree.ElementTree <span class="keyword">import</span> ElementTree,Element</span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> osgeo <span class="keyword">import</span> gdal, gdalconst</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_plane</span>(<span class="params">image_path, xml_path,image_save_path,xml_save_path</span>):</span></span><br><span class="line">    plane_label = [<span class="string">&quot;Boeing737&quot;</span>,<span class="string">&quot;Boeing747&quot;</span>,<span class="string">&quot;Boeing777&quot;</span>,<span class="string">&quot;C919&quot;</span>,<span class="string">&quot;A220&quot;</span>,<span class="string">&quot;A321&quot;</span>,<span class="string">&quot;A330&quot;</span>,<span class="string">&quot;A350&quot;</span>,<span class="string">&quot;ARJ21&quot;</span>,<span class="string">&quot;other-airplane&quot;</span>]</span><br><span class="line">    <span class="keyword">for</span> count, xml <span class="keyword">in</span> <span class="built_in">enumerate</span>(glob.glob(xml_path + <span class="string">&#x27;/*.xml&#x27;</span>)):<span class="comment">#进行索引，*代表任意匹配，得到可迭代对象</span></span><br><span class="line">        <span class="comment">#flag =0</span></span><br><span class="line">        <span class="comment"># to avoid path error in different development platform</span></span><br><span class="line">        xml = xml.replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>)<span class="comment">#用/替换\\,后面的替换前面的</span></span><br><span class="line">        (_, xml_name) = os.path.split(xml)<span class="comment">#分割。返回路径和文件名。若本身就是一个文件夹路径，则返回路径和空</span></span><br><span class="line">        <span class="comment">#print(xml,xml_name)</span></span><br><span class="line"></span><br><span class="line">        img_name=xml_name.replace(<span class="string">&#x27;.xml&#x27;</span>, <span class="string">&#x27;.tif&#x27;</span>)</span><br><span class="line">        img_path = image_path + <span class="string">&#x27;/&#x27;</span> + img_name</span><br><span class="line">        <span class="comment">#1. 读取xml文件</span></span><br><span class="line">        tree = ElementTree()</span><br><span class="line">        tree.parse(xml)</span><br><span class="line">        root = tree.getroot()</span><br><span class="line"></span><br><span class="line">        <span class="comment">#A. 找到父节点</span></span><br><span class="line">        node_root  = root.findall( <span class="string">&quot;objects&quot;</span>)<span class="comment">#不能直接找object,直接找object，删掉的只是object，但是tree只能删除objects，没有用，所以进行替换</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> objects_list <span class="keyword">in</span> node_root:<span class="comment">#遍历objects,其实只有一个</span></span><br><span class="line">            <span class="keyword">for</span> object_list <span class="keyword">in</span> <span class="built_in">list</span>(objects_list):<span class="comment">#得到object,不加list竟然无法遍历，迷惑</span></span><br><span class="line">                <span class="comment">#print(object_list)</span></span><br><span class="line">                <span class="keyword">if</span> object_list.find(<span class="string">&quot;possibleresult/name&quot;</span>).text <span class="keyword">not</span> <span class="keyword">in</span> plane_label:</span><br><span class="line">                    <span class="comment">#print(&quot;成功移除&quot;)</span></span><br><span class="line">                    objects_list.remove(object_list)</span><br><span class="line">                    <span class="comment">#flag    =   1</span></span><br><span class="line">            <span class="comment">#替换直系节点objects</span></span><br><span class="line">            <span class="keyword">for</span> i,external_node <span class="keyword">in</span> <span class="built_in">enumerate</span>(root):</span><br><span class="line">                <span class="keyword">if</span> external_node.tag == <span class="string">&quot;objects&quot;</span>:<span class="comment">#寻找objects节点</span></span><br><span class="line">                    root[i]=objects_list </span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> objects_list:<span class="comment">#判断是否为空,空的list的bool属性为false.非空的为true</span></span><br><span class="line">            <span class="comment">#if flag == 1:</span></span><br><span class="line">            <span class="comment">#    print(xml,object_list.find(&quot;possibleresult/name&quot;).text,objects_list.find(&quot;object/possibleresult/name&quot;).text)</span></span><br><span class="line">            <span class="comment">#存储xml</span></span><br><span class="line">            xml_save=xml_save_path+<span class="string">&quot;/&quot;</span>+xml_name</span><br><span class="line">            tree.write(xml_save, encoding=<span class="string">&quot;utf-8&quot;</span>,xml_declaration=<span class="literal">True</span>)   </span><br><span class="line">            <span class="comment">#存储图片</span></span><br><span class="line">            shutil.copyfile(img_path, image_save_path+<span class="string">&quot;/&quot;</span>+img_name)<span class="comment">#将第一个参数的文件复制到第二个里面            </span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(img_path):</span><br><span class="line">            print(<span class="string">&#x27;&#123;&#125; is not exist!&#x27;</span>.<span class="built_in">format</span>(img_path))</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># view_bar(&#x27;Conversion progress&#x27;, count + 1,</span></span><br><span class="line">        <span class="comment">#          len(glob.glob(xml_path + &#x27;/*.xml&#x27;)))</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;\nStatistics is complete!&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    image_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1/images&quot;</span></span><br><span class="line">    xml_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1/labelXmls&quot;</span></span><br><span class="line"></span><br><span class="line">    image_save_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1_plane/images&quot;</span></span><br><span class="line">    xml_save_path=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1_plane/labelXmls&quot;</span></span><br><span class="line"></span><br><span class="line">    choose_plane(image_path,xml_path,image_save_path,xml_save_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>其主体就是choose_plane,来分析一下choose_plane：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plane_label = [<span class="string">&quot;Boeing737&quot;</span>,<span class="string">&quot;Boeing747&quot;</span>,<span class="string">&quot;Boeing777&quot;</span>,<span class="string">&quot;C919&quot;</span>,<span class="string">&quot;A220&quot;</span>,<span class="string">&quot;A321&quot;</span>,<span class="string">&quot;A330&quot;</span>,<span class="string">&quot;A350&quot;</span>,<span class="string">&quot;ARJ21&quot;</span>,<span class="string">&quot;other-airplane&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>这一句是用来后续判别xml的objects/object/possibleresult/name的属性值是否是飞机的，只要判断name的属性值是否在该列表中即可。</p>
<p>然后循环进行读取文件夹下的xml文件，通过通过split找到文件名、文件路径，由此通过replace即可得到对应的image路径。</p>
<p>然后读取xml文件，代码如下，其中findall用于寻找annotation的直系子节点或者下属节点，也可以传入”objects/object”，其返回的是一个列表，列表中的元素是objects，我们知道objects只有一个，所以其实node_root[0]就是objects，用for循环也可以(本代码是用for来的)。其实我们希望删除的是object，为什么不findall(“objects/object”)呢？因为我们如果寻找object的话，的确可以很好的删除，但是删除了object后，原来的root是没变的，所以需要把删除后的替换进去，如果直接root[3]赋值我们删除后的object的话(root[3]是objects)，objects就变成object了，所以这是不对的。如果我们直接寻找出来的就是objects，那么再往下找object，删除objects中的object，然后用该objects替换root中的objects，就可以完美解决。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_name=xml_name.replace(<span class="string">&#x27;.xml&#x27;</span>, <span class="string">&#x27;.tif&#x27;</span>)</span><br><span class="line">img_path = image_path + <span class="string">&#x27;/&#x27;</span> + img_name</span><br><span class="line"><span class="comment">#1. 读取xml文件</span></span><br><span class="line">tree = ElementTree()</span><br><span class="line">tree.parse(xml)</span><br><span class="line">root = tree.getroot()</span><br><span class="line"></span><br><span class="line"><span class="comment">#A. 找到父节点</span></span><br><span class="line">node_root  = root.findall( <span class="string">&quot;objects&quot;</span>)<span class="comment">#不能直接找object,直接找object，删掉的只是object，但是tree只能删除objects，没有用，所以进行替换</span></span><br></pre></td></tr></table></figure>
<p>上面已经说了如何删除object了，那么删除后，需要对root中的objects进行替换，一般objects是root的第四个属性，就是root[3]，但是也可能不是，所以就用到了下面这个循环了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#替换直系节点objects</span></span><br><span class="line"><span class="keyword">for</span> i,external_node <span class="keyword">in</span> <span class="built_in">enumerate</span>(root):</span><br><span class="line">    <span class="keyword">if</span> external_node.tag == <span class="string">&quot;objects&quot;</span>:<span class="comment">#寻找objects节点</span></span><br><span class="line">        root[i]=objects_list </span><br></pre></td></tr></table></figure>
<p>最后需要判断一下删除后的objects是否为空，如果空了，说明没有飞机，如果不是空(不是空的列表的bool值是true)，则存储替换objects后的root和对应的tif，存储的文件夹自行指定;</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> objects_list:<span class="comment">#判断是否为空,空的list的bool属性为false.非空的为true</span></span><br><span class="line">    <span class="comment">#if flag == 1:</span></span><br><span class="line">    <span class="comment">#    print(xml,object_list.find(&quot;possibleresult/name&quot;).text,objects_list.find(&quot;object/possibleresult/name&quot;).text)</span></span><br><span class="line">    <span class="comment">#存储xml</span></span><br><span class="line">    xml_save=xml_save_path+<span class="string">&quot;/&quot;</span>+xml_name</span><br><span class="line">    tree.write(xml_save, encoding=<span class="string">&quot;utf-8&quot;</span>,xml_declaration=<span class="literal">True</span>)   </span><br><span class="line">    <span class="comment">#存储图片</span></span><br><span class="line">    shutil.copyfile(img_path, image_save_path+<span class="string">&quot;/&quot;</span>+img_name)<span class="comment">#将第一个参数的文件复制到第二个里面     </span></span><br></pre></td></tr></table></figure>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>星图杯</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>-星图杯 -r3det -python -xml</tag>
      </tags>
  </entry>
  <entry>
    <title>python常用语法汇总--不断更新中</title>
    <url>/2021/08/17/python%E5%B8%B8%E7%94%A8%E8%AF%AD%E6%B3%95%E6%B1%87%E6%80%BB/</url>
    <content><![CDATA[<p>之前也总结过一个，faster rcnn时候总结的，详见链接如下：</p>
<a id="more"></a>
<h2 id="R3det"><a href="#R3det" class="headerlink" title="R3det"></a>R3det</h2><h3 id="字符串类："><a href="#字符串类：" class="headerlink" title="字符串类："></a>字符串类：</h3><p>path前面加r含义</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">path=<span class="string">r&quot;C:\data\a.csv&quot;</span></span><br><span class="line">path=path.replace(<span class="string">&quot;\\&quot;</span>,<span class="string">&quot;/&quot;</span>)</span><br><span class="line">a=pd.read_csv(path)</span><br></pre></td></tr></table></figure>
<p>字符串是否在字符串中：</p>
<p>“tif” in “0.tif”的结果是True</p>
<p>strip</p>
<p>os.path.split</p>
<h3 id="列表类："><a href="#列表类：" class="headerlink" title="列表类："></a>列表类：</h3><p>判断是否在列表中，并获取下标，例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">animal = <span class="string">&quot;dog&quot;</span></span><br><span class="line">animals= [<span class="string">&quot;cat&quot;</span>,<span class="string">&quot;dog&quot;</span>,<span class="string">&quot;pig&quot;</span>]</span><br><span class="line"><span class="keyword">if</span> animal <span class="keyword">in</span> animals:</span><br><span class="line">    index=animals.index(animal)</span><br><span class="line">    print(index)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>列表之前说过，其乘法和加法是区别于np的，其实拼接的意思，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">print(a+b)</span><br><span class="line">print(a*<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<h3 id="np数组："><a href="#np数组：" class="headerlink" title="np数组："></a>np数组：</h3><p>保存/读取数组的方法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.save(<span class="string">&quot;filename.npy&quot;</span>,a)<span class="comment">#保存数组a，保存为的文件名是filename.npy</span></span><br><span class="line">b = np.load(<span class="string">&quot;filename.npy&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>float可以对单个数来，对[“123”,”234”]的，应该用np.float32()进行,例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">digit=[<span class="string">&quot;123&quot;</span>,<span class="string">&quot;456&quot;</span>,<span class="string">&quot;22&quot;</span>]</span><br><span class="line">digit_float=np.float32(digit)</span><br><span class="line">print(digit_float)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">123.</span> <span class="number">456.</span>  <span class="number">22.</span>]</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>xml格式转json</title>
    <url>/2021/08/16/xml%E6%A0%BC%E5%BC%8F%E8%BD%ACjson/</url>
    <content><![CDATA[<p>很多目标追踪给的数据集给的label都是xml格式的文件，labelme可视化工具需要的格式是json格式，所以需要对xml格式进行转换成json，师兄给了以前的代码，但是跑不通，下面便来具体解析一下这个代码并进行修改。</p>
<a id="more"></a>
<p>首先来看一下xml格式的label文件</p>
<p><a href="https://imgtu.com/i/fgg6f0"><img src="https://z3.ax1x.com/2021/08/15/fgg6f0.png" alt="fgg6f0.png"></a></p>
<p>那么下面附一下读取xml文件的函数。通过ET。parse可以将xml格式文件解析为元素树(一个个节点接下去)，然后通过tree.getroot()得到元素树的根节点，根节点中的元素就是最外层的几个参数，即source、research、size和objects，其实这些参数里面有存储了下一层节点参数，本质都是可迭代对象。可以来测试一下，通过list来转换一下可迭代对象：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> child_of_root <span class="keyword">in</span> root:</span><br><span class="line">    print(child_of_root.tag)</span><br><span class="line">    print(<span class="built_in">list</span>(child_of_root))</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出根节点包含的的确是那四个，而那四个又是可迭代对象，而其中包含的参数，即是下一层的节点参数。其中child_of_root.tag可以获取当前的元素名称，其实tag类似于字典里的key，而text类似于字典里的value。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">source</span><br><span class="line">[&lt;Element <span class="string">&#x27;filename&#x27;</span> at <span class="number">0x000001CB382693B0</span>&gt;, &lt;Element <span class="string">&#x27;origin&#x27;</span> at <span class="number">0x000001CB392C7BD0</span>&gt;]</span><br><span class="line">research</span><br><span class="line">[&lt;Element <span class="string">&#x27;version&#x27;</span> at <span class="number">0x000001CB392C78B0</span>&gt;, &lt;Element <span class="string">&#x27;provider&#x27;</span> at <span class="number">0x000001CB392C71D0</span>&gt;, &lt;Element <span class="string">&#x27;author&#x27;</span> at <span class="number">0x000001CB392C7770</span>&gt;, &lt;Element <span class="string">&#x27;pluginname&#x27;</span> at <span class="number">0x000001CB392C7C20</span>&gt;, &lt;Element <span class="string">&#x27;pluginclass&#x27;</span> at <span class="number">0x000001CB392C7360</span>&gt;, &lt;Element <span class="string">&#x27;time&#x27;</span> at <span class="number">0x000001CB392C7DB0</span>&gt;]</span><br><span class="line">size</span><br><span class="line">[&lt;Element <span class="string">&#x27;width&#x27;</span> at <span class="number">0x000001CB392C7680</span>&gt;, &lt;Element <span class="string">&#x27;height&#x27;</span> at <span class="number">0x000001CB392C77C0</span>&gt;, &lt;Element <span class="string">&#x27;depth&#x27;</span> at <span class="number">0x000001CB392C7EF0</span>&gt;]</span><br><span class="line">objects</span><br><span class="line">[&lt;Element <span class="string">&#x27;object&#x27;</span> at <span class="number">0x000001CB392C7590</span>&gt;, &lt;Element <span class="string">&#x27;object&#x27;</span> at <span class="number">0x000001CB392C7720</span>&gt;]</span><br></pre></td></tr></table></figure>
<p>而下述代码中，根节点中，if只判断了source和objects，没有使用size，那么后面转换成json的话，必然需要读取图片得到长宽和通道数，其实不太合适，可以自行加上几个if，通过不断的for产生可迭代对象，进行遍历元素树。我们仔细看一下objects元素，遍历后，只处理其中的object。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> scipy.misc <span class="keyword">as</span> misc</span><br><span class="line"><span class="keyword">from</span> xml.dom.minidom <span class="keyword">import</span> Document</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy, cv2</span><br><span class="line"><span class="comment"># import imageio</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> osgeo <span class="keyword">import</span> gdal, gdalconst</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_xml_gtbox_and_label</span>(<span class="params">xml_path</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param xml_path: the path of voc xml</span></span><br><span class="line"><span class="string">    :return: a list contains gtboxes and labels, shape is [num_of_gtboxes, 9],</span></span><br><span class="line"><span class="string">           and has [x1, y1, x2, y2, x3, y3, x4, y4, label] in a per row</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    tree = ET.parse(xml_path)</span><br><span class="line">    root = tree.getroot()</span><br><span class="line">    box_list = []</span><br><span class="line">    difficult_list = []</span><br><span class="line">    isObjNone=<span class="literal">True</span></span><br><span class="line">    tmp_score_list=[]</span><br><span class="line">    <span class="keyword">for</span> child_of_root <span class="keyword">in</span> root:</span><br><span class="line">        <span class="comment"># if child_of_root.tag == &#x27;filename&#x27;:</span></span><br><span class="line">        <span class="comment">#     assert child_of_root.text == xml_path.split(&#x27;/&#x27;)[-1].split(&#x27;.&#x27;)[0] \</span></span><br><span class="line">        <span class="comment">#                                  + FLAGS.img_format, &#x27;xml_name and img_name cannot match&#x27;</span></span><br><span class="line">        <span class="keyword">if</span> child_of_root.tag == <span class="string">&#x27;source&#x27;</span>:</span><br><span class="line">            <span class="keyword">for</span> child_item <span class="keyword">in</span> child_of_root:</span><br><span class="line">                <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;filename&#x27;</span>: </span><br><span class="line">                    img_name = child_item.text</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> child_of_root.tag == <span class="string">&#x27;objects&#x27;</span>:</span><br><span class="line">            label = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> child_item <span class="keyword">in</span> child_of_root:</span><br><span class="line">                <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">                    <span class="keyword">for</span> child_pic <span class="keyword">in</span> child_item:</span><br><span class="line">                        <span class="keyword">if</span> child_pic.tag == <span class="string">&#x27;possibleresult&#x27;</span>:</span><br><span class="line">                            <span class="keyword">for</span> child_name <span class="keyword">in</span> child_pic:</span><br><span class="line">                                <span class="keyword">if</span> child_name.tag == <span class="string">&#x27;name&#x27;</span>:</span><br><span class="line">                                    label = child_name.text</span><br><span class="line">                                <span class="keyword">if</span> child_name.tag == <span class="string">&#x27;probability&#x27;</span>:</span><br><span class="line">                                    tmp_score=<span class="built_in">float</span>(child_name.text)</span><br><span class="line">                                    tmp_score_list.append(tmp_score)</span><br><span class="line">                                    <span class="comment"># difficult_tmp = int(not(child_name.text))</span></span><br><span class="line">                                    tmp_list=re.findall(<span class="string">&#x27;[^.\d]&#x27;</span>,child_name.text)</span><br><span class="line">                                    <span class="keyword">if</span> <span class="built_in">len</span>(tmp_list)&gt;<span class="number">0</span>:</span><br><span class="line">                                        print(xml_path)</span><br><span class="line">                                        print(tmp_list)</span><br><span class="line">                                    <span class="keyword">if</span> child_name.text.strip()==<span class="string">&#x27;&#x27;</span>:</span><br><span class="line">                                        print(xml_path)</span><br><span class="line">                                        print(<span class="string">&#x27;empty&#x27;</span>)</span><br><span class="line">                                    difficult_tmp = <span class="number">0</span><span class="comment">#int(not(child_name.text))</span></span><br><span class="line">                                    difficult_list.append(difficult_tmp)</span><br><span class="line">                        <span class="keyword">if</span> child_pic.tag == <span class="string">&#x27;points&#x27;</span>:</span><br><span class="line">                            tmp_box = []</span><br><span class="line">                            <span class="keyword">for</span> node <span class="keyword">in</span> child_pic:</span><br><span class="line">                                tmp_list=re.findall(<span class="string">&#x27;[^-,.\d]&#x27;</span>,node.text)</span><br><span class="line">                                <span class="keyword">if</span> <span class="built_in">len</span>(tmp_list)&gt;<span class="number">0</span>:</span><br><span class="line">                                    print(xml_path)</span><br><span class="line">                                    print(tmp_list)</span><br><span class="line">                                <span class="keyword">if</span> node.text.strip()==<span class="string">&#x27;&#x27;</span>:</span><br><span class="line">                                    print(xml_path)</span><br><span class="line">                                    print(<span class="string">&#x27;empty&#x27;</span>)</span><br><span class="line">                            <span class="keyword">for</span> node <span class="keyword">in</span> child_pic[:<span class="number">4</span>]:</span><br><span class="line">                                tmp_box=tmp_box+node.text.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">                                isObjNone=<span class="literal">False</span></span><br><span class="line">                            tmp_box.append(label)</span><br><span class="line">                            box_list.append(tmp_box)</span><br><span class="line">    <span class="keyword">if</span> isObjNone:</span><br><span class="line">        print(xml_path)</span><br><span class="line">        print(<span class="string">&#x27;obj none!&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img_name, box_list, difficult_list, tmp_score_list</span><br></pre></td></tr></table></figure>
<p>我们将objects一块拿出来，见下述的#1的代码，for遍历，如果遍历的元素是object的，那么继续遍历处理，然后我们希望处理的是object中的possibleresult:这里面是包含所属类别和置信度(分类)和points:里面包含了追踪目标的四个角点(其实都是有五队，形成来一个收尾连接的闭合空间，我们只需要四对即四队角点即可)。</p>
<p>先看处理possibleresult的，里面会保存元素name中的参数，即label的类别；然后判断力probability，这个是置信度，child_name.text得到分数后，将分数加入列表中，不过后面对child_name.text进行了一个正则表达式匹配(和深度学习中的正则化区别，深度学习中的正则化是为了加入一些约束防止过拟合)，re.findall(‘[^.\d]‘,child_name.text)是前面第一个元素进行对后面的字符串进行匹配，[]代表系列组合，比如[0-99],就会匹配0、1、2。。。99的元素。而^可以理解为取反，[abc]是匹配a、b和c的字符，即[^abc ]即匹配除了abc的所有字符，这里的\d代表所有数字，^\d代表匹配除了所有数字以外的字符，“.”的意思是匹配除了换行的所有字符串，但是这里前面加了^则不是这个含义了，这里的含义就是简单的浮点数的点点，^.是不匹配浮点数的点点的意思。这个正则表达式是用于检测置信度是否是浮点数的，如果有其他字符，则会检测出来。</p>
<p>正则表达式参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.runoob.com/python/python-reg-expressions.html">Python 正则表达式</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://docs.python.org/zh-cn/3/library/re.html#re.MULTILINE">re-正则表达式操作</a></p>

          </blockquote>
<p>下一个if child_name.text.strip()==’’是用于判断字符串(置信度也是以字符串形式存储的)是否为空，strip函数是用来移除字符串头尾指定的字符（默认为空格）或字符序列，需要注意的是，只移除开头和结尾的指定字符。这里就是默认移除空格，然后看其是否为空。</p>
<p>probability判断完后，开始检测points，就是坐标，依旧是正则表达式，不匹配负号、逗号、点和数字，如果还有其他字符，则认为有问题。第二个for用来遍历取出所有的坐标，通过遍历前四组坐标（第五组和第一组重合了），然后用split进行分割，分割的界限以”,”进行，然后把结果加入list列表，list列表的加法是拼接的意思，乘法是重复拼接的意思，区别于np的操作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1        </span></span><br><span class="line">    	<span class="keyword">if</span> child_of_root.tag == <span class="string">&#x27;objects&#x27;</span>:</span><br><span class="line">            label = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> child_item <span class="keyword">in</span> child_of_root:</span><br><span class="line">                <span class="keyword">if</span> child_item.tag == <span class="string">&#x27;object&#x27;</span>:</span><br><span class="line">                    <span class="keyword">for</span> child_pic <span class="keyword">in</span> child_item:</span><br><span class="line">                        <span class="keyword">if</span> child_pic.tag == <span class="string">&#x27;possibleresult&#x27;</span>:</span><br><span class="line">                            <span class="keyword">for</span> child_name <span class="keyword">in</span> child_pic:</span><br><span class="line">                                <span class="keyword">if</span> child_name.tag == <span class="string">&#x27;name&#x27;</span>:</span><br><span class="line">                                    label = child_name.text</span><br><span class="line">                                <span class="keyword">if</span> child_name.tag == <span class="string">&#x27;probability&#x27;</span>:</span><br><span class="line">                                    tmp_score=<span class="built_in">float</span>(child_name.text)</span><br><span class="line">                                    tmp_score_list.append(tmp_score)</span><br><span class="line">                                    <span class="comment"># difficult_tmp = int(not(child_name.text))</span></span><br><span class="line">                                    tmp_list=re.findall(<span class="string">&#x27;[^.\d]&#x27;</span>,child_name.text)</span><br><span class="line">                                    <span class="keyword">if</span> <span class="built_in">len</span>(tmp_list)&gt;<span class="number">0</span>:</span><br><span class="line">                                        print(xml_path)</span><br><span class="line">                                        print(tmp_list)</span><br><span class="line">                                    <span class="keyword">if</span> child_name.text.strip()==<span class="string">&#x27;&#x27;</span>:</span><br><span class="line">                                        print(xml_path)</span><br><span class="line">                                        print(<span class="string">&#x27;empty&#x27;</span>)</span><br><span class="line">                                    difficult_tmp = <span class="number">0</span><span class="comment">#int(not(child_name.text))</span></span><br><span class="line">                                    difficult_list.append(difficult_tmp)</span><br><span class="line">                        <span class="keyword">if</span> child_pic.tag == <span class="string">&#x27;points&#x27;</span>:</span><br><span class="line">                            tmp_box = []</span><br><span class="line">                            <span class="keyword">for</span> node <span class="keyword">in</span> child_pic:</span><br><span class="line">                                tmp_list=re.findall(<span class="string">&#x27;[^-,.\d]&#x27;</span>,node.text)</span><br><span class="line">                                <span class="keyword">if</span> <span class="built_in">len</span>(tmp_list)&gt;<span class="number">0</span>:</span><br><span class="line">                                    print(xml_path)</span><br><span class="line">                                    print(tmp_list)</span><br><span class="line">                                <span class="keyword">if</span> node.text.strip()==<span class="string">&#x27;&#x27;</span>:</span><br><span class="line">                                    print(xml_path)</span><br><span class="line">                                    print(<span class="string">&#x27;empty&#x27;</span>)</span><br><span class="line">                            <span class="keyword">for</span> node <span class="keyword">in</span> child_pic[:<span class="number">4</span>]:</span><br><span class="line">                                tmp_box=tmp_box+node.text.split(<span class="string">&#x27;,&#x27;</span>)</span><br><span class="line">                                isObjNone=<span class="literal">False</span></span><br><span class="line">                            tmp_box.append(label)</span><br><span class="line">                            box_list.append(tmp_box)</span><br></pre></td></tr></table></figure>
<p>下面看一下把xml转换成json格式的函数#2，这个函数中调用了上述描述的函数。代码如下：</p>
<p>这里需要讲一下这个python中的路径问题，python中用的是\作为路径文件夹相隔符，然而window中常常用\（ubuntu是/），而\这个字符是转义字符，用来作为程序中的文件路径肯定是不合适的，所以一般都是用/，所以在路径使用之前，先进行检测替换一下，用/替换\,即用到了replace函数，似乎应该用replace(“\\”,”/“),但是\\是转义字符的意思，所以需要使用\\\,这样才表示window中路径斜杠。</p>
<p>save_path赋值时候，前面加了r，对于这些路径加上r有啥含义呢？如果我们的路径直接从windows拷贝过来的，格式都是类似于C:\Users\这样的，直接作为字符串赋值，\会作为转义字符直接被忽略掉，要么变成\\\\这样，人为修改太麻烦，加上一个r后，强制把后面的这字符串强制存为字符串，即\直接就是真的字符\了，而不是转移字符。</p>
<p>下述函数中用到了os.path.split，其作用是，如果你传入一个文件路径，它会返回元组，包括两个参数，第一个是路径(精确到文件夹)，第二个是文件名，比如os.path.split(“C:\\Users\\21311\\machine_learning\\tensorflow\\xingtubei\\1.tif”)，返回(‘C:\\Users\\21311\\machine_learning\\tensorflow\\xingtubei’, ‘1.tif’)。然后下面调用了上面分析的解析xml的函数，由于解析xml的时候，没有解析size，所以不知道长宽和深度，所以这里用gdal库读入了图片，获取了width和length，这里显然是更浪费内存的，其实直接解析xml就可以得到了。所谓的json格式可以就看做为一个字典，字典里面可以嵌套字典，key值需要按照规定的来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_pascal_to_tfrecord</span>(<span class="params">image_path, xml_path</span>):</span></span><br><span class="line">    <span class="comment">#葛改save_path=r&#x27;/emwuser/gzl/gf2021/data/plane_2020/train_split_test/train/json_c&#x27;</span></span><br><span class="line">    save_path=<span class="string">r&#x27;/emwuser/gzl/gf2021/data/FAIR1M/train/part1/json_c&#x27;</span><span class="comment">#json的保存地址#r其实是为了强制认为后面的都是字符串，不考虑转义字符，无r则会考虑转义字符，这里加不加其实无所谓，但是C:\data这种的就得加上r，或者直接C:\\data</span></span><br><span class="line">    <span class="keyword">for</span> count, xml <span class="keyword">in</span> <span class="built_in">enumerate</span>(glob.glob(xml_path + <span class="string">&#x27;/*.xml&#x27;</span>)):<span class="comment">#进行索引，*代表任意匹配，得到可迭代对象</span></span><br><span class="line">        <span class="comment"># to avoid path error in different development platform</span></span><br><span class="line">        xml = xml.replace(<span class="string">&#x27;\\&#x27;</span>, <span class="string">&#x27;/&#x27;</span>)<span class="comment">#用/替换\\,即后面的替换前面的</span></span><br><span class="line">        (_, xml_name) = os.path.split(xml)<span class="comment">#分割。返回路径和文件名。若本身就是一个文件夹路径，则返回路径和空</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># img_name = xml.split(&#x27;/&#x27;)[-1].split(&#x27;.&#x27;)[0] + FLAGS.img_format</span></span><br><span class="line">        <span class="comment"># img_path = image_path + &#x27;/&#x27; + img_name</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># if not os.path.exists(img_path):</span></span><br><span class="line">        <span class="comment">#     print(&#x27;&#123;&#125; is not exist!&#x27;.format(img_path))</span></span><br><span class="line">        <span class="comment">#     continue</span></span><br><span class="line"></span><br><span class="line">        img_name, box , difficult_list, tmp_score_list = read_xml_gtbox_and_label(xml)</span><br><span class="line">        img_name=xml_name.replace(<span class="string">&#x27;.xml&#x27;</span>, <span class="string">&#x27;.tif&#x27;</span>)</span><br><span class="line">        img_path = image_path + <span class="string">&#x27;/&#x27;</span> + img_name</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(img_path):</span><br><span class="line">            print(<span class="string">&#x27;&#123;&#125; is not exist!&#x27;</span>.<span class="built_in">format</span>(img_path))</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        json_str=&#123;&#125;</span><br><span class="line">        shapes=[]</span><br><span class="line"></span><br><span class="line">        json_str[<span class="string">&#x27;version&#x27;</span>]=<span class="string">&quot;4.5.6&quot;</span></span><br><span class="line">        json_str[<span class="string">&#x27;flags&#x27;</span>]=&#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># json_str[&#x27;image_name&#x27;]=img_name</span></span><br><span class="line">        json_str[<span class="string">&#x27;imagePath&#x27;</span>]=os.path.join(<span class="string">&#x27;..&#x27;</span>,<span class="string">&#x27;images_c&#x27;</span>,img_name)</span><br><span class="line">        <span class="comment"># img_data = cv2.imread(json_str[&#x27;imagePath&#x27;])</span></span><br><span class="line">        json_str[<span class="string">&#x27;imageData&#x27;</span>]= <span class="literal">None</span></span><br><span class="line">        dataset = gdal.Open(os.path.join(image_path, img_name))</span><br><span class="line">        json_str[<span class="string">&#x27;imageHeight&#x27;</span>]=dataset.RasterYSize<span class="comment">#img_data.shape[0]</span></span><br><span class="line">        json_str[<span class="string">&#x27;imageWidth&#x27;</span>]=dataset.RasterXSize<span class="comment">#img_data.shape[1]</span></span><br><span class="line">        <span class="keyword">del</span> dataset</span><br><span class="line">        <span class="comment"># json_str[&#x27;image_depth&#x27;]=3</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(box)):</span><br><span class="line">            shape=&#123;&#125;</span><br><span class="line">            shape[<span class="string">&#x27;label&#x27;</span>]=box[i][-<span class="number">1</span>]</span><br><span class="line">            shape[<span class="string">&#x27;shape_type&#x27;</span>]=<span class="string">&quot;polygon&quot;</span></span><br><span class="line">            shape[<span class="string">&#x27;points&#x27;</span>]=[[box[i][<span class="number">0</span>],box[i][<span class="number">1</span>]],[box[i][<span class="number">2</span>],box[i][<span class="number">3</span>]],[box[i][<span class="number">4</span>],box[i][<span class="number">5</span>]],[box[i][<span class="number">6</span>],box[i][<span class="number">7</span>]]]</span><br><span class="line">            shape[<span class="string">&#x27;points&#x27;</span>]=np.array(shape[<span class="string">&#x27;points&#x27;</span>],dtype=np.<span class="built_in">float</span>)</span><br><span class="line">            shape[<span class="string">&#x27;points&#x27;</span>]=shape[<span class="string">&#x27;points&#x27;</span>].tolist()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># labelme 特有</span></span><br><span class="line">            shape[<span class="string">&#x27;group_id&#x27;</span>]= <span class="literal">None</span></span><br><span class="line">            shape[<span class="string">&#x27;flags&#x27;</span>]=&#123;&#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 图像标识平台 特有</span></span><br><span class="line">            shape[<span class="string">&#x27;score&#x27;</span>]=tmp_score_list[i]</span><br><span class="line">            shape[<span class="string">&#x27;difficult&#x27;</span>]=<span class="number">0</span><span class="comment">#difficult_list[i]#20200711</span></span><br><span class="line">            shape[<span class="string">&#x27;truncated&#x27;</span>]=<span class="number">0</span><span class="comment">#20200711</span></span><br><span class="line">            shape[<span class="string">&#x27;status&#x27;</span>]=<span class="string">&quot;add&quot;</span></span><br><span class="line"></span><br><span class="line">            shapes.append(shape)</span><br><span class="line">        json_str[<span class="string">&#x27;shapes&#x27;</span>]=shapes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(save_path, xml_name.replace(<span class="string">&#x27;xml&#x27;</span>, <span class="string">&#x27;json&#x27;</span>)), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fw:</span><br><span class="line">            json.dump(json_str, fw)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># view_bar(&#x27;Conversion progress&#x27;, count + 1,</span></span><br><span class="line">        <span class="comment">#          len(glob.glob(xml_path + &#x27;/*.xml&#x27;)))</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;\nConversion is complete!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>下面来看一下主函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    raw_label_dir=r&quot;/emwuser/gzl/gf2021/data/plane_2020/train_split_test/train/label_xml_c&quot;</span></span><br><span class="line"><span class="string">    raw_images_dir=r&#x27;/emwuser/gzl/gf2021/data/plane_2020/train_split_test/train/images_c&#x27;</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    raw_label_dir=<span class="string">r&quot;/emwuser/gzl/gf2021/data/FAIR1M/train/part1/labelXmls_c&quot;</span></span><br><span class="line">    raw_images_dir=<span class="string">r&#x27;/emwuser/gzl/gf2021/data/FAIR1M/train/part1/images_c&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># raw_label_dir=r&quot;/emwusr/jhc/data/plane/data/train/label_xml&quot;</span></span><br><span class="line">    <span class="comment"># raw_images_dir=r&#x27;/emwusr/jhc/data/plane/data/train/images&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#save_path=r&#x27;/emwusr/nuoyizhou/pickplanedataset/competition/data/train/json&#x27;</span></span><br><span class="line">    convert_pascal_to_tfrecord(raw_images_dir,raw_label_dir)</span><br></pre></td></tr></table></figure>
<p>其实可以发现其报错了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">&quot;/emwuser/gzl/gf2021/code/R3Det_Tensorflow_gf2021/tools_gf_2021/xml2json_zry.py&quot;</span>, line <span class="number">161</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    convert_pascal_to_tfrecord(raw_images_dir,raw_label_dir)</span><br><span class="line">  File <span class="string">&quot;/emwuser/gzl/gf2021/code/R3Det_Tensorflow_gf2021/tools_gf_2021/xml2json_zry.py&quot;</span>, line <span class="number">130</span>, <span class="keyword">in</span> convert_pascal_to_tfrecord</span><br><span class="line">    shape[<span class="string">&#x27;score&#x27;</span>]=tmp_score_list[i]</span><br><span class="line">IndexError: <span class="built_in">list</span> index out of <span class="built_in">range</span></span><br></pre></td></tr></table></figure>
<p>即tmp_score_list超界了，可以看一下该文最上面的xml文件，可以发现里面是没有置信度的参数，而解析xml文件的时候，如果有则写入list中，如果没有，则list就不写入，这显然是不合理的，没有的话我们应该进行填充默认值，作为给的训练集数据，那么置信度可以默认为1。我们可以在convert_pascal_to_tfrecord中进行判断一下，加入如下语句</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> tmp_score_list:</span><br><span class="line">    print(<span class="string">&quot;tmp_score_list是空的&quot;</span>)</span><br><span class="line">    tmp_score_list=[<span class="number">0</span>]*<span class="built_in">len</span>(box)</span><br><span class="line">    print(tmp_score_list)</span><br></pre></td></tr></table></figure>
<p>其中两个print可以屏蔽掉，因为循环的时候打印太多也没有意义。其实更好的应该是在read_xml_gtbox_and_label函数中判断probability那加入else，给予默认值。</p>
<p>填充好tmp_score_list后，就可以得到json格式的文件，我们可以用labelme进行打开，但是打开还是报错的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a byte-like <span class="built_in">object</span> <span class="keyword">is</span> required,<span class="keyword">not</span> “Nonetype”</span><br></pre></td></tr></table></figure>
<p>其实我们可以随便打开一张tif文件，然后进行标注，保存json格式的label，把json单独拿出，labelme打开json后，图像会自动显示，这个图像是json中路径寻找到的？你即使给它一个错误路径图片也可以正常显示，所以是json中存储了图片信息，在key为ImageData的value，然而我们生成json的时候，是用null填充的，所以是打不开的，不过这只是影响我们可视化而已，不影响我们图像处理，如果需要可视化的话，我们可以打开图片，随便标注一下，存下json，把这个json中的ImageData的value复制到我们函数产生的json即可。</p>
]]></content>
      <categories>
        <category>python</category>
        <category>xml</category>
        <category>json</category>
        <category>os库</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>xml</tag>
        <tag>json</tag>
        <tag>os库</tag>
      </tags>
  </entry>
  <entry>
    <title>r3det代码详解(一)</title>
    <url>/2021/08/07/r3net%E4%BB%A3%E7%A0%81%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>中图杯师兄给了去年的r3net代码，backbone用的是resnet，一堆堆代码垒起来了，假期还有时间，就慢慢看，看代码过程中，顺便把tf巩固一下。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Graph().as_default(), tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):<span class="comment">#指定一个默认的数据流图，在当前with外也不会关闭，指定使用的运算设备</span></span><br><span class="line">                                                        <span class="comment">#strip去掉字符串两边空格，split是分割字符串，返回列表，元素是分割的</span></span><br><span class="line">        num_gpu = <span class="built_in">len</span>(cfgs.GPU_GROUP.strip().split(<span class="string">&#x27;,&#x27;</span>))<span class="comment">#自定义包，包中都有init，这是必要的，GPU_GROUP是字符串，代表使用几个GPU，这里使用了2和3，其实可以使用四个的</span></span><br><span class="line">        global_step = slim.get_or_create_global_step()<span class="comment">#创建全局阶跃张量,2.3版本中说slim的这个函数未来会弃用，但是貌似建议使用的tf.train.get_or_create_global_step的已经在tf2.0被丢弃了，在compat.v1中了</span></span><br><span class="line">        lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;lr&#x27;</span>, lr)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;get_batch&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> cfgs.IMAGE_PYRAMID:</span><br><span class="line">                shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)</span><br><span class="line">                shortside_len = tf.random_shuffle(shortside_len_list)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                shortside_len = cfgs.IMG_SHORT_SIDE_LEN</span><br><span class="line"></span><br><span class="line">            img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \</span><br><span class="line">                next_batch(dataset_name=cfgs.DATASET_NAME,</span><br><span class="line">                           batch_size=cfgs.BATCH_SIZE * num_gpu,</span><br><span class="line">                           shortside_len=shortside_len,</span><br><span class="line">                           is_training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        optimizer = tf.train.MomentumOptimizer(lr, momentum=cfgs.MOMENTUM)</span><br><span class="line">        r3det_plusplus = build_whole_network_r3det_plusplus.DetectionNetwork(</span><br><span class="line">            base_network_name=cfgs.NET_NAME,</span><br><span class="line">            is_training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># data processing</span></span><br><span class="line">        inputs_list = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpu):</span><br><span class="line">            img = tf.expand_dims(img_batch[i], axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> cfgs.NET_NAME <span class="keyword">in</span> [<span class="string">&#x27;resnet152_v1d&#x27;</span>, <span class="string">&#x27;resnet101_v1d&#x27;</span>, <span class="string">&#x27;resnet50_v1d&#x27;</span>]:</span><br><span class="line">                img = img / tf.constant([cfgs.PIXEL_STD])</span><br><span class="line"></span><br><span class="line">            gtboxes_and_label_r = tf.py_func(backward_convert,</span><br><span class="line">                                             inp=[gtboxes_and_label_batch[i]],</span><br><span class="line">                                             Tout=tf.float32)</span><br><span class="line">            gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-<span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">            gtboxes_and_label_h = get_horizen_minAreaRectangle(gtboxes_and_label_batch[i])</span><br><span class="line">            gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">            num_objects = num_objects_batch[i]</span><br><span class="line">            num_objects = tf.cast(tf.reshape(num_objects, [-<span class="number">1</span>, ]), tf.float32)</span><br><span class="line"></span><br><span class="line">            img_h = img_h_batch[i]</span><br><span class="line">            img_w = img_w_batch[i]</span><br><span class="line"></span><br><span class="line">            inputs_list.append([img, gtboxes_and_label_h, gtboxes_and_label_r, num_objects, img_h, img_w])</span><br><span class="line"></span><br><span class="line">        tower_grads = []</span><br><span class="line">        biases_regularizer = tf.no_regularizer</span><br><span class="line">        <span class="comment"># weights_regularizer = tf.contrib.layers.l2_regularizer(cfgs.WEIGHT_DECAY)</span></span><br><span class="line">        weights_regularizer = tf.keras.regularizers.l2(cfgs.WEIGHT_DECAY)</span><br><span class="line"></span><br><span class="line">        total_loss_dict = &#123;</span><br><span class="line">            <span class="string">&#x27;cls_loss&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;reg_loss&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;refine_cls_loss&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;refine_reg_loss&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;refine_cls_loss_stage3&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;refine_reg_loss_stage3&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">            <span class="string">&#x27;total_losses&#x27;</span>: tf.constant(<span class="number">0.</span>, tf.float32),</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfgs.USE_SUPERVISED_MASK:</span><br><span class="line">            total_loss_dict[<span class="string">&#x27;mask_loss&#x27;</span>] = tf.constant(<span class="number">0.</span>, tf.float32)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(tf.get_variable_scope()):</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_gpu):</span><br><span class="line">                <span class="keyword">with</span> tf.device(<span class="string">&#x27;/gpu:%d&#x27;</span> % i):</span><br><span class="line">                    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;tower_%d&#x27;</span> % i):</span><br><span class="line">                        <span class="keyword">with</span> slim.arg_scope(</span><br><span class="line">                                [slim.model_variable, slim.variable],</span><br><span class="line">                                device=<span class="string">&#x27;/device:CPU:0&#x27;</span>):</span><br><span class="line">                            <span class="keyword">with</span> slim.arg_scope([slim.conv2d, slim.conv2d_in_plane,</span><br><span class="line">                                                 slim.conv2d_transpose, slim.separable_conv2d, slim.fully_connected],</span><br><span class="line">                                                weights_regularizer=weights_regularizer,</span><br><span class="line">                                                biases_regularizer=biases_regularizer,</span><br><span class="line">                                                biases_initializer=tf.constant_initializer(<span class="number">0.0</span>)):</span><br><span class="line"></span><br><span class="line">                                gtboxes_and_label_h, gtboxes_and_label_r = tf.py_func(get_gtboxes_and_label,</span><br><span class="line">                                                                                      inp=[inputs_list[i][<span class="number">1</span>],</span><br><span class="line">                                                                                           inputs_list[i][<span class="number">2</span>],</span><br><span class="line">                                                                                           inputs_list[i][<span class="number">3</span>]],</span><br><span class="line">                                                                                      Tout=[tf.float32, tf.float32])</span><br><span class="line">                                gtboxes_and_label_h = tf.reshape(gtboxes_and_label_h, [-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">                                gtboxes_and_label_r = tf.reshape(gtboxes_and_label_r, [-<span class="number">1</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line">                                img = inputs_list[i][<span class="number">0</span>]</span><br><span class="line">                                img_shape = inputs_list[i][-<span class="number">2</span>:]</span><br><span class="line">                                img = tf.image.crop_to_bounding_box(image=img,</span><br><span class="line">                                                                    offset_height=<span class="number">0</span>,</span><br><span class="line">                                                                    offset_width=<span class="number">0</span>,</span><br><span class="line">                                                                    target_height=tf.cast(img_shape[<span class="number">0</span>], tf.int32),</span><br><span class="line">                                                                    target_width=tf.cast(img_shape[<span class="number">1</span>], tf.int32))</span><br><span class="line"></span><br><span class="line">                                outputs = r3det_plusplus.build_whole_detection_network(input_img_batch=img,</span><br><span class="line">                                                                                       gtboxes_batch_h=gtboxes_and_label_h,</span><br><span class="line">                                                                                       gtboxes_batch_r=gtboxes_and_label_r,</span><br><span class="line">                                                                                       gpu_id=i)</span><br><span class="line">                                gtboxes_in_img_h = draw_boxes_with_categories(img_batch=img,</span><br><span class="line">                                                                              boxes=gtboxes_and_label_h[:, :-<span class="number">1</span>],</span><br><span class="line">                                                                              labels=gtboxes_and_label_h[:, -<span class="number">1</span>],</span><br><span class="line">                                                                              method=<span class="number">0</span>)</span><br><span class="line">                                gtboxes_in_img_r = draw_boxes_with_categories(img_batch=img,</span><br><span class="line">                                                                              boxes=gtboxes_and_label_r[:, :-<span class="number">1</span>],</span><br><span class="line">                                                                              labels=gtboxes_and_label_r[:, -<span class="number">1</span>],</span><br><span class="line">                                                                              method=<span class="number">1</span>)</span><br><span class="line">                                tf.summary.image(<span class="string">&#x27;Compare/gtboxes_h_gpu:%d&#x27;</span> % i, gtboxes_in_img_h)</span><br><span class="line">                                tf.summary.image(<span class="string">&#x27;Compare/gtboxes_r_gpu:%d&#x27;</span> % i, gtboxes_in_img_r)</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">if</span> cfgs.ADD_BOX_IN_TENSORBOARD:</span><br><span class="line">                                    detections_in_img = draw_boxes_with_categories_and_scores(</span><br><span class="line">                                        img_batch=img,</span><br><span class="line">                                        boxes=outputs[<span class="number">0</span>],</span><br><span class="line">                                        scores=outputs[<span class="number">1</span>],</span><br><span class="line">                                        labels=outputs[<span class="number">2</span>],</span><br><span class="line">                                        method=<span class="number">1</span>)</span><br><span class="line">                                    tf.summary.image(<span class="string">&#x27;Compare/final_detection_gpu:%d&#x27;</span> % i, detections_in_img)</span><br><span class="line"></span><br><span class="line">                                loss_dict = outputs[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                                total_losses = <span class="number">0.0</span></span><br><span class="line">                                <span class="keyword">for</span> k <span class="keyword">in</span> loss_dict.keys():</span><br><span class="line">                                    total_losses += loss_dict[k]</span><br><span class="line">                                    total_loss_dict[k] += loss_dict[k] / num_gpu</span><br><span class="line"></span><br><span class="line">                                total_losses = total_losses / num_gpu</span><br><span class="line">                                total_loss_dict[<span class="string">&#x27;total_losses&#x27;</span>] += total_losses</span><br><span class="line"></span><br><span class="line">                                <span class="keyword">if</span> i == num_gpu - <span class="number">1</span>:</span><br><span class="line">                                    regularization_losses = tf.get_collection(</span><br><span class="line">                                        tf.GraphKeys.REGULARIZATION_LOSSES)</span><br><span class="line">                                    <span class="comment"># weight_decay_loss = tf.add_n(slim.losses.get_regularization_losses())</span></span><br><span class="line">                                    total_losses = total_losses + tf.add_n(regularization_losses)</span><br><span class="line"></span><br><span class="line">                        tf.get_variable_scope().reuse_variables()</span><br><span class="line">                        grads = optimizer.compute_gradients(total_losses)</span><br><span class="line">                        <span class="keyword">if</span> cfgs.GRADIENT_CLIPPING_BY_NORM <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                            grads = slim.learning.clip_gradient_norms(grads, cfgs.GRADIENT_CLIPPING_BY_NORM)</span><br><span class="line">                        tower_grads.append(grads)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> total_loss_dict.keys():</span><br><span class="line">            tf.summary.scalar(<span class="string">&#x27;&#123;&#125;/&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(k.split(<span class="string">&#x27;_&#x27;</span>)[<span class="number">0</span>], k), total_loss_dict[k])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(tower_grads) &gt; <span class="number">1</span>:</span><br><span class="line">            grads = sum_gradients(tower_grads)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            grads = tower_grads[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfgs.MUTILPY_BIAS_GRADIENT <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            final_gvs = []</span><br><span class="line">            <span class="keyword">with</span> tf.variable_scope(<span class="string">&#x27;Gradient_Mult&#x27;</span>):</span><br><span class="line">                <span class="keyword">for</span> grad, var <span class="keyword">in</span> grads:</span><br><span class="line">                    scale = <span class="number">1.</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;/biases:&#x27;</span> <span class="keyword">in</span> var.name:</span><br><span class="line">                        scale *= cfgs.MUTILPY_BIAS_GRADIENT</span><br><span class="line">                    <span class="keyword">if</span> <span class="string">&#x27;conv_new&#x27;</span> <span class="keyword">in</span> var.name:</span><br><span class="line">                        scale *= <span class="number">3.</span></span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> np.allclose(scale, <span class="number">1.0</span>):</span><br><span class="line">                        grad = tf.multiply(grad, scale)</span><br><span class="line"></span><br><span class="line">                    final_gvs.append((grad, var))</span><br><span class="line">            apply_gradient_op = optimizer.apply_gradients(final_gvs, global_step=global_step)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)</span><br><span class="line"></span><br><span class="line">        variable_averages = tf.train.ExponentialMovingAverage(<span class="number">0.9999</span>, global_step)</span><br><span class="line">        variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line"></span><br><span class="line">        train_op = tf.group(apply_gradient_op, variables_averages_op)</span><br><span class="line">        <span class="comment"># train_op = optimizer.apply_gradients(final_gvs, global_step=global_step)</span></span><br><span class="line">        summary_op = tf.summary.merge_all()</span><br><span class="line"></span><br><span class="line">        restorer, restore_ckpt = r3det_plusplus.get_restorer()</span><br><span class="line">        saver = tf.train.Saver(max_to_keep=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">        init_op = tf.group(</span><br><span class="line">            tf.global_variables_initializer(),</span><br><span class="line">            tf.local_variables_initializer()</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        tfconfig = tf.ConfigProto(</span><br><span class="line">            allow_soft_placement=<span class="literal">True</span>, log_device_placement=<span class="literal">False</span>)</span><br><span class="line">        tfconfig.gpu_options.allow_growth = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">with</span> tf.Session(config=tfconfig) <span class="keyword">as</span> sess:</span><br><span class="line">            sess.run(init_op)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># sess.run(tf.initialize_all_variables())</span></span><br><span class="line">            coord = tf.train.Coordinator()</span><br><span class="line">            threads = tf.train.start_queue_runners(coord=coord, sess=sess)</span><br><span class="line"></span><br><span class="line">            summary_path = os.path.join(cfgs.SUMMARY_PATH, cfgs.VERSION)</span><br><span class="line">            tools.mkdir(summary_path)</span><br><span class="line">            summary_writer = tf.summary.FileWriter(summary_path, graph=sess.graph)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> restorer <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                restorer.restore(sess, restore_ckpt)</span><br><span class="line">                print(<span class="string">&#x27;restore model&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(cfgs.MAX_ITERATION // num_gpu):</span><br><span class="line">                training_time = time.strftime(<span class="string">&#x27;%Y-%m-%d %H:%M:%S&#x27;</span>, time.localtime(time.time()))</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> step % cfgs.SHOW_TRAIN_INFO_INTE != <span class="number">0</span> <span class="keyword">and</span> step % cfgs.SMRY_ITER != <span class="number">0</span>:</span><br><span class="line">                    _, global_stepnp = sess.run([train_op, global_step])</span><br><span class="line"></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> step % cfgs.SHOW_TRAIN_INFO_INTE == <span class="number">0</span> <span class="keyword">and</span> step % cfgs.SMRY_ITER != <span class="number">0</span>:</span><br><span class="line">                        start = time.time()</span><br><span class="line">                        _, global_stepnp, total_loss_dict_ = \</span><br><span class="line">                            sess.run([train_op, global_step, total_loss_dict])</span><br><span class="line"></span><br><span class="line">                        end = time.time()</span><br><span class="line"></span><br><span class="line">                        print(<span class="string">&#x27;***&#x27;</span>*<span class="number">20</span>)</span><br><span class="line">                        print(<span class="string">&quot;&quot;&quot;%s: global_step:%d  current_step:%d&quot;&quot;&quot;</span></span><br><span class="line">                              % (training_time, (global_stepnp-<span class="number">1</span>)*num_gpu, step*num_gpu))</span><br><span class="line">                        print(<span class="string">&quot;&quot;&quot;per_cost_time:%.3fs&quot;&quot;&quot;</span></span><br><span class="line">                              % ((end - start) / num_gpu))</span><br><span class="line">                        loss_str = <span class="string">&#x27;&#x27;</span></span><br><span class="line">                        <span class="keyword">for</span> k <span class="keyword">in</span> total_loss_dict_.keys():</span><br><span class="line">                            loss_str += <span class="string">&#x27;%s:%.3f\n&#x27;</span> % (k, total_loss_dict_[k])</span><br><span class="line">                        print(loss_str)</span><br><span class="line"></span><br><span class="line">                        <span class="keyword">if</span> np.isnan(total_loss_dict_[<span class="string">&#x27;total_losses&#x27;</span>]):</span><br><span class="line">                            sys.exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">if</span> step % cfgs.SMRY_ITER == <span class="number">0</span>:</span><br><span class="line">                            _, global_stepnp, summary_str = sess.run([train_op, global_step, summary_op])</span><br><span class="line">                            summary_writer.add_summary(summary_str, (global_stepnp-<span class="number">1</span>)*num_gpu)</span><br><span class="line">                            summary_writer.flush()</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (step &gt; <span class="number">0</span> <span class="keyword">and</span> step % (cfgs.SAVE_WEIGHTS_INTE // num_gpu) == <span class="number">0</span>) <span class="keyword">or</span> (step &gt;= cfgs.MAX_ITERATION // num_gpu - <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">                    save_dir = os.path.join(cfgs.TRAINED_CKPT, cfgs.VERSION)</span><br><span class="line">                    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(save_dir):</span><br><span class="line">                        os.mkdir(save_dir)</span><br><span class="line"></span><br><span class="line">                    save_ckpt = os.path.join(save_dir, <span class="string">&#x27;&#123;&#125;_&#x27;</span>.<span class="built_in">format</span>(cfgs.DATASET_NAME) +</span><br><span class="line">                                             <span class="built_in">str</span>((global_stepnp-<span class="number">1</span>)*num_gpu) + <span class="string">&#x27;model.ckpt&#x27;</span>)</span><br><span class="line">                    saver.save(sess, save_ckpt)</span><br><span class="line">                    print(<span class="string">&#x27; weights had been saved&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            coord.request_stop()</span><br><span class="line">            coord.join(threads)</span><br></pre></td></tr></table></figure>
<p>下面开始逐步解释：</p>
<p>1、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Graph().as_default(), tf.device(<span class="string">&#x27;/cpu:0&#x27;</span>):</span><br></pre></td></tr></table></figure>
<p>这里解释一下数据流图，学习神经网络的时候，经常会有绘制的多层感知机原理图，线代表数据流向，而圆圈就代表神经元，也就是运算单元，将数据流传输过去后，在神经元中得到运算。而tf中的数据流图和这一类的原理图颇为类似，在tf中，</p>
<p>TensorFlow计算，表示为数据流图。<br>TensorFlow 中的所有计算都会被转化为计算图上的节点。<br>TensorFlow 是一个通过计算图的形式来表述计算的编程系统。<br>TensorFlow中的每个计算都是计算图的一个节点，而节点之间的边描述了计算之间的依赖关系。<br>TensorFlow 的计算模型是有向图,采用数据流图 (Data Flow Graphs)，其中每个节点代表一些函数或计算，而边代表了数值、矩阵或张量</p>
<p>这样看来，是不是很像多层感知机的原理模型。</p>
<p>在模型任务开始的时候，系统就会指定一个数据流图作为默认图，而 tf.Graph()会返回一个新的数据流图，tf.Graph().as_default 这句话的含义是指定一个数据流图(a=tf.Graph()，a.as_default())作为当前上下文管理器中的默认图，上下文管理器就是with，其作用是即使出现错误异常也会进行退出操作，使得图间的运算分离，互不干扰，在with中的都认为在当前上下文管理之中，即运算会映射到当前设定的图中，如果不指定默认的数据流图，则会将所有操作都映射到系统的初始的默认图中。as_default的意义何在？as_default返回一个上下文管理器，with可以将内部内容作为上下文，但with只是指定当前内容作为其内容，with中的回话(即session，tf1.0中常用)只有在with中有效(with tf.session啥的)，一旦调用了函数跳出了with，会话自动关闭，只有指定了作为as_default，在跳出是才可以继续使用run和eval(run是tf1.0中手动运算需要的，1.0中定义操作和执行操作是分开的，而tf2.0中已经可以自动更新了)。tf.device是用于指定使用的运算设备。补充：tensor.graph可以得到该tensor使用的数据流图，tf.compat.v1.get_default_graph()可以获得当前的默认图（有compat.v1是为了tf1.0兼容tf2.0版本，该语句是2.0版本中的，等价于tf1.0版本中的tf.get_default_graph()）。</p>
<p>参考:<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/promisejia/article/details/80792134"> tensorflow基础知识(二) Graph计算图的创建和使用</a></p>

          </blockquote></p>
<blockquote class="blockquote-center">
            <p>[ <a href="https://blog.csdn.net/nanhuaibeian/article/details/101862790">tf.Graph().as_default() 详解</a></p>

          </blockquote>
<p>2、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_gpu = <span class="built_in">len</span>(cfgs.GPU_GROUP.strip().split(<span class="string">&#x27;,&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>这个cfgs库好像是自定义库(from libs.configs import cfgs),跳转查看GPU_GROUP可以看到其是一个常数：GPU_GROUP = “2,3”，这里可以视自己服务器上GPU数量来改，如果服务器上有四个GPU，且我们想全用，可以改为：GPU_GROUP = “0,1,2,3”,而strip函数是用来去掉字符串两端的空格的，split(‘,’)用于分割字符串，一个逗号进行分割，返回的是列表，列表的元素就是分割开的字符串。</p>
<p>3、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">global_step = slim.get_or_create_global_step()</span><br></pre></td></tr></table></figure>
<p>get_or_create_global_step是用于创建全局阶跃张量，简单来说主要是用于自动计算步长的，而不需要人为来纪步长。2.3版本中打印函数说明(即slim.get_or_creat_global_step?),说明中说slim的这个函数未来会弃用，建议使用的tf.train.get_or_create_global_step，但是在tf2.0中改函数已经被丢弃了，为了兼容tf1.0，将其放入了compat.v1中了，可以看出函数说明文档没有更新，还是用slim.get_or_creat_global_step吧。这个函数用于纪录步长，<strong>global_step</strong>在训练中是计数的作用，每训练一个batch就加1，可以传入参数，默认参数graph=None,即默认缺失，则会使用系统的图作为被纪录对象。纪录的全局步数很有意义，其相当于一个时钟表，方便控制程序还何时做何事，比如滑动平均、优化器、指数衰减学习率等方面都有用到，一些学习率大小是随步数来变的越来越小的，那么这个参数就很有意义了。</p>
<p>参考了篇博客，测试一下这个全局步数量的自加的原理(注：这里没有用get_or_create_global_step函数)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">x = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">w = tf.Variable(tf.constant(<span class="number">0.0</span>))</span><br><span class="line"> </span><br><span class="line">global_steps = tf.Variable(<span class="number">0</span>, trainable=<span class="literal">False</span>)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">learning_rate = tf.train.exponential_decay(<span class="number">0.1</span>, global_steps, <span class="number">10</span>, <span class="number">2</span>, staircase=<span class="literal">False</span>)</span><br><span class="line">loss = tf.<span class="built_in">pow</span>(w*x-y, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_steps)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        sess.run(train_step, feed_dict=&#123;x:np.linspace(<span class="number">1</span>,<span class="number">2</span>,<span class="number">10</span>).reshape([<span class="number">10</span>,<span class="number">1</span>]),</span><br><span class="line">            y:np.linspace(<span class="number">1</span>,<span class="number">2</span>,<span class="number">10</span>).reshape([<span class="number">10</span>,<span class="number">1</span>])&#125;)</span><br><span class="line">        print(sess.run(learning_rate))</span><br><span class="line">        print(sess.run(global_steps))</span><br></pre></td></tr></table></figure>
<p>由于这个代码是tf1.0的代码，所以我在此基础上改了改，tf1.0的静态操作都被放入了tensorflow.compat.v1库中了(tf.train.get_or_create_global_step还有session也以及session需要的全局初始化tf.global_variables_initializer()都放入了v1库中了)，所以导入时候特地用了tensorflow.compat.v1 as tf,使得tf1.0的代码可以在tf2.0中运行(除此以外可能还需要其他操作，比如禁止动态更新，后续讲)，代码中with 的含义是tf1.0中为了把with中的上下文都加入到一个会话(session)中，这样上下文都在sess的作用域下，可以通过sess.run进行更新。</p>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">0.10717734</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">0.11486983</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">0.123114444</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">0.1319508</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">0.14142136</span></span><br><span class="line"><span class="number">5</span></span><br><span class="line"><span class="number">0.15157166</span></span><br><span class="line"><span class="number">6</span></span><br><span class="line"><span class="number">0.16245048</span></span><br><span class="line"><span class="number">7</span></span><br><span class="line"><span class="number">0.17411011</span></span><br><span class="line"><span class="number">8</span></span><br><span class="line"><span class="number">0.1866066</span></span><br><span class="line"><span class="number">9</span></span><br><span class="line"><span class="number">0.2</span></span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>其实可以看出，global_step初始化为1，但其是作为参数传入给minimize的，所以更新应该是在该函数中进行的，minimize进行求导是对一个batch来的，然后梯度下降优化器进行更新参数，所以一个batch全局步长自加一是合理的，而learning_rate再通过global_step进行改变自身。若从简单的参数来说，函数传递过去的应该是形参，不会改变global_step，但是这里的变量是tf自定义的Variable，大胆的猜测一下，这里传过去的应该是类似于引用的方法，直接改变图中的原值。</p>
<p>参考链接：</p>
<p>参考:<blockquote class="blockquote-center">
            <p><a href="https://www.cnblogs.com/xc90/articles/10154277.html">TensorFlow中global_step的简单分析</a></p>

          </blockquote></p>
<blockquote class="blockquote-center">
            <p><a href="https://cloud.tencent.com/developer/article/1495172">slim.get_or_create_global_step()</a></p>

          </blockquote>
<p>4、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr = warmup_lr(cfgs.LR, global_step, cfgs.WARM_SETP, num_gpu)</span><br></pre></td></tr></table></figure>
<p>这里的warmup_lr是一个函数，代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">warmup_lr</span>(<span class="params">init_lr, global_step, warmup_step, num_gpu</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">warmup</span>(<span class="params">end_lr, global_step, warmup_step</span>):</span></span><br><span class="line">        start_lr = end_lr * <span class="number">0.1</span></span><br><span class="line">        global_step = tf.cast(global_step, tf.float32)</span><br><span class="line">        <span class="keyword">return</span> start_lr + (end_lr - start_lr) * global_step / warmup_step</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decay</span>(<span class="params">start_lr, global_step, num_gpu</span>):</span></span><br><span class="line">        lr = tf.train.piecewise_constant(global_step,</span><br><span class="line">                                         boundaries=[np.int64(cfgs.DECAY_STEP[<span class="number">0</span>] // num_gpu),</span><br><span class="line">                                                     np.int64(cfgs.DECAY_STEP[<span class="number">1</span>] // num_gpu),</span><br><span class="line">                                                     np.int64(cfgs.DECAY_STEP[<span class="number">2</span>] // num_gpu)],</span><br><span class="line">                                         values=[start_lr, start_lr / <span class="number">10.</span>, start_lr / <span class="number">100.</span>, start_lr / <span class="number">1000.</span>])</span><br><span class="line">        <span class="keyword">return</span> lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.cond(tf.less_equal(global_step, warmup_step),</span><br><span class="line">                   true_fn=<span class="keyword">lambda</span>: warmup(init_lr, global_step, warmup_step),</span><br><span class="line">                   false_fn=<span class="keyword">lambda</span>: decay(init_lr, global_step, num_gpu))</span><br></pre></td></tr></table></figure>
<p>函数传入的clsg.LR是5e-4,全局步长(之前也说过了，全局步长是在和梯度、优化器一起变化的)，cfgs.WARM_SETP以及调用的GPU数量。进入cfgs中看一下WARM_SETP，可以看到WARM_SETP = int(1.0 / 4.0 * SAVE_WEIGHTS_INTE)，其中设置了SAVE_WEIGHTS_INTE = 20000(至于为啥乘1/4不太了解了)。</p>
<p>那么来看一下这个函数，函数里就一句return，直接看return就好了，tf.cond啥意思呢？在交互式窗口输入tf.cond?看看(注意，tf2.0中有cond函数，所以可以import tensorflow as tf,但是为了兼容tf1.0,tf1.0的所有函数方法都放入了tf.compat.v1中，所以tf.compat.v1.cond就好，tf1.0在2.0环境中运行时候，直接import tensorflow.compat.v1 as tf即可保证原来的代码都完好运行)，函数返回如下(tf1.0下的返回，即2.0环境中tf.compat.v1.cond?)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.compat.v1.cond(</span><br><span class="line">    pred,</span><br><span class="line">    true_fn=<span class="literal">None</span>,</span><br><span class="line">    false_fn=<span class="literal">None</span>,</span><br><span class="line">    strict=<span class="literal">False</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    fn1=<span class="literal">None</span>,</span><br><span class="line">    fn2=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Return `true_fn()` <span class="keyword">if</span> the predicate `pred` <span class="keyword">is</span> true <span class="keyword">else</span> `false_fn()`. (deprecated arguments)</span><br></pre></td></tr></table></figure>
<p>可以看出这个函数功能和if  else是类似的，当pred为真的时候，返回true_fn后面的语句，否则返回false_fn的。举个例子如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line">x=tf.constant([<span class="number">1</span>],dtype=tf.float32)</span><br><span class="line">y=tf.constant([<span class="number">2</span>],dtype=tf.float32)</span><br><span class="line">a=tf.constant([<span class="number">2</span>],dtype=tf.float32)</span><br><span class="line">b=tf.constant([<span class="number">5</span>],dtype=tf.float32)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer()) <span class="comment">#模型初始化</span></span><br><span class="line">    z = tf.multiply(a, b)</span><br><span class="line">    print(sess.run(z))</span><br><span class="line">    result = tf.cond(x &lt; y, <span class="keyword">lambda</span>: tf.add(x, z), <span class="keyword">lambda</span>: tf.square(y))</span><br><span class="line">    print(sess.run(result))</span><br></pre></td></tr></table></figure>
<p>输入如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">10.</span>]</span><br><span class="line">[<span class="number">11.</span>]</span><br></pre></td></tr></table></figure>
<p>例子中都统一设定dtype为float32了，因为tf不会自动类型转换，挺傻瓜式的，至于为什么加会话即tf.Session()，是因为tf1.0中运算定义，运算执行是分开的，在一个会话内，初始化后，然后用run执行，否则print得不到想看到的结果。</p>
<p>cond解释完毕，回到模型中的函数，如果global_step(全局步数，一个batch自加一，因为训练也是以batch为单位的，其自动更新)小于等于 warmup_step的时候，返回warmup(升温)函数，否则返回decay(衰弱)函数。看升温函数，其升温初始值为传入学习率的0.1，升温至传入的学习率位置，此阶段为升温阶段；后面开始衰弱阶段，看一下decay函数，我们来看一下tf.train.piecewise_constant()函数，如下：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.compat.v1.train.piecewise_constant?</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Signature: tf.compat.v1.train.piecewise_constant(x, boundaries, values, name=<span class="literal">None</span>)</span><br><span class="line">Docstring:</span><br><span class="line">Piecewise constant <span class="keyword">from</span> boundaries <span class="keyword">and</span> interval values.</span><br><span class="line"></span><br><span class="line">Example: use a learning rate that<span class="string">&#x27;s 1.0 for the first 100001 steps, 0.5</span></span><br><span class="line"><span class="string">  for the next 10000 steps, and 0.1 for any additional steps.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```python</span></span><br><span class="line"><span class="string">global_step = tf.Variable(0, trainable=False)</span></span><br><span class="line"><span class="string">boundaries = [100000, 110000]</span></span><br><span class="line"><span class="string">values = [1.0, 0.5, 0.1]</span></span><br><span class="line"><span class="string">learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,</span></span><br><span class="line"><span class="string">values)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"># Later, whenever we perform an optimization step, we increment global_step.</span></span><br><span class="line"><span class="string">```</span></span><br></pre></td></tr></table></figure>
<p>这个函数的描述是：边界和区间值的分段常数。看函数说明文档的例子，可以看出，前100001步(global_step)用学习率1.0,前10000步是指0-100000闭区间正好100001步；然后100001步到110000的10000步用0.5，后面global_step再增加的时候，学习率用0.1，这是一种明显的逐渐逼近最优解的方式，在刚开始远离最优解位置的时候，学习调大一点，使得快速接近最优解点，在靠近后，调小学习率，防止跨度太大，跳过了该最优解到其他地方了。</p>
<p>那么回到模型函数中，显然第一个参数是global_step，第二个参数是步数边界，跳入cfgs看参数，可以看到如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">DECAY_STEP = [SAVE_WEIGHTS_INTE*<span class="number">12</span>, SAVE_WEIGHTS_INTE*<span class="number">16</span>, SAVE_WEIGHTS_INTE*<span class="number">20</span>]</span><br></pre></td></tr></table></figure>
<p>SAVE_WEIGHTS_INTE之前说过，设置成20000了，这里乘12、16、20应该是可以自行修改的边界，而decay中//num_gpu应该是因为多GPU并行运算，所以步数需要除以gpu数量，//代表取除法结果的整数部分，等价于np.floor(a/b),而学习率是从初始传入的值，变为十分之一，再百分之一，最后的是千分之一。</p>
<p>5、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.summary.scalar(<span class="string">&#x27;lr&#x27;</span>, lr)</span><br></pre></td></tr></table></figure>
<p>我们来看一下这个函数的说明文档：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">tf.compat.v1.summary.scalar?</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Signature: tf.compat.v1.summary.scalar(name, tensor, collections=<span class="literal">None</span>, family=<span class="literal">None</span>)</span><br><span class="line">Docstring:</span><br><span class="line">Outputs a `Summary` protocol buffer containing a single scalar value.</span><br><span class="line"></span><br><span class="line">The generated Summary has a Tensor.proto containing the <span class="built_in">input</span> Tensor.</span><br><span class="line"></span><br><span class="line">Args:</span><br><span class="line">  name: A name <span class="keyword">for</span> the generated node. Will also serve <span class="keyword">as</span> the series name <span class="keyword">in</span></span><br><span class="line">    TensorBoard.</span><br><span class="line">  tensor: A real numeric Tensor containing a single value.</span><br><span class="line">  collections: Optional <span class="built_in">list</span> of graph collections keys. The new summary op <span class="keyword">is</span></span><br><span class="line">    added to these collections. Defaults to `[GraphKeys.SUMMARIES]`.</span><br><span class="line">  family: Optional; <span class="keyword">if</span> provided, used <span class="keyword">as</span> the prefix of the summary tag name,</span><br><span class="line">    which controls the tab name used <span class="keyword">for</span> display on Tensorboard.</span><br><span class="line"></span><br><span class="line">Returns:</span><br><span class="line">  A scalar `Tensor` of <span class="built_in">type</span> `string`. Which contains a `Summary` protobuf.</span><br></pre></td></tr></table></figure>
<p>可以看出name是当做图中的生成的节点名称，也作为tensorboard的名称。而tensor就是一个包含单个值的张量。该函数返回一个协议缓冲区(用于保存数据)，即该函数主要用来显示标量的信息，一般在画<strong>loss，accuary</strong>时会用到这个函数(比如tensorboard中绘制可视化图)。</p>
<p>6、</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;get_batch&#x27;</span>):<span class="comment">#建立一个区域空间</span></span><br><span class="line">    <span class="keyword">if</span> cfgs.IMAGE_PYRAMID:</span><br><span class="line">        shortside_len_list = tf.constant(cfgs.IMG_SHORT_SIDE_LEN)</span><br><span class="line">        shortside_len = tf.random_shuffle(shortside_len_list)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        shortside_len = cfgs.IMG_SHORT_SIDE_LEN</span><br><span class="line"></span><br><span class="line">    img_name_batch, img_batch, gtboxes_and_label_batch, num_objects_batch, img_h_batch, img_w_batch = \</span><br><span class="line">        next_batch(dataset_name=cfgs.DATASET_NAME,</span><br><span class="line">                   batch_size=cfgs.BATCH_SIZE * num_gpu,</span><br><span class="line">                   shortside_len=shortside_len,</span><br><span class="line">                   is_training=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里顺便解释一下tf.name_scope和tf.variable_scope以及tensorboard。tf.name_scope是为了给一些Variable变量起一个区域名字，即使得数据流图更有条理性，而variable_scope也是给区域取个名字，但是其是结合tf.get_variable使用的，get_variable和普通的tf.Variable的区别在于tf.Variable总会创建一个新的变量(name属性是图中的名字，我们给的a=tf.Variable中的a是普通变量名)，如果name相同了，系统会自动修改一下这个name，进行创建；而tf.get_Variable则是可以进行共享的，如果name相同，则返回图中的那个变量，具有共享功能(比如a=tf.get_variable([1,2,3],name=”a”),b=tf.get_variable([1,2,3],name=”a”),那么a和b是共享内存的，可以理解为引用)，值得注意的是，想要进行共享，需要把共享功能设置为true，如下,这样就可以参数共享了，有些地方没有开启也可以进行共享，往往是使用的语句内部已经使用了reuse_variables。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">with tf.variable_scope(&quot;image_filters&quot;) as scope:</span><br><span class="line">    scope.reuse_variables()</span><br><span class="line">#或者如下也可以：</span><br><span class="line">with tf.variable_scope(&quot;image_filters&quot;, reuse&#x3D;True):</span><br><span class="line">	a&#x3D;tf.get_variable([1,2,3],name&#x3D;&quot;a&quot;)</span><br></pre></td></tr></table></figure>
<p>tensorboard是一个用来可视化的工具，可以看scalar(标量)的变化曲线，也可以看模型的数据流图，通过各类scope使得可视化效果更清晰，下面看个例子(来源网络)，下述就是将一个scalar保存到本地：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn_tensor_board_2</span>():</span></span><br><span class="line">    <span class="comment"># prepare the original data</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;data&#x27;</span>):</span><br><span class="line">        x_data = np.random.rand(<span class="number">100</span>).astype(np.float32)</span><br><span class="line">        y_data = <span class="number">0.3</span> * x_data + <span class="number">0.1</span></span><br><span class="line">    <span class="comment">##creat parameters</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;parameters&#x27;</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;weights&#x27;</span>):</span><br><span class="line">            weight = tf.Variable(tf.random_uniform([<span class="number">1</span>], -<span class="number">1.0</span>, <span class="number">1.0</span>))</span><br><span class="line">            tf.summary.histogram(<span class="string">&#x27;weight&#x27;</span>, weight)</span><br><span class="line">        <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;biases&#x27;</span>):</span><br><span class="line">            bias = tf.Variable(tf.zeros([<span class="number">1</span>]))</span><br><span class="line">            tf.summary.histogram(<span class="string">&#x27;bias&#x27;</span>, bias)</span><br><span class="line">    <span class="comment">##get y_prediction</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;y_prediction&#x27;</span>):</span><br><span class="line">        y_prediction = weight * x_data + bias</span><br><span class="line">    <span class="comment">##compute the loss</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;loss&#x27;</span>):</span><br><span class="line">        loss = tf.reduce_mean(tf.square(y_data - y_prediction))</span><br><span class="line">        tf.summary.scalar(<span class="string">&#x27;loss&#x27;</span>, loss)</span><br><span class="line">    <span class="comment">##creat optimizer</span></span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>)</span><br><span class="line">    <span class="comment"># creat train ,minimize the loss</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;train&#x27;</span>):</span><br><span class="line">        train = optimizer.minimize(loss)</span><br><span class="line">    <span class="comment"># creat init</span></span><br><span class="line">    <span class="keyword">with</span> tf.name_scope(<span class="string">&#x27;init&#x27;</span>):</span><br><span class="line">        init = tf.global_variables_initializer()</span><br><span class="line">    <span class="comment">##creat a Session</span></span><br><span class="line">    sess = tf.Session()</span><br><span class="line">    <span class="comment"># merged</span></span><br><span class="line">    merged = tf.summary.merge_all()</span><br><span class="line">    <span class="comment">##initialize</span></span><br><span class="line">    writer = tf.summary.FileWriter(<span class="string">&quot;tensorboard/loss-2&quot;</span>, sess.graph)</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="comment">## Loop</span></span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">201</span>):</span><br><span class="line">        sess.run(train)</span><br><span class="line">        rs = sess.run(merged)</span><br><span class="line">        writer.add_summary(rs, step)</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    learn_tensor_board_2()</span><br></pre></td></tr></table></figure>
<p>运行上述命令后，在命令行窗口输入如下命令tensorboard打开保存的scalar图:</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=C:\Users\<span class="number">21311</span>\machine_learning\tensorflow\tensorboard\loss-<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>然后在浏览器中输入如下：</p>
<figure class="highlight cmd"><table><tr><td class="code"><pre><span class="line"><span class="function">localhost:6006</span></span><br></pre></td></tr></table></figure>
<p>这里的localhost其实就是本地ip，所以使用本地ip是一样的，6006是端口号，就像jupyter使用的是8888端口似的。</p>
<p>这样就可以打开tensorboard进行查看数据流图和scalar等等了。</p>
<p>注意：variable变量都是数据流图的量，和普通的变量不一样，普通的量在函数调用退出时内存销毁，然而variable会一直在图中保留，所以不要认为调用的函数中的variable变量会消失。</p>
<p>参考文章：</p>
<p>:<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/Jerr__y/article/details/70809528?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-9.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7Edefault-9.control">TensorFlow入门（七） 充分理解 name / variable_scope</a></p>

          </blockquote></p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/qq_27825451/article/details/82349984">TensorFlow中的name_scope和variable_scope的使用</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/awxj68104/article/details/101636783">什么是TensorBoard？</a></p>

          </blockquote>
<p>继续回到模型程序中，第一个if cfgs.IMAGE_PYRAMID，跳转进入可以看到IMAGE_PYRAMID是False，这里应该是用于决定是否使用图像金字塔模型的。else中，cfgs.IMG_SHORT_SIDE_LEN=800,代表图像短的一边设置为800。</p>
<p>然后我们转入batch生成的函数内看一看，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">next_batch</span>(<span class="params">dataset_name, batch_size, shortside_len, is_training</span>):</span></span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    img_name_batch: shape(1, 1)</span></span><br><span class="line"><span class="string">    img_batch: shape:(1, new_imgH, new_imgW, C)</span></span><br><span class="line"><span class="string">    gtboxes_and_label_batch: shape(1, Num_Of_objects, 5] .each row is [x1, y1, x2, y2, label]</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># assert batch_size == 1, &quot;we only support batch_size is 1.We may support large batch_size in the future&quot;</span></span><br><span class="line"></span><br><span class="line">    valid_dataset= [<span class="string">&#x27;DOTA1.5&#x27;</span>, <span class="string">&#x27;ICDAR2015&#x27;</span>, <span class="string">&#x27;pascal&#x27;</span>, <span class="string">&#x27;coco&#x27;</span>, <span class="string">&#x27;bdd100k&#x27;</span>, <span class="string">&#x27;DOTA&#x27;</span>, <span class="string">&#x27;DOTA800&#x27;</span>, <span class="string">&#x27;DOTA600&#x27;</span>,</span><br><span class="line">                    <span class="string">&#x27;HRSC2016&#x27;</span>, <span class="string">&#x27;UCAS-AOD&#x27;</span>, <span class="string">&#x27;OHD-SJTU&#x27;</span>, <span class="string">&#x27;OHD-SJTU-600&#x27;</span>, <span class="string">&#x27;OHD-SJTU-ALL-600&#x27;</span>, <span class="string">&#x27;DOTATrain&#x27;</span>]</span><br><span class="line">    <span class="keyword">if</span> dataset_name <span class="keyword">not</span> <span class="keyword">in</span> valid_dataset:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&#x27;dataSet name must be in &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(valid_dataset))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">        pattern = os.path.join(<span class="string">&#x27;../data/tfrecord&#x27;</span>, dataset_name + <span class="string">&#x27;_train*&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        pattern = os.path.join(<span class="string">&#x27;../data/tfrecord&#x27;</span>, dataset_name + <span class="string">&#x27;_test*&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">&#x27;tfrecord path is --&gt;&#x27;</span>, os.path.abspath(pattern))</span><br><span class="line"></span><br><span class="line">    filename_tensorlist = tf.train.match_filenames_once(pattern)</span><br><span class="line"></span><br><span class="line">    filename_queue = tf.train.string_input_producer(filename_tensorlist)</span><br><span class="line"></span><br><span class="line">    img_name, img, gtboxes_and_label, num_obs, img_h, img_w = read_and_prepocess_single_img(filename_queue, shortside_len,</span><br><span class="line">                                                                                            is_training=is_training)</span><br><span class="line">    img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch = \</span><br><span class="line">        tf.train.batch(</span><br><span class="line">                       [img_name, img, gtboxes_and_label, num_obs, img_h, img_w],</span><br><span class="line">                       batch_size=batch_size,</span><br><span class="line">                       capacity=<span class="number">16</span>,</span><br><span class="line">                       num_threads=<span class="number">16</span>,</span><br><span class="line">                       dynamic_pad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> img_name_batch, img_batch, gtboxes_and_label_batch, num_obs_batch, img_h_batch, img_w_batch</span><br></pre></td></tr></table></figure>
<p>可以看到valid_dataset中有很多著名的数据集，若使用自己的数据集，封装好后，把名字加入该列表中即可，若不加，则后面的if判断后会抛出错误。后面的if&amp;else是判断是训练模式or推理模式。os库常用于管理路径文件等等，os.path.join函数用于连接两个或更多的路径名组件，功能如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1.如果各组件名首字母不包含’/’，则函数会自动加上</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.如果有一个组件是一个绝对路径，则在它之前的所有组件均会被舍弃</td>
</tr>
<tr>
<td>3.如果最后一个组件为空，则生成的路径以一个’/’分隔符结尾</td>
</tr>
</tbody>
</table>
</div>
<p>示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">Path1 = <span class="string">&#x27;home&#x27;</span></span><br><span class="line">Path2 = <span class="string">&#x27;develop&#x27;</span></span><br><span class="line">Path3 = <span class="string">&#x27;code&#x27;</span></span><br><span class="line"></span><br><span class="line">Path10 = Path1 + Path2 + Path3</span><br><span class="line">Path20 = os.path.join(Path1,Path2,Path3)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Path10 = &#x27;</span>,Path10)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;Path20 = &#x27;</span>,Path20)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Path10 =  homedevelopcode</span><br><span class="line">Path20 =  home\develop\code</span><br></pre></td></tr></table></figure>
<p>那么os.path.abspath()啥功能呢，从字面上也可以看出来，其是绝对路径的意思，若当前路径是A，你传入路径参数B，返回A/B，测试一下，如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(os.path.abspath(Path20))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">C:\Users\<span class="number">21311</span>\machine_learning\tensorflow\home\develop\code</span><br></pre></td></tr></table></figure>
<p>而  filename_tensorlist = tf.train.match_filenames_once(pattern)这句话，是类似于正则表达式，在当前路径下找匹配的文件，’_train*’中的星号就是啥都行的意思。也就是说会匹配到/data/tfrecord/DOTA_train.tfrecord文件。(r3det中测试没测试好，竟然是空[],也就是没检测到)</p>
<p>如下看一下这个函数的测试案例：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"> </span><br><span class="line">directory = <span class="string">&quot;*.*&quot;</span></span><br><span class="line">file_names = tf.train.match_filenames_once(directory)</span><br><span class="line"> </span><br><span class="line">init = (tf.global_variables_initializer(), tf.local_variables_initializer())</span><br><span class="line"> </span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    print(sess.run(file_names))</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="string">b&#x27;.\\.ipynb_checkpoints&#x27;</span></span><br><span class="line"> <span class="string">b&#x27;.\\.ipynb_checkpoints\\Untitled-checkpoint.ipynb&#x27;</span> <span class="string">b&#x27;.\\Untitled.ipynb&#x27;</span></span><br><span class="line"> <span class="string">b&#x27;.\\tensorboard&#x27;</span> <span class="string">b&#x27;.\\tensorboard\\loss-2&#x27;</span></span><br><span class="line"> <span class="string">b&#x27;.\\tensorboard\\loss-2\\events.out.tfevents.1628397138.DESKTOP-P038UOU&#x27;</span></span><br><span class="line"> <span class="string">b&#x27;.\\tensorboard\\loss-2\\events.out.tfevents.1628397214.DESKTOP-P038UOU&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>参考链接如下:</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/zsg2063/article/details/75103654">tf.train.match_filenames_once如何验证文件是否正确读取？</a></p>

          </blockquote>
<p>对于  filename_queue = tf.train.string_input_producer(filename_tensorlist)这句话，这个函数的描述说明如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Signature:</span><br><span class="line">tf.train.string_input_producer(</span><br><span class="line">    string_tensor,</span><br><span class="line">    num_epochs=<span class="literal">None</span>,</span><br><span class="line">    shuffle=<span class="literal">True</span>,</span><br><span class="line">    seed=<span class="literal">None</span>,</span><br><span class="line">    capacity=<span class="number">32</span>,</span><br><span class="line">    shared_name=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    cancel_op=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Output strings (e.g. filenames) to a queue <span class="keyword">for</span> an <span class="built_in">input</span> pipeline. (deprecated)</span><br></pre></td></tr></table></figure>
<p>其本质就是生成一个文件名队列(其实就是pipeline流水线)，但是tf1.0定义和执行是分开的，需要 tf.train.start_queue_runners函数才能真的把文件加载到文件队列中，而将文件队列中的文件加载到内存中还需要reader = tf.WholeFileReader()和key, value = reader.read(filename_queue)这两句函数。</p>
<p>参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/lty_sky/article/details/104925112">详解TensorFlow数据读取机制（tf.train.string_input_producer）</a></p>

          </blockquote>
<p>然后read_and_prepocess_single_img函数，这个函数对读出的sample进行预处理。来看一下该函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_and_prepocess_single_img</span>(<span class="params">filename_queue, shortside_len, is_training</span>):</span></span><br><span class="line"></span><br><span class="line">    img_name, img, gtboxes_and_label, num_objects = read_single_example_and_decode(filename_queue)</span><br><span class="line"></span><br><span class="line">    img = tf.cast(img, tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfgs.RGB2GRAY:</span><br><span class="line">            <span class="comment"># img, gtboxes_and_label = image_preprocess.aspect_ratio_jittering(img, gtboxes_and_label)</span></span><br><span class="line">            img = image_preprocess.random_rgb2gray(img_tensor=img, gtboxes_and_label=gtboxes_and_label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfgs.IMG_ROTATE:</span><br><span class="line">            <span class="comment"># rotate with 0.5 prob. and if rotate, if will random choose a theta from : tf.range(-90, 90+16, delta=15)</span></span><br><span class="line">            img, gtboxes_and_label = image_preprocess.random_rotate_img(img_tensor=img,</span><br><span class="line">                                                                        gtboxes_and_label=gtboxes_and_label)</span><br><span class="line"></span><br><span class="line">        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img,</span><br><span class="line">                                                                                  gtboxes_and_label=gtboxes_and_label,</span><br><span class="line">                                                                                  target_shortside_len=shortside_len,</span><br><span class="line">                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cfgs.HORIZONTAL_FLIP:</span><br><span class="line">            img, gtboxes_and_label = image_preprocess.random_flip_left_right(img_tensor=img,</span><br><span class="line">                                                                             gtboxes_and_label=gtboxes_and_label)</span><br><span class="line">        <span class="keyword">if</span> cfgs.VERTICAL_FLIP:</span><br><span class="line">            img, gtboxes_and_label = image_preprocess.random_flip_up_down(img_tensor=img,</span><br><span class="line">                                                                          gtboxes_and_label=gtboxes_and_label)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img, gtboxes_and_label, img_h, img_w = image_preprocess.short_side_resize(img_tensor=img, gtboxes_and_label=gtboxes_and_label,</span><br><span class="line">                                                                                  target_shortside_len=shortside_len,</span><br><span class="line">                                                                                  length_limitation=cfgs.IMG_MAX_LENGTH)</span><br><span class="line">    <span class="comment"># gtboxes_and_label = tf.reshape(tf.py_func(filter_small_gt, inp=[gtboxes_and_label], Tout=[tf.float32]), [-1, 9])</span></span><br><span class="line">    <span class="keyword">if</span> cfgs.NET_NAME <span class="keyword">in</span> [<span class="string">&#x27;resnet152_v1d&#x27;</span>, <span class="string">&#x27;resnet101_v1d&#x27;</span>, <span class="string">&#x27;resnet50_v1d&#x27;</span>]:</span><br><span class="line">        img = img / <span class="number">255</span> - tf.constant([[cfgs.PIXEL_MEAN_]])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        img = img - tf.constant([[cfgs.PIXEL_MEAN]])  <span class="comment"># sub pixel mean at last</span></span><br><span class="line">    <span class="keyword">return</span> img_name, img, gtboxes_and_label, num_objects, img_h, img_w</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>这个函数中的第一句使用了read_single_example_and_decode函数，这个函数功能其实是读取出一个sample，由于sample的数据格式是特定的，所以需要进行解析，这个函数就是对其进行解析的。来看一下这个函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_single_example_and_decode</span>(<span class="params">filename_queue</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tfrecord_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.ZLIB)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># reader = tf.TFRecordReader(options=tfrecord_options)</span></span><br><span class="line">    reader = tf.TFRecordReader()</span><br><span class="line">    _, serialized_example = reader.read(filename_queue)</span><br><span class="line"></span><br><span class="line">    features = tf.parse_single_example(</span><br><span class="line">        serialized=serialized_example,</span><br><span class="line">        features=&#123;</span><br><span class="line">            <span class="string">&#x27;img_name&#x27;</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">            <span class="string">&#x27;img_height&#x27;</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">            <span class="string">&#x27;img_width&#x27;</span>: tf.FixedLenFeature([], tf.int64),</span><br><span class="line">            <span class="string">&#x27;img&#x27;</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">            <span class="string">&#x27;gtboxes_and_label&#x27;</span>: tf.FixedLenFeature([], tf.string),</span><br><span class="line">            <span class="string">&#x27;num_objects&#x27;</span>: tf.FixedLenFeature([], tf.int64)</span><br><span class="line">        &#125;</span><br><span class="line">    )</span><br><span class="line">    img_name = features[<span class="string">&#x27;img_name&#x27;</span>]</span><br><span class="line">    img_height = tf.cast(features[<span class="string">&#x27;img_height&#x27;</span>], tf.int32)</span><br><span class="line">    img_width = tf.cast(features[<span class="string">&#x27;img_width&#x27;</span>], tf.int32)</span><br><span class="line">    img = tf.decode_raw(features[<span class="string">&#x27;img&#x27;</span>], tf.uint8)</span><br><span class="line"></span><br><span class="line">    img = tf.reshape(img, shape=[img_height, img_width, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    gtboxes_and_label = tf.decode_raw(features[<span class="string">&#x27;gtboxes_and_label&#x27;</span>], tf.int32)</span><br><span class="line">    gtboxes_and_label = tf.reshape(gtboxes_and_label, [-<span class="number">1</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line">    num_objects = tf.cast(features[<span class="string">&#x27;num_objects&#x27;</span>], tf.int32)</span><br><span class="line">    <span class="keyword">return</span> img_name, img, gtboxes_and_label, num_objects</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>第一句是为了可以读取文件名队列中文件到内存中，reader.read就是读到内存中，tf.parse_single_example就是对其进行解析，其返回的是一个字典类型，我们可以打印一下看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="built_in">type</span>(features))</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> &#x27;<span class="title">dict</span>&#x27;&gt;</span></span><br></pre></td></tr></table></figure>
<p>解析后将值赋值给img_name等等即可，由于img解析的时候用的是str格式，需要继续解码用tf.decode_raw，这个函数可以将原始字节存储变为tensor，这个函数的描述如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">Signature:</span><br><span class="line">tf.decode_raw(</span><br><span class="line">    input_bytes=<span class="literal">None</span>,</span><br><span class="line">    out_type=<span class="literal">None</span>,</span><br><span class="line">    little_endian=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    <span class="built_in">bytes</span>=<span class="literal">None</span>,</span><br><span class="line">)</span><br><span class="line">Docstring:</span><br><span class="line">Convert raw byte strings into tensors. (deprecated arguments)</span><br></pre></td></tr></table></figure>
<p>注意这个函数的输出形式需要指定，示例如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.compat.v1 <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    a=tf.decode_raw(<span class="built_in">str</span>(<span class="number">0</span>),tf.uint8)   </span><br><span class="line">    print(sess.run(a))</span><br></pre></td></tr></table></figure>
<p>输出结果如下，48就是字符0的ascii码.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[<span class="number">48</span>]</span><br></pre></td></tr></table></figure>
<p>后面又对img进行了reshape处理，reshape处理成的格式是为了和模型输入相匹配，但是这样变是否会改变数据结构呢??比如本来是[3,img_wight,img_height],这样reshape后数据应该是拉跨了的。后续又对gtbox进行处理，至于为啥一个框是9个坐标呢??应该是四个角的坐标+1个label或者是三个角坐标+中心坐标+1个label。</p>
<p>最终函数返回了目标名，图像，gtbox和label以及目标数量。</p>
<p>现在返回read_and_prepocess_single_img函数。</p>
<p>第一个if判断是否需要灰度化，cfgs里该参数设定为了False不用管，第二个if判断图像是否rotate即旋转了，设定为True，那么if里的函数就是以0.5的概率进行旋转，旋转的角度在(-90,96)之间，跨度为15度??传入原始图和gtbox_and_label后返回旋转后的图像和gtbox_and_label。</p>
<p>image_preprocess.short_side_resize这个函数用于尺寸限制，不过有点奇怪，传入的最短边在cfgs中设置为800，cfgs.IMG_MAX_LENGTH这个最大值限制也是800??迷惑。</p>
<p>后面的两个if判断是否水平翻转和垂直翻转，cfgs中设置的都是True，然后以概率来进行翻转。</p>
<p>如果不是训练模式，而是推理模式，那么直接进行resize到规定的尺寸就好了。</p>
<p>cfgs.NET_NAME在cfgs中设定的是resnet152，执行的除以255应该是类似于归一化的，至于为啥减去那个参数：有三个参数(广播机制会进行使每个像素点RGB都减去这个值)。</p>
<p>最终这个函数返回预处理后的图像名，图像，图像的gtbox和label，图像数量以及图像的高和宽。</p>
<p>这个函数也解读完毕，再返回到next_batch函数中，到了最后一句的batch函数，这个函数将第一个参数进行入队，再以batch_size大小进行读出，具体含义如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">tf.train.batch(</span><br><span class="line">    tensors,</span><br><span class="line">    batch_size,</span><br><span class="line">    num_threads=<span class="number">1</span>,</span><br><span class="line">    capacity=<span class="number">32</span>,</span><br><span class="line">    enqueue_many=<span class="literal">False</span>,</span><br><span class="line">    shapes=<span class="literal">None</span>,</span><br><span class="line">    dynamic_pad=<span class="literal">False</span>,</span><br><span class="line">    allow_smaller_final_batch=<span class="literal">False</span>,</span><br><span class="line">    shared_name=<span class="literal">None</span>,</span><br><span class="line">    name=<span class="literal">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>函数功能：利用一个tensor的列表或字典来获取一个batch数据</p>
<p>参数介绍：</p>
<p>tensors：一个列表或字典的tensor用来进行入队<br>batch_size：设置每次从队列中获取出队数据的数量<br>num_threads：用来控制入队tensors线程的数量，如果num_threads大于1，则batch操作将是非确定性的，输出的batch可能会乱序<br>capacity：一个整数，用来设置队列中元素的最大数量<br>enqueue_many：在tensors中的tensor是否是单个样本<br>shapes：可选，每个样本的shape，默认是tensors的shape<br>dynamic_pad：Boolean值.允许输入变量的shape，出队后会自动填补维度，来保持与batch内的shapes相同<br>allow_samller_final_batch：可选，Boolean值，如果为True队列中的样本数量小于batch_size时，出队的数量会以最终遗留下来的样本进行出队，如果为Flalse，小于batch_size的样本不会做出队处理<br>shared_name：可选，通过设置该参数，可以对多个会话共享队列<br>name：可选，操作的名字<br>但是这里略有疑问，我们之前获取文件名(该文件包含了所有的训练集数据)，但读取的是全读出来了??还是只读了一个，若是全读出来了，入队倒是可以理解，然后以batch_size=16进行读出，但是数据处理shape那里又感觉颇有问题；如果是只读了一个数据??那么感觉如队的时候就有问题。</p>
<p>参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/sinat_29957455/article/details/83152823">tensorflow中的tf.train.batch详解</a></p>

          </blockquote>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>



]]></content>
      <categories>
        <category>tensorflow</category>
        <category>r3det</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>r3det</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow中的参数更新机制详解</title>
    <url>/2021/08/05/tensorflow%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3/</url>
    <content><![CDATA[<p>之前用的都是torch，tf倒是没怎么用过，最近由于种种原因，需要学一学tensorflow，不过tf中的大多数函数类方法都无法自动进行int—&gt;float类型强制转换，还得自行cast转换，着实麻烦。深度学习的核心所在就是梯度下降，各种优化器都是基于导数的梯度下降来的，所以自动求导更新就是至关重要的，这一节就来讲一讲tensorflow中的参数更新机制。</p>
<a id="more"></a>
<p>tf和torch中一样，各类参数都是以tensor即张量类型进行存储，由于模型中并不是所有的量都需要计算梯度，所以tf中会用tf.Variable(x)将tensor标记为可训练即可求导的变量。其实如果直接从少量参数和少量代码来看，似乎引入Variable意义不大，我们可以直接对需要求导的参数进行求导，得到梯度后，进行w=w-r*grad进行更新即可(w代表需要训练的参数，r为学习率，grad代表参数)，但是神经网络模型中参数巨量，这样操作会导致参数混乱，而且需要我们自己更新参数，远没有模块化的便利，所以我们希望的模块化，以函数或者方法的形式，可以反复的复用，极大的提高便捷性，所以才会出现一系列函数和方法来辅助简便化参数自动更新。</p>
<p>值得注意的是，如果搜过以前的很多博客，应该知道图变量Variable没法自动数据流更新，也就是说你赋值过后，对图变量进行一些运算后，其数据流并没有运转起来(可以理解为没有进行运算)，需要如下才能使得数据流运转起来进行计算：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = tf.Variable(<span class="number">3</span>, name=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">y = x * <span class="number">5</span></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line">sess.run(tf.global_variables_initializer())</span><br><span class="line">sess.run(y)</span><br></pre></td></tr></table></figure>
<p>也就是使用了交互接口，然后运行初始化，在运行y在能得到x*5后操作，显得特别臃肿，但在现在的tensorflow2.0版本中，终于取消了这种颇为愚昧的操作，直接简单的像np一样运算操作就好了,即有了自动更新机制。(网上大多数以前的博客都没更新，其实已经不需要使用run来操作了)，我们可以看一下1.0版本时候的操作，那个时候tf中的数据流定义和运算是分开的(定义一个操作和执行这个操作是分开的)，</p>
<p>在通过tf.Variable对tensor标记后，我们可以通过tf.GradientTape()对这些可训练参数进行导数计算，然后将这些计算得到的梯度信息都存在磁盘里(“Tape”),磁盘名可以自取。用法如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>,watch_accessed_variables=<span class="literal">True</span>) <span class="keyword">as</span> www:</span><br><span class="line">    a2=tf.<span class="built_in">pow</span>(a,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>其中tf.GradientTape有两个参数，含义入下：</p>
<script type="math/tex; mode=display">
persistent: 布尔值，用来指定新创建的gradient tape是否是可持续性的。默认是False，意味着只能够调用一次gradient（）函数。
\\watch\_accessed\_variables: 布尔值，表明这个gradien tap是不是会自动追踪任何能被训练（trainable）的变量。默认是True。要是为False的话，意味着你需要手动去指定你想追踪的那些变量,手动追踪用watch。</script><p>上述的代码中，如果a是可训练参数，那么tf.GrandientTape就会自动计算a的所有操作步骤的梯度，(通过这些所有操作步骤的梯度，链式法则就可以知道各个操作之间的导数关系)。</p>
<p>而 with A as B是什么意思呢?其实样式和for很类似，是控制流语句，for是不断遍历可迭代对象，将数据取出来(通过__next__ 操作的)，而with是把 A中的返回送给B，不过B可以接收A的任何返回（可以是方法类等等，取决自己如何用），在with下的语句执行完毕后，还会执行魔法函数__exit__（应该是在该退出函数时候将所有结果保存在了Tape中）,具体功能其实和try except是很像的，具体分析链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/qiqicos/article/details/79200089">python中with…as的用法</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/frank_ljiang/article/details/106281919">with在python中的用法和含义</a></p>

          </blockquote>
<p>下面贴一个具体例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=tf.constant([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],shape=(<span class="number">2</span>,<span class="number">3</span>),dtype=tf.float32)</span><br><span class="line">b=tf.Variable(a)</span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>,watch_accessed_variables=<span class="literal">True</span>) <span class="keyword">as</span> www:</span><br><span class="line">    www.watch(a)</span><br><span class="line">    a2=tf.<span class="built_in">pow</span>(a,<span class="number">3</span>)</span><br><span class="line">    b2=tf.exp(b)</span><br><span class="line">grad_a=www.gradient(a2,a)</span><br><span class="line">grad_b=www.gradient(b2,b)</span><br><span class="line">print(grad_a,grad_b)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.Tensor(</span><br><span class="line">[[  <span class="number">3.</span>  <span class="number">12.</span>  <span class="number">27.</span>]</span><br><span class="line"> [ <span class="number">48.</span>  <span class="number">75.</span> <span class="number">108.</span>]], shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=float32) tf.Tensor(</span><br><span class="line">[[  <span class="number">2.7182817</span>   <span class="number">7.389056</span>   <span class="number">20.085537</span> ]</span><br><span class="line"> [ <span class="number">54.59815</span>   <span class="number">148.41316</span>   <span class="number">403.4288</span>   ]], shape=(<span class="number">2</span>, <span class="number">3</span>), dtype=float32)</span><br></pre></td></tr></table></figure>
<p>上述将tf.GradientTape的两个属性设为True,是可以连续使用gradient和自动纪录可训练参数的导数，而由于tensor类型不是可训练参数，所以需要手动指定watch来纪录其操作是的导数计算，如果tf.GradientTape的第二个参数设定为False，即使是可训练参数也需要手动进行指定。而www.gradient(A,B)就是从磁盘中提取出A关于B的导数。</p>
<p>本文参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/Forrest97/article/details/105913952">with tf.GradientTape() as tape 梯度带 Tensorflow自动求导API</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/xierhacker/article/details/53174558?utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.control&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-3.control">TensorFlow学习（四）：梯度带(GradientTape)，优化器(Optimizer)和损失函数(losses)</a></p>

          </blockquote>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <tags>
        <tag>-深度学习 -tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>西瓜书--线性模型</title>
    <url>/2021/08/04/%E8%A5%BF%E7%93%9C%E4%B9%A6-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>西瓜书</category>
      </categories>
      <tags>
        <tag>西瓜书</tag>
        <tag>线性模型</tag>
      </tags>
  </entry>
  <entry>
    <title>漫谈--记于光华楼前</title>
    <url>/2021/07/30/%E6%BC%AB%E8%B0%88/</url>
    <content><![CDATA[<p>来上海有一个多月了，却还没好好转过校园，昨日是烟花过后第一个晴日，奥，倒也不能说是晴天，不过总算是不下雨了，难得一个不下雨也不太热的好天气，而最近自己也感觉有点累了，就出了实验室散散心，转了校园一圈，到了光华楼前的草坪处坐了一会，人不多，零零碎碎的几个情侣，还有几个母亲带着小孩在嬉戏，听说开学的时候会有很多人，那一定会是个很壮观的场景吧。</p>
<a id="more"></a>
<p>光华楼很高，一楼的电梯可以到达一到十五层，到十五层以上则需要到十五层再转电梯，看到光华楼总能想到双子楼，总感觉两边各一个就像双子一样。</p>
<p><a href="https://imgtu.com/i/WqlIbT"><img src="https://z3.ax1x.com/2021/07/29/WqlIbT.jpg" alt="WqlIbT.jpg"></a></p>
<p><a href="https://imgtu.com/i/WqlqPJ"><img src="https://z3.ax1x.com/2021/07/29/WqlqPJ.jpg" alt="WqlqPJ.jpg"></a></p>
<p>光华楼前面是个草坪，草坪靠近路那还有一堆小花，特地搜了搜，好像叫孔雀草万寿菊啥的，不是很懂，周围的树上会有小吊牌，估计是学校校训或者校歌里的吧，路上偶尔还会有几个保安叔叔经过，可能是在巡逻或者准备下班回家吧。</p>
<p><a href="https://imgtu.com/i/Wq1AxI"><img src="https://z3.ax1x.com/2021/07/29/Wq1AxI.jpg" alt="Wq1AxI.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq1iPH"><img src="https://z3.ax1x.com/2021/07/29/Wq1iPH.jpg" alt="Wq1iPH.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq1FGd"><img src="https://z3.ax1x.com/2021/07/29/Wq1FGd.jpg" alt="Wq1FGd.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq19aD"><img src="https://z3.ax1x.com/2021/07/29/Wq19aD.jpg" alt="Wq19aD.jpg"></a></p>
<p>傍晚光华楼前草坪的风真的很大，台风过后，雨后初晴，云儿飘着，风儿吹着，很舒服，在草地上一坐就坐了一个多小时，不过可惜错过了龙队的奥运会，所幸今天此时此刻知道男女单打金银都已既定我们了。</p>
<p><a href="https://imgtu.com/i/Wq1pVO"><img src="https://z3.ax1x.com/2021/07/29/Wq1pVO.jpg" alt="Wq1pVO.jpg"></a></p>
<p><a href="https://imgtu.com/i/WqlzqK"><img src="https://z3.ax1x.com/2021/07/29/WqlzqK.jpg" alt="WqlzqK.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq1pVO"><img src="https://z3.ax1x.com/2021/07/29/Wq1pVO.jpg" alt="Wq1pVO.jpg"></a></p>
<p>之前本科时候买的帽子一直没怎么带，室友说有些奇怪，所有就一直闲置了，现在骑车总是嫌风有点大会吹乱头发，就带了起来，大概看了看，感觉倒也不错。</p>
<p><a href="https://imgtu.com/i/Wq1kRA"><img src="https://z3.ax1x.com/2021/07/29/Wq1kRA.jpg" alt="Wq1kRA.jpg"></a></p>
<p>今天又来光华楼这，记录了昨天的东西，发现校外的几座大楼金光闪闪的，还有个楼像个魔仙堡似的，可能是个游乐园。</p>
<p><a href="https://imgtu.com/i/Wq1ZsP"><img src="https://z3.ax1x.com/2021/07/29/Wq1ZsP.jpg" alt="Wq1ZsP.jpg"></a></p>
<p>七点四十了，可能猫咪也困了，坐在我旁边椅子上睡着了，时候是不早了，该回实验室了，录毕。—-记于光华楼前草坪座椅上</p>
<p><a href="https://imgtu.com/i/Wq1eqf"><img src="https://z3.ax1x.com/2021/07/29/Wq1eqf.jpg" alt="Wq1eqf.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq1nZ8"><img src="https://z3.ax1x.com/2021/07/29/Wq1nZ8.jpg" alt="Wq1nZ8.jpg"></a></p>
<p><a href="https://imgtu.com/i/Wq1udS"><img src="https://z3.ax1x.com/2021/07/29/Wq1udS.jpg" alt="Wq1udS.jpg"></a></p>
]]></content>
      <categories>
        <category>光华楼</category>
        <category>邯郸校区</category>
      </categories>
      <tags>
        <tag>光华楼</tag>
        <tag>漫谈</tag>
      </tags>
  </entry>
  <entry>
    <title>基于FPGA的无人机目标跟踪系统实现</title>
    <url>/2021/07/26/%E5%9F%BA%E4%BA%8EFPGA%E7%9A%84%E6%97%A0%E4%BA%BA%E6%9C%BA%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[<p>这是第三届全国大学生集成电路创新创业大赛中科芯杯的赛题，当时没有找到合适的队友，就选择了一个人参赛，所以队名还起了名字叫一个人的风景，当时准确率是华东赛区本科生组第一名，可惜华东赛区只有三个队伍能够晋级总决赛，那三个队伍是研究生队伍，我是第四，赛区赛就比较倒霉呀，他们三个队伍总决赛的时候分别是杯赛第一名、第二和第四名，这就很坑了。。。</p>
<a id="more"></a>
<p>大概说一下赛题流程，中科芯提供了一个上位机，该上位机的功能是将PC端视频路径加载到上位机中，上位机将该视频流以HDMI格式从PC端的HDMI发送出去，FPGA端进行HDMI数据接收，然后进行处理，上位机在发送HDMI视频流的时候，同时会通过串口发送视频第一帧的无人机坐标，包含左上角坐标、w和h，而FPGA则通过第一帧的无人机信息进行追踪后续所有帧的无人机位置，将追踪得到的左上角坐标进行发送返回。由于串口是无握手通信协议，所以还需要设计链路层的通信协议。下面进行详细说明</p>
<h2 id="一、设计概述"><a href="#一、设计概述" class="headerlink" title="一、设计概述"></a>一、设计概述</h2><h3 id="1-1、硬件选取"><a href="#1-1、硬件选取" class="headerlink" title="1.1、硬件选取"></a>1.1、硬件选取</h3><p>项目设计是基于XLINX ARTIX-7 系列的 FPGA 开发平台（型号：XC7A35T-2FGG484I），用到USB转TTL模块、HDMI接口、USB接口线.</p>
<h3 id="1-2、项目说明"><a href="#1-2、项目说明" class="headerlink" title="1.2、项目说明"></a>1.2、项目说明</h3><h4 id="1-项目内容"><a href="#1-项目内容" class="headerlink" title="1.项目内容"></a>1.项目内容</h4><p>（一）、针对中科芯提供的旋翼无人机机动测试视频设计基于FPGA的目标跟踪器，实现对视频中无人机目标的稳定连续跟踪，目标跟踪算法必须完全使用FPGA实现。</p>
<p>（二）、各阶段中科芯均提供若干段比测视频，每段视频均给出目标尺寸w*h和第一帧初始位置坐标x1,y1，用于提供跟踪算法初始化信息。跟踪器测试系统如下图所示，测试设备将受试视频经HDMI接口发送至FPGA开发板，并通过串口接收其跟踪坐标结果。选择将个人计算机作为测试设备，通过下载主办方提供的上位机软件实现上述功能。基于FPGA设计的跟踪器须对每段视频分别进行测试，结果以跟踪结果输出坐标精确度为准。FPGA应选择带HDMI接口芯片的Virtex5、6、7或 Artix-7 系列开发平台进行实现。</p>
<p><a href="https://imgtu.com/i/W5kaVg"><img src="https://z3.ax1x.com/2021/07/27/W5kaVg.png" alt="W5kaVg.png"></a></p>
<p>​                                                            图1 输入输出接口配置</p>
<p>（三）、考虑到程移植便利性，建议采用以下输入输出接口配置：参赛队跟踪器图像输入采用标准HDMI接口，目标初始位置坐标采用RS422异步串口输入，跟踪位置坐标采用同一串口输出。在此介绍一下串口的设置和链路层通信协议：</p>
<p>串口通信物理层协议：波特率：9600；数据位位数：8位；校验位：无；停止位：有</p>
<p>链路层通信协议：帧内字节数：8字节；校验码：LRC累加和校验码；位置坐标数据高位在前、高字节在前</p>
<p>例： 目标初始位置坐标（255，511），长宽均为20 ，则PC发送帧为 AA 00 FF 01     FF 14 14 2F，发送帧的意义如下图：</p>
<p><a href="https://imgtu.com/i/W5Z40K"><img src="https://z3.ax1x.com/2021/07/27/W5Z40K.png" alt="W5Z40K.png"></a></p>
<p>跟踪结果位置坐标(255,511)，则FPGA发送帧为 55 00 FF 01 FF 00 00 AC，返回帧的意义如下：</p>
<p><a href="https://imgtu.com/i/W5Zhm6"><img src="https://z3.ax1x.com/2021/07/27/W5Zhm6.png" alt="W5Zhm6.png"></a></p>
<h4 id="2-项目指标"><a href="#2-项目指标" class="headerlink" title="2.项目指标"></a>2.项目指标</h4><p>（一）、提供的测试和比赛用视频均为彩色，8bit位深，分辨率为720P@25FPS，视频格式为AVI，视频中无人机目标大小&gt;20*20pixel，时长约30秒。赛题跟踪难度见官网发布的受测视频。</p>
<p>​                                                                    视频中目标左上角初始位置坐标(x,y)，目标尺寸(w,h)</p>
<p>​                                                                    视频1为564,305,132,48；视频2为720,312,66,30  </p>
<p>​                                                                    视频3为820,241,110,44；视频4为592,325,40,20</p>
<p>（二）、 跟踪器在受试过程中禁止人工干预，但允许跟踪器具备自动参数调整和跟踪丢失后的目标自动重捕功能。</p>
<p>（三）、 比赛结果按各跟踪精度性能指标进行排名。精度指标衡量方法基于VOT挑战赛的无监督（unsupervised）模式，以全部受测的N帧图像内，算法跟踪结果波门与实际波门平均重合率AO（Average Overlap）描述。</p>
<script type="math/tex; mode=display">
AO=\frac{1}{N}\sum^N_{i=1}\frac{S_i}{S}</script><p>其中，S_i表示跟踪结果波门与真实波门重叠面积，S=w*h即目标面积由组织方给,其实就是ground truth的面积。若目标跟踪丢失，则跟踪器在当前帧得分为0。因此，应谨慎设计算法，确保连续跟踪，并视需求加入丢失后重捕机制（在本次设计的算法中，是可以通过阈值来推断是否追踪丢失的）。最终性能将以多段比赛视频测试AO指标累加结果为准，排名则依据AO指标从高到低排列。</p>
<p><strong>3.总结</strong></p>
<p>项目设计中，通过上位机发送视频流数据给FPGA，FPGA通过HDMI端口接收数据，进行定位追踪视频中的无人机，通过串口发送给上位机，同时为了显示更加明了，将定位结果通过输出的HDMI接口发送给显示屏，显示屏也同时显示追踪结果.</p>
<h2 id="二、系统组成及算法讲解"><a href="#二、系统组成及算法讲解" class="headerlink" title="二、系统组成及算法讲解"></a>二、系统组成及算法讲解</h2><h3 id="2-1-系统介绍"><a href="#2-1-系统介绍" class="headerlink" title="2.1 系统介绍"></a>2.1 系统介绍</h3><p>上位机将视频流发送给FPGA，FPGA对视频进行解码转为VGA时序信号和像素信息RGB888，得到RGB888后，将其转到YCBCR色域，然后对其进行灰度化处理，灰度化处理后将灰度像素传送给二值化模块，通过阈值设定对其进行二值化，二值化输出信息传送给sobel边缘提取模块，进行边缘提取，至此图像预处理结束，将预处理输出结果送给追踪模块，追踪模块将生成多个待检测目标窗口，然后与初始帧的无人机目标对比，选出最优窗口作为当前帧的无人机位置。系统框图如图2。<a href="https://imgtu.com/i/W5eBjI"><img src="https://z3.ax1x.com/2021/07/27/W5eBjI.png" alt="W5eBjI.png"></a></p>
<h3 id="2-2、HDMI-VGA"><a href="#2-2、HDMI-VGA" class="headerlink" title="2.2、HDMI-VGA"></a>2.2、HDMI-VGA</h3><p>待追踪视频由PC端上位机发送，通过扩展显示屏，把电脑布局与缩放调为100%，扩展屏设置为60hz，HDMI与FPGA连接后，FPGA被PC端识别为扩展屏，并向其发送视频流，FPGA接收到HDMI类型的视频流，由于使用的黑金的alinx7035系列FGPA没有硬核解码器解码HDMI视频，所以通过软核进行解码并将其转换成VGA信号，此软核为github开源IP。</p>
<h3 id="2-3、算法讲解及其仿真结果展示"><a href="#2-3、算法讲解及其仿真结果展示" class="headerlink" title="2.3、算法讲解及其仿真结果展示"></a>2.3、算法讲解及其仿真结果展示</h3><h4 id="2-3-1、图像处理算法模块"><a href="#2-3-1、图像处理算法模块" class="headerlink" title="2.3.1、图像处理算法模块"></a>2.3.1、图像处理算法模块</h4><p>（1）灰度化</p>
<p>将彩色图像转化为灰度的方法有两种，一个是令 RGB 三个分量的数值相等，输出后便可以得到灰度图像，另一种是转化为 YCbCr 格式，将 Y 分量提取出来，YCbCr 格式中的 Y 分量表示的是图像的亮度和浓度所以只输出 Y 分量，得到的图像就是灰度图像了。我们在这里选择第二种方法实现。</p>
<p>YCbCr是通过有序的三元组来表示的，三元由Y(Luminance)、Cb(Chrominance-Blue)、和Cr(Chrominance-Red)组成，其中Y表示颜色的明亮度和浓度，而Cb和Cr则分别表示颜色的蓝色浓度偏移量和红色浓度偏移量。人的肉眼对由YCbCr色彩空间编码的视频中的Y分量更敏感，而Cb和Cr的微小变化不会引起视觉上的不同。为了得到灰度图像，要将采集到的彩色图像转化为YCbCr后，将Y分量分别送给R、G、B，即此时R、G、B数值上是一致的，这样便得到了灰度图。灰度化效果如图3和图4。</p>
<p><a href="https://imgtu.com/i/W5m5Je"><img src="https://z3.ax1x.com/2021/07/27/W5m5Je.png" alt="W5m5Je.png"></a></p>
<p><a href="https://imgtu.com/i/W5nkwV"><img src="https://z3.ax1x.com/2021/07/27/W5nkwV.png" alt="W5nkwV.png"></a></p>
<p>FPGA开发板通过HDMI采集到的数据是RGB888的格式，官方给出的转化公式是严格的RGB888转为YCbCr888。</p>
<p>官方给出的RGB888-&gt;YCbCr888转化公式如下：</p>
<script type="math/tex; mode=display">
Y=0.229R+0.587G+0.114B
\\Cb=0.568(B-Y)+128
\\Cr=0.713(R-Y)+128</script><p>由于FPGA无法实现浮点数运算，所以需要把系数变为整数，我们不妨将Y、Cb和Cr都扩大1024倍，然后所有系数取整，那么变成如下公式：</p>
<script type="math/tex; mode=display">
Y=306R+601G+116B
\\Cb=-176R-347G+523B+131072
\\Cr=523R-438G-85B+131072</script><p>这样便可以得到整型的运算，最后得到的结果右移10位即可，但为了时序的科学严谨性，我们不应该一次在always块中算出Y、Cb、Cr，因为一个关系式中涉及到三次乘法和两次加法，越多的运算量就越可能导致时序延时错乱，此处或许不会有问题，但不在一个块中用太复杂的运算式是一种好的习惯，我们应该选择业界普遍使用的流水线做法，将乘法在一个always块里实现，在另一个always块中实现加法。</p>
<p>（2）二值化</p>
<p> 在灰度化后，进行二值化操作，二值化操作即是设定一个阈值，当灰度值大于该阈值是赋值为1，小于该值是赋值为0，二值化效果如图5和图6。阈值确定较为经典的算法是大津算法（OTSU），大津算法可以使得前景与背景图像的类间方差最大，它被认为是图像分割中阈值选取的最佳算法，计算简单，不受图像亮度和对比度的影响，但其需要一帧数据得到阈值确定，即追踪会造成一帧延迟，所以此处未使用大津算法而是使用的是测试的相对较好的固定阈值，在后续完善中可以考虑运用一帧的部分数据求大津阈值，使其不造成一帧的延迟。</p>
<p>原图和二值化图片如下：</p>
<p><a href="https://imgtu.com/i/W5Qlge"><img src="https://z3.ax1x.com/2021/07/27/W5Qlge.png" alt="W5Qlge.png"></a></p>
<p><a href="https://imgtu.com/i/W5Q0gg"><img src="https://z3.ax1x.com/2021/07/27/W5Q0gg.png" alt="W5Q0gg.png"></a></p>
<p>（4）、Sobel边缘化</p>
<p>二值化会使得前景和背景相区分，但是目标所处地位可能和背景相交换，为了在这种结果发生时依旧可以进行正常追踪，此步我们进行Sobel算子边缘化提取，从而提高后续的追踪算法的稳定性。</p>
<p>Sobel边缘检测的核心在于像素矩阵的卷积，卷积对于数字图像处理非常重要，很多图像处理算法都是做卷积来实现的。卷积运算的本质就是对制定的图像区域的像素值进行加权求和的过程，其计算过程为图像区域中的每个像素值分别与卷积模板的每个元素对应相乘，将卷积的结果作求和运算，运算到的和就是卷积运算的结果。</p>
<p>3×3的窗口M与卷积模板C 的卷积运算如下：</p>
<script type="math/tex; mode=display">
M= \left[
 \begin{matrix}
   M1 & M2 & M3 \\
   M4 & M5 & M6 \\
   M7 & M8 & M9
  \end{matrix}
  \right] \ \ \ \ \ \ \ \
  C= \left[
 \begin{matrix}
   C1 & C2 & C3 \\
   C4 & C5 & C6 \\
   C7 & C8 & C9
  \end{matrix}
  \right]
  \\M5'=M1*C1+M2*C2+M3*C3+M4*C4+M5*C5+\\M6*C6+M7*C7+M8*C8+M9*C9</script><p>G_x和G_y是图像边缘梯度，将两个因子和原始图像做如下卷积得到边缘梯度，其中A表示原视图像。</p>
<script type="math/tex; mode=display">
G_x=\left[
 \begin{matrix}
   -1 & 0 & +1 \\
   -2 & 0 & +2 \\
   -1 & 0 & +1
  \end{matrix}
  \right] *A\ \ \ \ \ \ \
  G_y=\left[
 \begin{matrix}
   -1 & -2 & -1 \\
   0  & 0  & 0 \\
   +1 & +2 & +1
  \end{matrix}
  \right] *A</script><p>得到图像中的每一个点的横向纵向梯度G_x、G_y。最后通过如下公式来计算该点总体梯度的大小。</p>
<script type="math/tex; mode=display">
G=\sqrt{(G^2_x+G^2_y)}</script><p>我们此时还需要设定一个阈值，该如果算出的G大于设定的阈值，那么认为此处是边缘处，使其为黑色，否则认为不是边缘，使其为白色。</p>
<p>上述是算法原理，很显然，这里用到了3<em>3矩阵，对于matlab来说，获取是比较简单的，但是FPGA实现时，我们需要考虑如何得到这个3</em>3的矩阵，因为FPGA扫描像素点是一个一个进行的。在 FPGA 中生成 3x3 矩阵有以下三种方法：</p>
<p>（1） 通过 2 个或者 3 个 RAM 的存储来实现 3X3 像素窗口；</p>
<p>（2） 通过 2 个或者 3 个 FIFO 的存储来实现 3X3 像素窗口；</p>
<p>（3） 通过 2 行或者 3 行 Shift_RAM 的存储来实现 3X3 像素窗口；</p>
<p>在此我们选用shift ram进行获取矩阵,shift ram是一个移位存储ram,这个 IP 支持一个时钟周期移位一个或多个 bit 的数据，位宽是可以设置的。下面图7和图8是 shift_ram 的移位示意图及仿真图:</p>
<p><a href="https://imgtu.com/i/W56U3D"><img src="https://z3.ax1x.com/2021/07/27/W56U3D.png" alt="W56U3D.png"></a></p>
<p><a href="https://imgtu.com/i/W56sEt"><img src="https://z3.ax1x.com/2021/07/27/W56sEt.png" alt="W56sEt.png"></a></p>
<p>如上图顺序可以容易看出，shift_ram 的工作方式是移位存储，后一个数据将前一个数据往前推，当填满一行的时候，跳到下一行再继续移位存储，相应的第一行的数据填满之后，会填上矩阵的第一行的数据，操作是同时的，如果进行均值滤波的话显然这样目标像素会是 0，以此类推，第二行移位存储完成，刚开始第一行的每一个像素点是目标像素，但是第三行还没有数据，所以第一行的目标像素滤波后显然是不准确的，等到把第三行数据填满后，进行原本第二行的滤波，这时目标像素周围均有数据，所以均值滤波会使图像的边缘不清楚。这是将每一行的宽度设置成 3，如果将宽度设置为 640,800 等分辨率的宽度，那么生成 3x3 矩阵就十分方便。同样的还可以生成 5x5、7x7 等矩阵用于图像处理的算法研究。</p>
<p>Xilinx Vivado 有自己的 Shift_RAM IP Core，不过这里只能缓存出来一行数据，我们这里需要多个 Shift_RAM IP Core。此外720p视频流一行有像素1280个数据，考虑到shift ram深度不能大于1024，所以我们用两个640深度的shift ram组成一个1280深度的shift ram，那么我们需要四个这样的IP核，进行循环移位得到3*3的矩阵，然后进行sobel边缘提取。边缘提取结果如图9和图10</p>
<p><a href="https://imgtu.com/i/W56bCT"><img src="https://z3.ax1x.com/2021/07/27/W56bCT.md.png" alt="W56bCT.md.png"></a></p>
<p><a href="https://imgtu.com/i/W56xbR"><img src="https://z3.ax1x.com/2021/07/27/W56xbR.png" alt="W56xbR.png"></a></p>
<p>（4）追踪算法</p>
<p>  在得到边缘化的数据后，将第一帧的目标数据进行保存，在后续的帧数据传来时，我们选择开窗操作，根据上一帧的定位结果，我们在此帧附近进行开窗，窗口不能太小，否则会丢失目标，在此处，我们选用的是上一帧结果往四周扩展40个像素点进行开窗，在此窗口内，从窗口左上角进行平移获取与目标大小一致的待检测矩阵，获得待检测矩阵后，为了防止周围背景的影响导致误判，需要将待检测矩阵与目标矩阵进行相与操作，相与操作后，对该二值矩阵进行简单求和，得到的结果与目标矩阵和进行对比，当结果最相近时，该待检测矩阵即是该帧的目标位置。下述描述FPGA对该算法的实现的主要模块</p>
<p>1、矩阵生成模块：</p>
<p>在1280*720的图片中，我们由上一帧定位结果判断下一帧的范围，即进行开窗，在此窗口内进行进行追踪定位，我们选取的窗口是212<em>128的窗口，在此窗口内，不断生成132\</em>48的待检测矩阵(无人机大小小于132*48时，进行w和h扩充为132*48).如下图</p>
<p><a href="https://imgtu.com/i/W5cuIP"><img src="https://z3.ax1x.com/2021/07/27/W5cuIP.md.png" alt="W5cuIP.md.png"></a></p>
<p>黄色边框代表一帧大小，红色框是选取的开窗大小，黑色框是需生成的待检测矩阵，待检测矩阵就是一个待检测的无人机目标.</p>
<p>由于1280*720图片过大，仿真长度过大，所以验证的时候是使用的一帧6*4，开窗大小是4*4，位于一帧中间位置，生成待检测矩阵是3*3.仿真图如图</p>
<p><a href="https://imgtu.com/i/W5c2Ix"><img src="https://z3.ax1x.com/2021/07/27/W5c2Ix.png" alt="W5c2Ix.png"></a></p>
<p>如上图仿真图所示，post_martix_clken代表矩阵有效信号，martix_p1共三bit，代表矩阵第一行的3个数据，martix_p2和martix_p3分别代表第二行和第三行，x和y代表此时矩阵左上角在窗口中的坐标(窗口左上角坐标记为(1,1))，sobel_data代表窗内信号，per_martix_clken代表sobel_data有效。仿真输入的窗口信号第一行的四个数据是：1010；第二行是：0101；第三行是：1010；第四行是0101。在进行仿真时，在4*4的窗口内，可以生成4个完整的矩阵，从仿真中可见共四个clk内有效。在四个使能有效时，对应的3*3矩阵是完全正确的，对应的(x,y)坐标是(1,1)，(1,2)，(2,1)，(2,2)与矩阵符合.</p>
<p>2、sum_h的仿真：如下图</p>
<p><a href="https://imgtu.com/i/W5g1Fx"><img src="https://z3.ax1x.com/2021/07/27/W5g1Fx.png" alt="W5g1Fx.png"></a></p>
<p>per_martix1_clken是输入数据的有效信号,如上图,在天蓝色处有两个clk的有效时钟,对应的输入是132’habc3d2e1f0和132’h167cccee55ff,对应的二进制分别是：1010101111000011110100101110000111110000和101100111110011001100 1110111001010101111111 11,含有1的个数分别是21和30.post_martix1_clken是输出结果的有效信号,如上图,在post_martix1_clken为1的时候，说明输出有效,此时输出是0x15和0x1e,十进制就是21和30.可以看出,求1个数的流水线处理模块结果正确.</p>
<p>3、上述模块是对待检测窗口行求和,下述sum模块是对返回的48行值进行累和sum模块的仿真：如下图</p>
<p><a href="https://imgtu.com/i/W5gW0s"><img src="https://z3.ax1x.com/2021/07/27/W5gW0s.png" alt="W5gW0s.png"></a></p>
<p>如图上图,per_martix1_clken是输入数据有效信号,sum代表待检测窗的累和,post_sum_clken代表输出累和数据的有效信号，如上图，仿真是给予的48行数据分别都是132’d1和132’d3,可以知道一行数据经过sum_h模块输出后分别是1和2，则48行累和分别是48和96，如上图仿真所示，在输出有效信号为1时，对应的结果分别是0x30和0x60即48和96，可以看出sum模块的流水线处理是没有问题的。</p>
<p>4、比较输出模块：如图</p>
<p>在比较模块中，sum是sum模块流水线输出的待检测窗口值，sum0是初始帧目标值，将两者对比，x和y是与sum对应的此时在大窗口中的坐标，x_current_frame和y_current_frame是此时帧定位的窗口坐标，若sum越接近sum0，则更新当前帧的坐标x_current_frame和y_current_frame为x和y，仿真图如上，设定的sum0是100，流水线输入sum值是0x2c，0x96,0x46,0x2h和0x5c，最终x_current_frame和y_current_frame更新为1和5，正常输出。</p>
<p>上述即是追踪算法的大致思想，适合FPGA的逻辑实现，其追踪性能很好，下面我们看一下追踪效果.</p>
<h2 id="三、追踪情况及性能参数"><a href="#三、追踪情况及性能参数" class="headerlink" title="三、追踪情况及性能参数"></a>三、追踪情况及性能参数</h2><h3 id="3-1、matlab追踪视频展示"><a href="#3-1、matlab追踪视频展示" class="headerlink" title="3.1、matlab追踪视频展示"></a>3.1、matlab追踪视频展示</h3><p>视频一追踪演示：<a href="https://www.bilibili.com/video/BV1mz4y197h1">视频一追踪演示</a></p>
<p>视频二追踪演示：<a href="https://www.bilibili.com/video/BV15i4y1x74k">视频二追踪演示</a></p>
<p>视频三追踪演示：<a href="https://www.bilibili.com/video/BV1yV411r7tn">视频三追踪演示</a></p>
<p>视频四追踪演示：<a href="https://www.bilibili.com/video/BV1pa4y1v7h5">视频四追踪演示</a></p>
<h3 id="3-2、matlab追踪AO值展示-四个AO值图展示如下图"><a href="#3-2、matlab追踪AO值展示-四个AO值图展示如下图" class="headerlink" title="3.2、matlab追踪AO值展示:四个AO值图展示如下图"></a>3.2、matlab追踪AO值展示:四个AO值图展示如下图</h3><p><a href="https://imgtu.com/i/W5Rdit"><img src="https://z3.ax1x.com/2021/07/27/W5Rdit.png" alt="W5Rdit.png"></a></p>
<p><a href="https://imgtu.com/i/W5Rssg"><img src="https://z3.ax1x.com/2021/07/27/W5Rssg.png" alt="W5Rssg.png"></a></p>
<p><a href="https://imgtu.com/i/W5RIQU"><img src="https://z3.ax1x.com/2021/07/27/W5RIQU.png" alt="W5RIQU.png"></a></p>
<p><a href="https://imgtu.com/i/W5ROF1"><img src="https://z3.ax1x.com/2021/07/27/W5ROF1.png" alt="W5ROF1.png"></a></p>
<p>四个视频追踪的平均AO值如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>视频一</th>
<th>视频二</th>
<th>视频三</th>
<th>视频四</th>
</tr>
</thead>
<tbody>
<tr>
<td>平均AO值</td>
<td>99.45%</td>
<td>98.18%</td>
<td>99.20%</td>
<td>99.04%</td>
</tr>
</tbody>
</table>
</div>
<p>从结果上看，全程没有追丢的情况发生，平均AO值全部达到98%以上。</p>
<p>3.3、FPGA板级验证：板级验证图如下图</p>
<p><a href="https://imgtu.com/i/W5W4AA"><img src="https://z3.ax1x.com/2021/07/27/W5W4AA.png" alt="W5W4AA.png"></a></p>
<p><a href="https://imgtu.com/i/W5WIht"><img src="https://z3.ax1x.com/2021/07/27/W5WIht.png" alt="W5WIht.png"></a></p>
<p>至此差不多结束了，具体上位机软件等可以从集创赛官网找，往届题目应该会有保存。</p>
]]></content>
      <categories>
        <category>FPGA</category>
      </categories>
      <tags>
        <tag>FPGA</tag>
      </tags>
  </entry>
  <entry>
    <title>深度学习常用词汇及常用编程语法--不断更新中</title>
    <url>/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<p>之前说要更新一下本科期间做的一些东西，然后再讲解一下加速器如何在FPGA上搭建，结果太忙了。。。。木得时间，后面有时间慢慢补上。</p>
<p>最近读了faster rcnn和yolo的源码，里面用到的一些函数自己不常用或者用法新奇或者有时候自己忘掉的，而且最先读faster rcnn，two-stage的源码的确比较复杂，原理很快看懂，读源码花了几天才搞明白，写篇博客不断更新记录一下遇到的一些函数和用法，防止遗忘，后续手机端查看也方便。跑faster rcnn深有感触，这玩意不是给普通人玩的，backbone用mobilenet v2情况下debug一下用10s多，内存耗掉四个g，要是restnet50就更夸张了，还是老老实实等服务器吧。。。(更新：拿到了，4块3090)</p>
<a id="more"></a>
<h2 id="常用名词"><a href="#常用名词" class="headerlink" title="常用名词"></a>常用名词</h2><p>backbone:主干，object detection中提取特征的cnn网络简称backbone</p>
<p>bounding box ：边界框，object detection中框住物体的边界框</p>
<p>ground truth:目标的真实位置</p>
<p>logits:意思是输入时候不需要sigmoid概率化</p>
<h2 id="简称"><a href="#简称" class="headerlink" title="简称"></a>简称</h2><p>fpn:feature pyramid network 图像金字塔网络，多个遍历的窗口</p>
<p>rpn：region proposal network 区域提名网络</p>
<h2 id="Pytorch-amp-Numpy基础原理类"><a href="#Pytorch-amp-Numpy基础原理类" class="headerlink" title="Pytorch &amp; Numpy基础原理类"></a>Pytorch &amp; Numpy基础原理类</h2><p>别人讲的太好了，所以有些就直接放链接了。</p>
<p>dir(类)可以查看类中的方法和属性，比如range(10)打印不可视化，可以dir(range(10)),可以看到有str和repr方法，但其实这两个打印的都不可视化，可以用list强制转换就可以打印了(一些自定义类中，可以自己重写__len__，这样就可以使用len()函数了)。</p>
<h3 id="1、广播机制。"><a href="#1、广播机制。" class="headerlink" title="1、广播机制。"></a>1、广播机制。</h3><p>广播机制主要是在两个tensor/array进行运算时候，维度不一时候用到的机制，由于广播机制元素是从内往外扩充的，所以两个tensor/array之间的维度大小从内到外要一致，不一致的两个tensor/array至少有一个维度为1或者有一个压根没那个维度。</p>
<p>具体介绍的链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://zhuanlan.zhihu.com/p/60365398">2个规则弄懂numpy的broadcast广播机制 - 知乎 (zhihu.com)</a></p>

          </blockquote>
<h3 id="2、存储和视图。"><a href="#2、存储和视图。" class="headerlink" title="2、存储和视图。"></a>2、存储和视图。</h3><p>torch中经常出现共享内存，只是改变视图和stride，而不分配新的内存空间，比如转置、expand等，<strong>可以用tensor.clone()来拷贝一份得到新的内存空间</strong>，共享内存很大程度是为了应对深度学习超大的参数量，这样不需要新的内存空间，也不需要变动内存存储顺序，可以起到节约空间和加速运行的效果。具体介绍如下：</p>
<blockquote class="blockquote-center">
            <p><a href="http://www.360doc.com/content/19/1130/10/32196507_876476008.shtml">由浅入深地带你了解分析张量</a></p>

          </blockquote>
<p>torch中可以有很多用.来进行调用的方法，可以帮我们查看一些信息，显然就是因为torch底层大多数是以面向对象的类封装的，所以arrya用点调用方法显然比tensor少。</p>
<p>tensor.size(),可以得到其shape</p>
<p>tensor.len()，这个真没有，其实不是没有，只不过用的是tensor.<strong>len</strong>而已，可以用len(tensor)，len内部会调用tensor.<strong>len</strong>,</p>
<h3 id="善用？查看函数"><a href="#善用？查看函数" class="headerlink" title="善用？查看函数"></a>善用？查看函数</h3><h2 id="Pytorch-amp-Numpy"><a href="#Pytorch-amp-Numpy" class="headerlink" title="Pytorch &amp; Numpy"></a>Pytorch &amp; Numpy</h2><p>以下函数都在Python3中得到验证</p>
<p><strong>0、tensor生成</strong></p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.randn((2,3))#生成2行3列的正态分布随机数</span><br><span class="line">b&#x3D;torch.randint(3,10,(4,4))#Returns a tensor filled with random integers generated uniformly between :attr:&#96;low&#96; (inclusive) and :attr:&#96;high&#96; (exclusive).均匀分布，产生low-high的整数，只填一个数默认作为high，low默认是0</span><br><span class="line">c&#x3D;torch.randperm(5)#Returns a random permutation of integers from &#96;&#96;0&#96;&#96; to &#96;&#96;n - 1&#96;&#96;,返回一个0到n-1的随机排序.可以用于随机采样样本，比如采样前百分之70。</span><br><span class="line">d&#x3D;torch.rand((2,3))#2行3列的[0,1)的均匀分布，uniform distribution</span><br><span class="line">e&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a,&quot;\n&quot;,b,&quot;\n&quot;,c,&quot;\n&quot;,d,&quot;\n&quot;,e)</span><br></pre></td></tr></table></figure>
<p>输入如下，其中torch.randint的三个参数含义分别是low，high和size(即shape)，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 0.8787,  0.5344,  0.3134],</span><br><span class="line">       [ 0.9224, -1.2589,  1.7708]]) </span><br><span class="line">tensor([[9, 5, 4, 4],</span><br><span class="line">       [3, 7, 4, 8],</span><br><span class="line">       [8, 6, 9, 9],</span><br><span class="line">       [9, 8, 5, 8]]) </span><br><span class="line">tensor([1, 4, 3, 2, 0]) </span><br><span class="line">tensor([[0.1877, 0.8270, 0.3400],</span><br><span class="line">       [0.2405, 0.6493, 0.7478]]) </span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">       [4., 5., 6.]])</span><br></pre></td></tr></table></figure>
<p><strong>1、zip函数</strong></p>
<p>python自带函数 作用：将np/tensor/dict/list等可迭代对象元素组合，这个元素是指第0维的元素。在目标追踪网络中常出现，用于拼接x、y、w、h等等。其格式：zip``(iterable1,iterable2, …)，即输入的是可迭代对象。</p>
<p>例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;[1,2,3]</span><br><span class="line">b&#x3D;[4,5,6]</span><br><span class="line">c&#x3D;zip(a,b)</span><br><span class="line">print(c,type(c))</span><br><span class="line">print(list(c))</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>结果如下，直接print其实调用的是魔法函数<strong>str</strong>，由于未重写该方法，所以其和<strong>repr</strong>方法输出是一致，就是类名 + object at + 地址。而通过list函数，可以zip类的结果变成可视化的list列表，列表中每个元素都是拼接的值，<strong>值得注意的是，在list(c)后，不知道list函数中调用了c自身的某种方法，但这种方法改变了c的某些元素值，使得list(c)再list(c)时候，得到的是一个空列表，所以这个c传递更类似于引用传递，改变了c自身。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;zip object at 0x000001B48B796200&gt; &lt;class &#39;zip&#39;&gt;</span><br><span class="line">[(1, 4), (2, 5), (3, 6)]</span><br></pre></td></tr></table></figure>
<p>上述是可迭代对象list，下面测试一下可迭代对象dict：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#v1,v2,v3可是是任何可迭代对象，如：字符串、列表、元祖、字典</span><br><span class="line">v1 &#x3D; &#123; 1 : 11 , 2 : 22 &#125; #此处可迭代对象为字典</span><br><span class="line">v2 &#x3D; &#123; 3 : 33 , 4 : 44 &#125;</span><br><span class="line">v3 &#x3D; &#123; 5 : 55 , 6 : 66 &#125;</span><br><span class="line"> </span><br><span class="line">v &#x3D; zip (v1,v2,v3)   #压缩</span><br><span class="line">print ( list (v))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出，对于字典，压缩组合是对于索引号的。(通过索引号索引值，用法和np类似)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(1, 3, 5), (2, 4, 6)]</span><br></pre></td></tr></table></figure>
<p>通过zip(*压缩组合元素)可以进行解压会原来样式（未必是完全一样，因为压缩的时候可能就不完整）。例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(list(zip(*zip (v1,v2,v3))))</span><br></pre></td></tr></table></figure>
<p>结果如下，<strong>为什么不直接list(zip(*v))呢？上面说过了，经过list后就会改变原值，会变成空元素，压缩得到的也是空的。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[(1, 2), (3, 4), (5, 6)]</span><br></pre></td></tr></table></figure>
<p>zip由于是组合，list后可以看出，其是可以变成list列表的，而list是Iterable的，即可迭代对象，自然可以用for，而list中元素都是一个个元组，元组单个元素赋值给单个变量，可以通过逗号进行(要么一个变量直接接受的赋值是一个元组，要么变量个数就是元组内元素个数，否则报错)，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;[1,2,3,4]</span><br><span class="line">b&#x3D;[4,5,6,7]</span><br><span class="line">c&#x3D;[1,3,5,7]</span><br><span class="line">for i in zip(a,b):</span><br><span class="line">    print(i)</span><br><span class="line">for i,j in zip(a,b):</span><br><span class="line">    print(i,j)</span><br><span class="line">for i,j,k in zip(a,b,c):</span><br><span class="line">    print(i,j,k)</span><br><span class="line">(1, 4)</span><br><span class="line">(2, 5)</span><br><span class="line">(3, 6)</span><br><span class="line">(4, 7)</span><br><span class="line">1 4</span><br><span class="line">2 5</span><br><span class="line">3 6</span><br><span class="line">4 7</span><br><span class="line">1 4 1</span><br><span class="line">2 5 3</span><br><span class="line">3 6 5</span><br><span class="line">4 7 7</span><br></pre></td></tr></table></figure>
<p><strong>1.1、元组赋值方式</strong></p>
<p>元组赋值方式，要么直接赋值给一个变量，那个变量就是接收到一个元组，即那个变量也是元组方式，要么接收值的变量个数和元组内元素数量一样（否则报错），变量之间用逗号隔开。</p>
<p><strong>2、for循环使用方法</strong></p>
<p>for常用功能两个：循环遍历和列表生成式 (其实列表生成式本质也是循环遍历，都用到了<strong>iter</strong>和<strong>next</strong>方法，即迭代器生成和不断获取下一个元素方法)。</p>
<p>本节看完后可以继续看迭代器扩展—生成器等等：<a href="http://gezhilai.com/2021/12/20/算法精粹-一/">算法精粹(一) | Apers’ Blog (gezhilai.com)</a></p>
<p>2.1、循环遍历</p>
<p>循环遍历常用的 for i in range(start,end,stride)：不用细讲(需要注意的是，python语法中需要起始终止位置的，似乎从来不包括end的，比如切片，和verilog区别度显然)。此处主要想讲的是循环遍历的原理，for循环中，起始本质是使用了<strong>iter</strong>制造迭代器，然后用<strong>next</strong>方法不断顺序的读取下一个数据，不可回退，直至最后无元素可读，此刻抛出StopIteration 异常(这个异常估计就是try except处理了)，然后退出。下面介绍一下迭代器等。</p>
<p>2.1.1、Iterable</p>
<p>Iterable即可迭代对象，循环遍历的对象必须是可迭代对象，其是一个元素个数明确且可迭代的对象(可迭代即可遍历)，常用的有list、np、tensor、str、tuple、st、dict等</p>
<p>可迭代对象自带<strong>iter</strong>方法，通过<strong>iter</strong>即可得到迭代器Iterator(迭代器)，比如a是个list，则可以通过a.<strong>lter</strong>调用迭代器方法，不过其实这个方法不需要我们调用，for的时候会自行调用。这里可以调用一下试试，例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.zeros((1,2))</span><br><span class="line">print(a.__iter__)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：其中method-wrapper的意思是”包装的方法”，后面就是数据类型 + object(物体，对象)+at+地址。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;method-wrapper &#39;__iter__&#39; of numpy.ndarray object at 0x000001B48BF1D0D0&gt;</span><br></pre></td></tr></table></figure>
<p>2.1.2、Iterator</p>
<p>Iterator即迭代器，Iterable对象通过<strong>iter</strong>方法可以得到Iterator(注意得到Iterator并不是返回，需要得到返回Iterator类型的，用iter()强制转换)，Iterable即可迭代对象是一个元素个数已知且可遍历的对象，通过<strong>iter</strong>方法就得到了个数未知但依旧可遍历的对象Iterator，然后通过<strong>next</strong>进行遍历Iterator，<strong>next</strong>方法不断往下取数据，无数据取出时抛出StopIteration 异常停止迭代。</p>
<p>其实可以看出Iterator和Iterable很像，其实Iterable调用<strong>iter</strong>方法得到的Iterator，是一个继承Iterable的子类</p>
<p>2.1.3、判别Iterable和Iterator</p>
<p>判断一个变量的类型，可以通过isinstance()或者type()函数，但两者有所区别，type不认为子类是和父类是同一个类型，但isinstance则认为子类属于父类的类型，父类不属于子类的类型。推荐使用isinstance，因为平时往往认为子类和父类是一个类别的。(instance中是子类属于父类，父类不是子类，针对父类的函数，子类都可以传进去，还可以正确运行：看缪雪峰的面向对象章，这就是类的多态，)</p>
<p>isinstance例子如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;1</span><br><span class="line">print(isinstance(a,int),isinstance(a,str),isinstance(a,dict),isinstance(a,(int,str,dict,list)))</span><br></pre></td></tr></table></figure>
<p>结果如下，判别属于某个类别，isinstance第二个元素可以是元组。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True False False True</span><br></pre></td></tr></table></figure>
<p>在看一下isinstance和type区别，如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class A():</span><br><span class="line">    pass</span><br><span class="line">class B(A):</span><br><span class="line">    pass</span><br><span class="line">print(isinstance(A(),A),isinstance(B(),A),type(B())&#x3D;&#x3D;A)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True True False</span><br></pre></td></tr></table></figure>
<p>下面看一下迭代器的类别：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from collections import Iterator,Iterable</span><br><span class="line">a&#x3D;np.zeros((1,2))</span><br><span class="line">print(isinstance(a,Iterable))</span><br><span class="line">print(isinstance(a,Iterator))</span><br><span class="line">print(len(a),a.__len__())</span><br></pre></td></tr></table></figure>
<p>结果如下，np数组是可迭代对象，可以看出父类不属于子类；同时前面也提过，可迭代对象可以有长度属性，其实len()的本质就是调用了Iterable中的<strong>len</strong>方法，不过不加括号的话只会显示方法+类+object+at+地址（具体原因不知）。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br><span class="line">1 1</span><br></pre></td></tr></table></figure>
<p>下面可以看出子类输入父类，<strong>通过iter函数即可强制将变量转成迭代器</strong>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a1&#x3D;iter(a)#iter转换成迭代器</span><br><span class="line">print(isinstance(a1,Iterable))</span><br><span class="line">print(isinstance(a1,Iterator))</span><br><span class="line">print(len(a1))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出a1既属于可迭代对象，也属于迭代器，即子类既是属于子类也属于父类，用len的时候，报错了，因为迭代器长度是未知的，无法用len方法，只能用<strong>next</strong>方法。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-163-dff46c0da30e&gt; in &lt;module&gt;</span><br><span class="line">      2 print(isinstance(a1,Iterable))</span><br><span class="line">      3 print(isinstance(a1,Iterator))</span><br><span class="line">----&gt; 4 print(len(a1))</span><br><span class="line"></span><br><span class="line">TypeError: object of type &#39;iterator&#39; has no len()</span><br></pre></td></tr></table></figure>
<p><strong>记住需要返回迭代器得用iter(),需要返回取迭代器值的，用next,直接调用<strong>iter</strong>和<strong>next</strong>没有想要的返回值。</strong></p>
<p>下面看一下迭代器利用next取值：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a2&#x3D;np.arange(0,10,1).reshape(2,-1)</span><br><span class="line">print(a2)</span><br><span class="line">a2&#x3D;iter(a2)</span><br><span class="line">print(next(a2))</span><br><span class="line">print(next(a2))</span><br><span class="line">print(next(a2))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出，通过next方法不断往下取值，当没有值可取时，抛出StopIteration</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0 1 2 3 4]</span><br><span class="line"> [5 6 7 8 9]]</span><br><span class="line">[0 1 2 3 4]</span><br><span class="line">[5 6 7 8 9]</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">StopIteration                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-182-473194238b9f&gt; in &lt;module&gt;</span><br><span class="line">      4 print(next(a2))</span><br><span class="line">      5 print(next(a2))</span><br><span class="line">----&gt; 6 print(next(a2))</span><br><span class="line"></span><br><span class="line">StopIteration: </span><br></pre></td></tr></table></figure>
<p>2.1.4 for本质</p>
<p>for的本质就是Iterator通过不断调用next()实现的。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.arange(0,5,1)</span><br><span class="line">for i in a:</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<p>这个for循环等价于如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a1&#x3D;iter(a)</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        i&#x3D;next(a1)</span><br><span class="line">    except StopIteration:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>
<p>之前说过了，next是不可以回退的，所以一个Iterator只能遍历一次即用不了了，但Iterable是可以多次遍历的，所以每次for时都用到<strong>iter</strong>，每次<strong>iter</strong>都会得到一个新的Iterator，再next就好了。</p>
<p>2.2、列表生成式、集合生成式、字典生成式</p>
<p>列表生成式如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.arange(4)</span><br><span class="line">b&#x3D;[i for i in a]</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[0, 1, 2, 3]</span><br></pre></td></tr></table></figure>
<p>字典生成式如下，其中items()方法是输入字典类型，返回list形式的可迭代对象。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#d &#x3D; &#123;key: value for (key, value) in iterable&#125;</span><br><span class="line">d1 &#x3D; &#123;&#39;x&#39;: 1, &#39;y&#39;: 2, &#39;z&#39;: 3&#125;</span><br><span class="line">d2 &#x3D; &#123;k: v for (k, v) in d1.items()&#125;</span><br><span class="line">print(d2)</span><br><span class="line">print(d1.items())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#39;x&#39;: 1, &#39;y&#39;: 2, &#39;z&#39;: 3&#125;</span><br><span class="line">dict_items([(&#39;x&#39;, 1), (&#39;y&#39;, 2), (&#39;z&#39;, 3)])</span><br></pre></td></tr></table></figure>
<p>集合生成式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#集合生成式</span><br><span class="line">s1&#x3D;&#123;x for x in range(10)&#125;</span><br><span class="line">print(s1)#集合无序性？所以没法切片</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;</span><br></pre></td></tr></table></figure>
<p>谈到了各类生成式，却没说元组生成式，貌似按照上述的做法，在括号内进行for得到的就是元组生成式，然而其实不是这样的，其实得到的是Generator。下面讲一下生成器</p>
<p>2.2.1、Generator生成器</p>
<p>先来看一下用法：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#生成器</span><br><span class="line">a&#x3D;(i**2 for i in [1,2,3,4])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出显然得到的不是一个元组，而是一个generator</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;generator object &lt;genexpr&gt; at 0x000001B48B769970&gt;</span><br></pre></td></tr></table></figure>
<p>生成器和迭代器很像，我们用next试试：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">9</span><br><span class="line">16</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">StopIteration                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-194-2251f521890f&gt; in &lt;module&gt;</span><br><span class="line">      3 print(next(a))</span><br><span class="line">      4 print(next(a))</span><br><span class="line">----&gt; 5 print(next(a))</span><br><span class="line"></span><br><span class="line">StopIteration: </span><br></pre></td></tr></table></figure>
<p>似乎这个和迭代器没有区别呀，似乎就是含有1,4,9,16的迭代器。</p>
<p>其实可以理解为生成器Generator就是特殊的迭代器，只不过其生成器一种迭代遍历过程中才计算的迭代器，就是说它存储的是1,2,3,4，在迭代过程中，才进行了平方操作，即next取值的时候进行了计算，可以理解为：生成器的元素在访问前不会生成，只有当访问时才会生成；如果继续向后访问，那么当前的元素会销毁，这个也可以理解，毕竟next是不可以回头的，之前的数据没有意义，所以销毁节约内存。<strong>而生成器的一种生成方式是将列表生成式改为小括号包裹。</strong></p>
<p>下面谈一下生成器的本质(引用了)：</p>
<ul>
<li>生成器本质上是一个函数</li>
<li><strong>当一个生成器被调用时，它返回一个生成器对象，而不用执行该函数。 当第一次调用 <code>next()</code>方法时，函数向下执行，如果遇到yield则返回 <code>yield 后面的</code>值。 再次调用<code>next()</code>方法时，函数从上次结束的位置继续向下执行，如果遇到yield则返回 <code>yield 后面的</code>值。</strong></li>
<li>可以使用yield来定义一个生成器。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(&quot;\n----使用yield生成generator-------&quot;)</span><br><span class="line">def ge():</span><br><span class="line">    print(&quot;第一次yield&quot;)</span><br><span class="line">    yield 1</span><br><span class="line">    print(&quot;第二次yield&quot;)</span><br><span class="line">    yield 2</span><br><span class="line">    print(&quot;第三次yield&quot;)</span><br><span class="line">    yield 3</span><br><span class="line">print(type(o))</span><br><span class="line">o &#x3D; ge()</span><br><span class="line">print(next(o))</span><br><span class="line">print(next(o))</span><br><span class="line">print(next(o))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出o是一个生成器。即生成器其实是可以通过yield关键字来得到。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">----使用yield生成generator-------</span><br><span class="line">&lt;class &#39;generator&#39;&gt;</span><br><span class="line">第一次yield</span><br><span class="line">1</span><br><span class="line">第二次yield</span><br><span class="line">2</span><br><span class="line">第三次yield</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<ul>
<li>生成器本质上是一个函数，如果想要获取这个函数的返回值，我们需要使用异常捕获来获取这个返回值：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def fib(max):</span><br><span class="line">    n,a,b &#x3D; 0,0,1</span><br><span class="line">    while n &lt;max:</span><br><span class="line">        yield b</span><br><span class="line">        a,b &#x3D;b,a+b</span><br><span class="line">        n &#x3D; n+1</span><br><span class="line">    return &#39;done&#39;</span><br><span class="line"></span><br><span class="line">print(&quot;\n-----尝试获得函数返回值------&quot;)</span><br><span class="line">gg&#x3D;fib(6)</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        x&#x3D;next(gg)</span><br><span class="line">        print(&quot;g:&quot;,x)</span><br><span class="line">    except StopIteration as e:</span><br><span class="line">        print(&#39;返回值等于:&#39;,e.value)</span><br><span class="line">        break</span><br><span class="line">-----尝试获得函数返回值------</span><br><span class="line">g: 1</span><br><span class="line">g: 1</span><br><span class="line">g: 2</span><br><span class="line">g: 3</span><br><span class="line">g: 5</span><br><span class="line">g: 8</span><br><span class="line">返回值等于: done</span><br></pre></td></tr></table></figure>
<ul>
<li>既可以使用next()来迭代生成器，也可以使用for来迭代：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def ge():</span><br><span class="line">    print(&quot;第一次yield&quot;)</span><br><span class="line">    yield 1</span><br><span class="line">    print(&quot;第二次yield&quot;)</span><br><span class="line">    yield 2</span><br><span class="line">    print(&quot;第三次yield&quot;)</span><br><span class="line">    yield 3</span><br><span class="line">o &#x3D; ge()</span><br><span class="line"></span><br><span class="line">print(&quot;\n---迭代generator的方法--------&quot;)</span><br><span class="line">for x in o:</span><br><span class="line">    print(x)#相当于进入到generator函数中，执行下去并得到返回值</span><br><span class="line">---迭代generator的方法--------</span><br><span class="line">第一次yield</span><br><span class="line">1</span><br><span class="line">第二次yield</span><br><span class="line">2</span><br><span class="line">第三次yield</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<p>至此for结束，参考链接如下:</p>
<blockquote class="blockquote-center">
            <p><a href="https://zhuanlan.zhihu.com/p/53664886">Python笔记整理 迭代器和生成器</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://www.cnblogs.com/progor/p/8414550.html">字典生成式、集合生成式、生成器</a></p>

          </blockquote>
<p><strong>3、拼接函数和分割函数、增加元素等等</strong>。</p>
<p>3.1、torch.stack会增加维度，传入时候需要是<strong>tensor组成的元组或列表，普通列表和元组不行</strong>，如(a,a)或[a,a]，<strong>扩充的维度如果是n，那么就是对n-1维度下的对应位置的元素拼接起来</strong>，然后加个维度。(np里也有stack,用法略有不同)</p>
<p>如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([[[1,2,3],[4,5,6]]])</span><br><span class="line">b&#x3D;torch.stack((a,a),dim&#x3D;3)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>结果如下，dim=3即是在第二维度后面进行扩充，将第二维度下面的元素对应位置组起来加个维度。值得注意的是，torch中习惯用dim，而np习惯用axis。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[[[1, 1],</span><br><span class="line">          [2, 2],</span><br><span class="line">          [3, 3]],</span><br><span class="line"></span><br><span class="line">         [[4, 4],</span><br><span class="line">          [5, 5],</span><br><span class="line">          [6, 6]]]])</span><br></pre></td></tr></table></figure>
<p>如果dim=2，结果如下，即是在第一维度后面加一个维度，则是将第一维度下的元素对应位置拼接起来组成一个维度。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[[[1, 2, 3],</span><br><span class="line">          [1, 2, 3]],</span><br><span class="line"></span><br><span class="line">         [[4, 5, 6],</span><br><span class="line">          [4, 5, 6]]]])</span><br></pre></td></tr></table></figure>
<p>append</p>
<p>3.2、torch.cat</p>
<p>cat不增加维度，传入时候需要是<strong>tensor组成的元组或列表，普通列表和元组不行</strong>，例子:cat([a,b,c],dim=1)。对于维度n，cat只是对该维度内元素进行<strong>顺序拼接，不增加维度。</strong></p>
<p>如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([1,2,3])</span><br><span class="line">b&#x3D;torch.Tensor([1,2,3])</span><br><span class="line">c&#x3D;torch.cat([a,b],dim&#x3D;0)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br></pre></td></tr></table></figure>
<p>由于cat是顺序拼接，其实上述的这个结果很容易用其他方式实现，比如如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(torch.Tensor(np.append(np.array(a),np.array(b))))</span><br><span class="line">print(torch.Tensor([list(a),list(b)]).reshape(-1))</span><br></pre></td></tr></table></figure>
<p>结果如下,<strong>可以看出通过append或者reshape都可以实现，但是torch中好像没有append函数，所以就先用np作为中介了。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br></pre></td></tr></table></figure>
<p>在torch中有cat，numpy中自然有对应的函数，其就是<strong>np.concatenate((a,b),axis = 1)</strong>,功能和torch.cat一致，与stack的区别仅在于没有扩充维度罢了。最典型的维度变换，cifar-10读进来的图片是batch*3*32*32,即一个数据有3072个点，前1024个为r，后1024个为g，最后1024个为b，所以读入后需要reshape(32,32,3),不能是reshape(3,32,32)，因为numpy的reshape是按顺序来的，这样就错了。reshape(32,32,3)后，我们希望可以imshow观测，imshow需要的格式是(32,32,3)这个3代表rgb，所以我们在得到a=img.reshape(3,32,32)后，先进行a0=a[0,:,:],a1=a[1,:,:],a1=a[2,:,:],拆开3个通道，再b0=a0.reshape(32,32,1),b1=a1.reshape(32,32,1),b2=a2.reshape(32,32,1),之所以多弄一个维度是为了后续拼接，然后np.concatenate((b0,b1,b2),axis=2)即可得到(32,32,3)的形状。</p>
<p>不过np中还有类似于stack的，就是np.vstack和np.hstack，vstack是在第0维度上拼接，即竖向拼接，hstack是在第1维度上拼接，即横向拼接。比如如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.vstack([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],[[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">np.hstack([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]],[[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<p>结果分别如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br></pre></td></tr></table></figure>
<p>3.3、split函数</p>
<p>第二个参数是表明其在第几个维度上分割，第一个元素代表分割的步长，函数返回的是元组类型，元组中元素是tensor类型。比如faster rcnn中，a的shape代表[batch_size,所有预测特征层anchors总数]，为了把每个预测特征层分开，就可以如下操作，假设第一个预测特征层anchor总数为3，第二个预测特征层anchors是2；此法适合之前拼接时候记住个数的，然后在分割开。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">a&#x3D;torch.tensor([[1,2,3,4,5],[6,7,8,9,10]])</span><br><span class="line">b&#x3D;[3,2]</span><br><span class="line">c&#x3D;a.split(b,1)</span><br><span class="line">print(a,&quot;\n&quot;,c)</span><br></pre></td></tr></table></figure>
<p>输出如下，说明分割是形参传递，并非引用，不改变原始a。返回的结果是元组形式。元组和list，tensor，np都是可迭代格式。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[ 1,  2,  3,  4,  5],</span><br><span class="line">        [ 6,  7,  8,  9, 10]]) </span><br><span class="line"> (tensor([[1, 2, 3],</span><br><span class="line">        [6, 7, 8]]), tensor([[ 4,  5],</span><br><span class="line">        [ 9, 10]]))</span><br></pre></td></tr></table></figure>
<p><strong>torch中split和numpy不一样，numpy中split是按照下标切的，如下的[0,1]就是按照下标0及其之前切成一个，0到1之间切成一个，剩下的切作为一个</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(np.split(np.array([[1,2,3],[4,5,6]]),[0,1]))</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出其得到的是列表，而torch是元组。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[array([], shape&#x3D;(0, 3), dtype&#x3D;int32), array([[1, 2, 3]]), array([[4, 5, 6]])]</span><br></pre></td></tr></table></figure>
<p>3.4、分割 tensor.unbind</p>
<p>tensor.unbind，比如[[1,2,3],[4,5,6]],则a.unbind(1)即在维度1上分割，得到[1,4],[2,5],[3,6]，其是把设定维度上每个元素分割开，得到一个元组。</p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a.unbind(0),&quot;\n&quot;,a.unbind(1))</span><br><span class="line">(tensor([1., 2., 3.]), tensor([4., 5., 6.])) </span><br><span class="line">(tensor([1., 4.]), tensor([2., 5.]), tensor([3., 6.]))</span><br></pre></td></tr></table></figure>
<p><strong>4.维度变化</strong></p>
<p>1、tensor.fltten（array的用法不一样）</p>
<p>flatten(0,-2)<strong>即代表从第0维度一直拉平到倒数第二维度截止，拉平的元素以倒数第二个维度的元素为一个整体</strong>，即只有倒数第二个维度的元素作为总体进行拉平，拉平应该是这样格式[[1,2,3],[4,5,6],[7,8,9]]。</p>
<p>flatten(1)代表从1维度一直拉平到最后。</p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.flatten(0,-1))</span><br><span class="line">print(a.flatten(0,-2))</span><br><span class="line">print(a.flatten(0))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br></pre></td></tr></table></figure>
<p>4.2、reshape</p>
<p>用于改变维度，常用-1进行自动填充某个维度。tensor.reshape((0维度大小，1维度大小。。。))。其可以用于不连续空间的维度调整</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.randn((2,3))</span><br><span class="line">print(a)</span><br><span class="line">print(a.reshape((3,2)))</span><br><span class="line">tensor([[-0.4904,  0.7570, -0.5010],</span><br><span class="line">        [ 0.7374,  2.9273, -2.4853]])</span><br><span class="line">tensor([[-0.4904,  0.7570],</span><br><span class="line">        [-0.5010,  0.7374],</span><br><span class="line">        [ 2.9273, -2.4853]])</span><br></pre></td></tr></table></figure>
<p>4.3、Tensor.view</p>
<p>view也可以用于改变维度，和reshape类似，但是有区别，<strong>和reshape的区别是view要内存连续存储，reshape可以不连续</strong>。<strong>需要注意的是，array的view是用来改变dtype和type的，用法不一样，可以用array.view？来查看函数的help。</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(b.view(-1))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.],</span><br><span class="line">        [5., 6.]])</span><br></pre></td></tr></table></figure>
<p>4.3、tensor.expand 和 tensor.expanda_as</p>
<p>expand（）函数的功能是用来扩展张量中某维数据的尺寸，它返回输入张量在某维扩展为更大尺寸后的张量。其扩展维度的本质是和广播机制一致(详见本文的广播机制)，即维度从后往前和扩展成的维度对比，必须要完全一致，不一致的必须是该tensor原始维度为1(广播机制是任意一个为1即可)。<strong>值得注意的是，扩展张量不会分配新的内存，只是在存在的张量上创建一个新的视图，其原始tensor和处理后的tensor是共享内存的。关于视图和存储，详见本文的tensor存储和视图原理</strong></p>
<p>使用如下：tensor.expand((第0维度大小，第1维度大小，第2维度大小。。。))例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1],[4]])</span><br><span class="line">b&#x3D;a.expand(2,3)</span><br><span class="line">print(a,&quot;\n&quot;,b)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[1.],</span><br><span class="line">        [4.]]) </span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [4., 4., 4.]])</span><br></pre></td></tr></table></figure>
<p>在来看一下内存共享，证明通过expand得到的b只是一个新的视图，不是一个新的存储：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">b[1,1]&#x3D;5</span><br><span class="line">print(a,&quot;\n&quot;,b)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出b变了一个值，导致一行全变了，a也变了，可以看出b只是一个新视图，对应新的stride而已，而没有新的空间，如果需要新的空间，可以使用<strong>b=tensor.copy()</strong>得到的b就是新存储。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[1.],</span><br><span class="line">        [5.]]) </span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [5., 5., 5.]])</span><br></pre></td></tr></table></figure>
<p>对于expand_as其实作用和expand是一致的，只是expand需要的参数直接是shape，而expand_as需要的参数直接是tensor变量，就可以将变量变为传入tensor维度一样了。例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3,4],[5,6,7,8]])</span><br><span class="line">b&#x3D;torch.Tensor([5,6,7,8])</span><br><span class="line">print(b.expand_as(a))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[5., 6., 7., 8.],</span><br><span class="line">        [5., 6., 7., 8.]])</span><br></pre></td></tr></table></figure>
<p>4.4 None—增加维度（可以用于np和tensor中，list不可以，list可以先转换在转回来）</p>
<p>None在i维度之后出现，其实就是给第i维度的元素增加一个维度，就是加个[]。在0维度上是None，就是给np数组总体多加个括号作为维度，比如a[:,None]就是在第0维度后加上一个维度，a[None]就是在最外面加上一个维度。。</p>
<p>如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;np.zeros((4))</span><br><span class="line">b&#x3D;np.ones((3))</span><br><span class="line">c&#x3D;np.ones((4,5))</span><br><span class="line">a&#x3D;a[:,None]</span><br><span class="line">print(a)</span><br><span class="line">b&#x3D;b[None,:]</span><br><span class="line">print(b)</span><br><span class="line">c&#x3D;c[:,None]</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下，在0维度后加None，其实就是给原来0维度的元素加个维度，在0维度之前加None，就总给np数组总体加个维度。注意：变量c有两个维度，所以中间加个None，应该是c[:,None,:]，如果None后还有维度，但是我们没写，默认是：，如果写了，则服从切片原理。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[[0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]]</span><br><span class="line">[[1. 1. 1.]]</span><br><span class="line">[[[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]]</span><br></pre></td></tr></table></figure>
<p>None除了增加维度外，还可以用于测试一个变量是否被定义，比如我们需要一个length变量来存储输入数据的长度，如果length没有定义，那么就丢出错误或者定义一个。例子如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if a is None:</span><br><span class="line">    print(&quot;a 不存在&quot;)</span><br><span class="line">    raise ValueError(&quot;a should not be None when box_predictor &quot;</span><br><span class="line">                                 &quot;is not specified&quot;)#圆括号隐式转换，加一个括号中，可以应对一行语句太长，分为多行。</span><br><span class="line">else:</span><br><span class="line">    print(&quot;a 是存在的&quot;)</span><br><span class="line">if length is None:</span><br><span class="line">    raise ValueError(&quot;length should not be None when box_predictor &quot;</span><br><span class="line">                                 &quot;is not specified&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;length 是存在的&quot;)</span><br></pre></td></tr></table></figure>
<p>结果如下，a是存在的，所以执行的是else语句，而length是不存在的，所以我们认为抛出一个错误。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a 是存在的</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-21-e7b9afa88565&gt; in &lt;module&gt;</span><br><span class="line">      5 else:</span><br><span class="line">      6     print(&quot;a 是存在的&quot;)</span><br><span class="line">----&gt; 7 if length is None:</span><br><span class="line">      8     raise ValueError(&quot;length should not be None when box_predictor &quot;</span><br><span class="line">      9                                  &quot;is not specified&quot;)#圆括号隐式转换，加一个括号中，可以应对一行语句太长，分为多行。   </span><br><span class="line"></span><br><span class="line">NameError: name &#39;length&#39; is not defined</span><br></pre></td></tr></table></figure>
<p>4.4.1、not</p>
<p>由于上面None说到了可以判别一个量是否被定义，这里就要说一下not，而not可以用于判断一个变量是否为空。首先看一下not的本质，其本质就是对布尔值进行取反，而空变量的布尔属性是False的(python中False和True首字母都大写)，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;[]</span><br><span class="line">print(bool(a))</span><br><span class="line">b&#x3D;[0,2,3]</span><br><span class="line">print(bool(b))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出空变量的布尔属性是False，而非空变量的布尔属性是True。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">False</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p>所以可以通过如下语句判断一个量是否空的:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">if not a:</span><br><span class="line">    print(&quot;a 是空的&quot;)</span><br></pre></td></tr></table></figure>
<p>其中not的功能可以理解为将布尔值取反。</p>
<p>4.5、tensor.permute</p>
<p>permute有排序、置换的意思，tensor.permute（1,0）就是把原来维度1的数量放到维度0上，把原来维度0的数量放到维度1上。比如a.permute(3,0,2,1)就是把原本shape：[a,b,c,d]变成[d,a,c,b]。其是用于维度交换的。</p>
<p>测试如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">b&#x3D;a.permute(1,0)</span><br><span class="line">print(b)</span><br><span class="line">b[1,1]&#x3D;0</span><br><span class="line">print(b,&quot;\n&quot;,a)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出，其实permute置换只是得到一个新的视图，而没有新的存储空间，其和a是共享的，这个也符合torch中节约空间和加速的出发点。可以用tensor.copy()开辟一段新存储空间。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[1., 4.],</span><br><span class="line">        [2., 5.],</span><br><span class="line">        [3., 6.]])</span><br><span class="line">tensor([[1., 4.],</span><br><span class="line">        [2., 0.],</span><br><span class="line">        [3., 6.]]) </span><br><span class="line"> tensor([[1., 2., 3.],</span><br><span class="line">        [4., 0., 6.]])</span><br></pre></td></tr></table></figure>
<p>4、unsqueeze</p>
<p>可以增加一个维度，和stack比较类似，只是stack有一个拼接的过程。其增加维度方式和stack类似。在a=[[1,2,3],[4,5,6]]的时候，a[:,0]显然会造成降维，可以a[:,0].unsqueeze(1)这样就可以保持也是二维的，增加的维度是维度1，即在维度0后面增加，其实对于unsqueeze增加维度，传入参数是n，就是给原tensor的n-1后加个维度，也就是给n-1维度的元素加个维度。</p>
<p>例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.randperm(10).reshape((2,5))</span><br><span class="line">print(a)</span><br><span class="line">print(a.unsqueeze(0),&quot;\n&quot;,a.unsqueeze(1),&quot;\n&quot;,a.unsqueeze(2))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[5, 3, 2, 9, 1],</span><br><span class="line">        [4, 0, 7, 6, 8]])</span><br><span class="line">tensor([[[5, 3, 2, 9, 1],</span><br><span class="line">         [4, 0, 7, 6, 8]]]) </span><br><span class="line">tensor([[[5, 3, 2, 9, 1]],</span><br><span class="line"></span><br><span class="line">        [[4, 0, 7, 6, 8]]]) </span><br><span class="line">tensor([[[5],</span><br><span class="line">         [3],</span><br><span class="line">         [2],</span><br><span class="line">         [9],</span><br><span class="line">         [1]],</span><br><span class="line"></span><br><span class="line">        [[4],</span><br><span class="line">         [0],</span><br><span class="line">         [7],</span><br><span class="line">         [6],</span><br><span class="line">         [8]]])</span><br></pre></td></tr></table></figure>
<p>squeeze是去除维度的，其只能去除维度值为1的维度，如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">print(a.unsqueeze(0).squeeze(0))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tensor([[5, 3, 2, 9, 1],</span><br><span class="line">        [4, 0, 7, 6, 8]])</span><br></pre></td></tr></table></figure>
<p><strong>5.排序函数torch.topk()</strong></p>
<p>torch.topk()，如名字般，是为了求tensor的某个维度的前k大或前k小的值（还有index）。其具体用法如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">topk(input, k, dim&#x3D;None, largest&#x3D;True, sorted&#x3D;True, *, out&#x3D;None) -&gt; (Tensor, LongTensor)</span><br><span class="line">input--tensor数据</span><br><span class="line">k--指定k值</span><br><span class="line">dim--指定维度</span><br><span class="line">largest--默认是 True，则从大到小排序，False则从小到大排序。</span><br><span class="line">sorted--默认是 True，即返回是按照顺序排好的</span><br></pre></td></tr></table></figure>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3,4],[5,6,7,8]])</span><br><span class="line">b&#x3D;torch.topk(a,3,dim&#x3D;1)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">torch.return_types.topk(</span><br><span class="line">values&#x3D;tensor([[4., 3., 2.],</span><br><span class="line">        [8., 7., 6.]]),</span><br><span class="line">indices&#x3D;tensor([[3, 2, 1],</span><br><span class="line">        [3, 2, 1]]))</span><br></pre></td></tr></table></figure>
<p>6、四舍五入，小数取舍，上下限设置等等</p>
<p>torch变量的方法clamp，设置下限为0，小于0的自动设置为0，可以用于切片时，但由非法(负数)的情况。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">matched_idxs.clamp(min&#x3D;0)</span><br></pre></td></tr></table></figure>
<p>round函数，使用:a.round()，a是torch变量，该函数是对a进行四舍五入。</p>
<p>还有floor，ceil</p>
<p>7、返回坐标信息torch.where和nonzero</p>
<p>其返回的是元组，例子如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,2,3,4,5,0,1,0])</span><br><span class="line">print(torch.where(torch.eq(a,0)))</span><br></pre></td></tr></table></figure>
<p>得到如下结果,即元组内才是torch变量，torch.where(torch.eq(a,0))[0]才能得到torch.tensor([5,7])</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(tensor([5, 7]),)</span><br></pre></td></tr></table></figure>
<p>之所以这样是因为where返回的元组结果是这样的([第0维度坐标]，[第一维度坐标],[第二维度坐标]，)。所以只有一个维度的时候元组也是([第0维度坐标]，)。所以需要片选0才能得到结果。</p>
<p>再例如这样</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">a&#x3D;torch.tensor([[1,2,3,4,5,0,1,0]])</span><br><span class="line">print(torch.where(torch.eq(a,0)))</span><br></pre></td></tr></table></figure>
<p>结果就是这样了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">(tensor([0, 0]), tensor([5, 7]))</span><br></pre></td></tr></table></figure>
<p>这时候坐标每个维度是分开的，如果想组合到一起，可以通过torch.stack函数来进行。</p>
<p>8、相等np.equal(对应元素相同)和np.array_equal(完全相同)。torch.equal和np.equal一致。</p>
<p>9、打乱顺序，随机采样，torch.randperm()函数，通过打乱顺序，切片取前n个就可以当做是随机采样了。</p>
<p>10、tensor.max(dim=1)</p>
<p>比如a=torch.tensor([[1,8,3],[4,5,6]]),则a.max(dim=0)就是[4,8,6],dim=1则就是为[8,6]</p>
<p>11、meshgrid函数，可以简单理解为网格划线</p>
<p>20、isinstance(a,(list,tuple))这个就是判断是否是list或者tuple类型。非的时候，这样表示：not isinstance()</p>
<p>21、torch.full((100,),0)即是得到100个元素的矩阵，都填充为0</p>
<p>100、不要改动框架中# tpye的语句，因为#代表注释符，但如果# type就是代表类型说明符，随便改动就会导致模型错误。</p>
<p>101、arange，np.arange(100)，生成0-99，np.arange(95,100),生成95-99</p>
<p>102、转置arrya.T。</p>
<p>103、语音处理相关库，图像处理相关库，文件读取相关库os</p>
<p>105、array[0,…]的三个省略号代表所有维度</p>
<p>106、range(start,end,stride)</p>
<p>只写一个数，默认start为0，stride为1</p>
<p><strong>107、图像存储问题</strong></p>
<p><strong>用minist数据集的时候，读入的图片都是1*32*32的，直接变成32<em>32的话，然后imshow(),或者imshow(img,cmap=’gray_r’)都是没有问题的，minist的像素数据很奇特，其中有负数，如果直接将该32\</em>32的数据保存，得到的是一个近乎全黑的图(测试了一下，那些保存的时候都是简单的进行取整，然后保存，然而minist的数据很多负数，其他正数也很小，也就3这样，255才是白色，3可以认为是黑色了)，但是为什么imshow却没问题呢，应该是imshow显示的时候，将负数默认为0了，然后最大值缩放到255，其他等比例放大(或者是总体加上了一个正数，使得最小的负数加上后恰好为0，然后等比例放缩)，所以观测没问题，但是那些PIL等等函数保存成jpg却成了黑图的原因(PIL这些图看来是保存的时候简单取整，读入的时候却会自动缩放为0-255)。那么我们保存的时候，先将负数变为0，然后等比例放大。img*(img&gt;0))*255/np.max(img),这样即可，值得注意的是，np中的sum和max无论输入多少维度的，得到的只有一个值。</strong></p>
<p>注：array都代表np变量，即np.array();tensor都代表tensor变量，即torch.Tensor</p>
<p>np里面习惯用axis，比如array.max函数，split（array,下标的list or 等分，dim）函数。split没有array.split格式。np中有很多都没有array.函数的格式，比如array.append(array)就不可以，得np.append(array1,array2)，而list却是可以的。</p>
<p><strong>np中的sum和max无论输入多少维度的，得到的只有一个值。</strong></p>
<p>而torch中喜欢用dim</p>
<p>函数+？可以得到函数介绍，比如torch.stack?，记得函数后面不要加括号。</p>
<p>verilog中负数不能比较大小，坑爹呀</p>
<p>json</p>
<h3 id="torch中网络输入的数据"><a href="#torch中网络输入的数据" class="headerlink" title="torch中网络输入的数据"></a>torch中网络输入的数据</h3><p>torch中CNN迭代器输入的数据，(batchsize，维度，行，列)，label是(batchsize)，输入的数据类型是torch.float,分类的label需要是int64(交叉熵函数需要这个数据类型)，int64在torch中是torch.long,一般定义数据的时候，可以直接指定dtype，也可以定义好变量a后，a.long这样修改dtype。对于一些机器学习的分类算法，比如sklearn的svm或者随机森林等等，输入(行，列)，一行是一个数据，label是(行数)。</p>
<p>如果我们有很多类的数据，如何进行拼接及其label生成呢，如下,以csv文件为例，通过glob得到所有csv文件，同时i即为对应标签。然后拼接即可。预先的data_all是空数组。得到数据后，然后把data_all.reshap(-1,batch_size,1,n,n)，batchsize代表一个bach数据量，1代表单通道，n*n代表图像长宽。label_all.reshape(-1,batch_size),然后变成tensor数据，data_all格式是torch.float,label格式是torch.long</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">data_all=np.empty((<span class="number">0</span>,<span class="number">256</span>))</span><br><span class="line">label_all=np.empty((<span class="number">1</span>,<span class="number">0</span>))</span><br><span class="line"><span class="keyword">for</span> i,filepath <span class="keyword">in</span> <span class="built_in">enumerate</span>(glob.glob(<span class="string">&quot;./lwqdata_5/*&quot;</span>)):</span><br><span class="line">    filepath = filepath.replace(<span class="string">&quot;\\&quot;</span>,<span class="string">&quot;/&quot;</span>)</span><br><span class="line">    print(i,filepath)</span><br><span class="line">    data = np.loadtxt(<span class="built_in">open</span>(<span class="string">&quot;./lwqdata_5/road.csv&quot;</span>,<span class="string">&quot;rb&quot;</span>),delimiter=<span class="string">&quot;,&quot;</span>,skiprows=<span class="number">1</span>)<span class="comment">#,usecols=[2,3] </span></span><br><span class="line">    label= np.full((<span class="number">1</span>,data.shape[<span class="number">0</span>]),i)</span><br><span class="line">    print(data.shape,label.shape)</span><br><span class="line">    data_all=np.vstack((data_all,data))</span><br><span class="line">    label_all=np.hstack((label_all,label))</span><br><span class="line">print(data_all.shape,label_all.shape)</span><br></pre></td></tr></table></figure>
<h2 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h2><p>np.loadtxt(“a.txt”,dtype = np.complex128),这就是以复数的形式读取文件，否则默认形式是float，float是实数域的，自然不可以读复数域的。</p>
<h2 id="库安装"><a href="#库安装" class="headerlink" title="库安装"></a>库安装</h2><p>指定镜像地址的命令如下</p>
<p>pip install -i 镜像地址 包名</p>
<p>例如： pip install -i <a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a> numpy</p>
<p>国内镜像地址：</p>
<p>清华：<a href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
<p>阿里云：<a href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a></p>
<p>中国科技大学 <a href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a></p>
<p>gdal库的安装：</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/nima1994/article/details/79207805/">python gdal安装与简单使用</a></p>

          </blockquote>
<p>华为的mindspore框架安装：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">conda create -n mindspore python=3.7.5</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mindspore</span><br></pre></td></tr></table></figure>
<p>之所以新建一个环境，是因为mindspore所需要的python环境是3.7及以上的，若3.7之下的版本，是装不成功的。</p>
<p>cv2安装：</p>
<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python</span><br></pre></td></tr></table></figure>
<h2 id="远程服务器登录报错，账号创建问题-usr-bin-xauth-error-timeout-in-locking-authority-file-home-gezhilai-Xauthority"><a href="#远程服务器登录报错，账号创建问题-usr-bin-xauth-error-timeout-in-locking-authority-file-home-gezhilai-Xauthority" class="headerlink" title="远程服务器登录报错，账号创建问题:/usr/bin/xauth: error/timeout in locking authority file /home/gezhilai/.Xauthority"></a>远程服务器登录报错，账号创建问题:/usr/bin/xauth: error/timeout in locking authority file /home/gezhilai/.Xauthority</h2><p>解决如下:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo mkdir &#x2F;home&#x2F;gezhilai</span><br><span class="line">sudo chown gezhilai:gezhilai -R &#x2F;home&#x2F;gezhilai</span><br><span class="line">sudo usermod -s &#x2F;bin&#x2F;bash gezhilai</span><br></pre></td></tr></table></figure>
<p>参考链接:<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/dong_liuqi/article/details/108842873">(29条消息) Liunx创建新用户登录异常：/usr/bin/xauth: error/timeout in locking authority file /home/liuqidong/.Xauthority_dong_liuqi的博客-CSDN博客</a></p>

          </blockquote></p>
<h2 id="anaconda中新建环境"><a href="#anaconda中新建环境" class="headerlink" title="anaconda中新建环境"></a>anaconda中新建环境</h2><p>命令如下：</p>
<p>conda create —name=labelme python=3.6</p>
<p>代表创建了一个新的环境，环境名称叫labelme，使用的是python版本是3.6.</p>
<p>可以通过conda env list查看我们所创建的所有环境，通过conda activate labelme来激活labelme环境。</p>
<p>若conda激活环境失败(很明显的就是都不在base环境中)，Your shell has not been properly configured to use ‘conda activate’。出现此报错的原因是因为之前的虚拟环境没有退出<code>source deactivate</code>，所以需要重新进入虚拟环境： <code>source activate</code>。</p>
<p>退出虚拟环境：<code>source deactivate</code>或<code>conda deactivate</code></p>
<p>参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.jianshu.com/p/88c52724b016">conda激活环境失败</a></p>

          </blockquote>
<h2 id="批量安装包"><a href="#批量安装包" class="headerlink" title="批量安装包"></a>批量安装包</h2><p>如果我们在git上需要下载一个代码，使用git clone即可把代码下载到服务器端。有些人会提供批量安装的包的txt文件，所以此时cd到该txt文件路径下:</p>
<p>进行如下即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install -r a.txt</span><br></pre></td></tr></table></figure>
<h2 id="RuntimeError-CUDA-error-no-kernel-image-is-available-for-execution-on-the-device报错"><a href="#RuntimeError-CUDA-error-no-kernel-image-is-available-for-execution-on-the-device报错" class="headerlink" title="RuntimeError: CUDA error: no kernel image is available for execution on the device报错"></a>RuntimeError: CUDA error: no kernel image is available for execution on the device报错</h2><p>torch运行时候出现了该错误，此时是由于torch的版本和cuda驱动的版本不兼容，导致torch无法使用cuda，从而无法使用GPU报错。以个人理解，torch之所以要和cuda匹配，是因为torch中一些语法使用了GPU时候，用来cuda语法编程，从而要和安装的cuda平台相匹配。(就像你用python2.0的编程，却用3.0的解释器，会出错，当然，也可能没有出错，毕竟每一代还是有很多内容相同)，这里报错时候使用的torch是1.8.1，cuda是10.1的，网上没有查到1.8.1的对应版本，出问题了，我们将torch卸载进行重装。</p>
<p>1、首先如下命令，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<p>结果第一行如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">NVIDIA-SMI 455.23.04    Driver Version: 455.23.04    CUDA Version: 11.1   </span><br></pre></td></tr></table></figure>
<p>可以看出，该服务器的RTX3090的配套最高驱动是11.1。</p>
<p>2、查看如今的CUDA驱动版本</p>
<p>可以使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;version.txt</span><br></pre></td></tr></table></figure>
<p>查看当前的CUDA运行版本。</p>
<p>CUDA运行API低于GPU支持驱动是可以的。因为CUDA分为CUDA Drvier API和CUDA Runtime API，所谓的Driver API其实是当前GPU底层支持的最高CUDA命令版本，而CUDA Runtime API是服务器上安装的实际版本，CUDA是向下兼容的，也就是说NVDIA设计3090是基于cuda11.1设计的，但是cuda11.1是含括了11.1以前的所有命令，所以cuda10的驱动命令都是适用的，用cuda10来做一些加速也是可以的，当然11.1cuda可能对其他很多东西做了更好的优化。理论上来说，nvdia-smi的型号是GPU支持的最高驱动版本，你安装更高的Driver API，它就看不懂里面的一些命令了。我们安装的时候自然是对应最好。</p>
<p>3、知道如上结果，就可以按照第2步的CUDA驱动版本安装对应的torch了，其中安装的时候torch、torchvision、torchaudio、cudatoolkit需要对应。</p>
<p>这里需要说明一下cudatoolkit，cat /usr/local/cuda/version.txt得到的结果是平时我们所说的CUDA版本，其是在CUDA Toolkit工具包中，所以该CUDA版本也是CUDA Toolkit工具包的版本。</p>
<p>但是装了Anaconda之后Anaconda也会提供一个cudatoolkit工具包，同样包含了CUDA的运行API，可以用来替代官方CUDA的CUDA Toolkit。这也就是为什么有时候我们通过nvcc-V查看的cuda版本很低(比如7.5)，但是能成功运行cuda9.0的pytorch的原因。因为在安装完anaconda后，运行pytorch代码就会使用anaconda的cudatoolkit，而忽视官方的CUDA Toolkit，所以我们只需要根据anaconda的cudatoolkit包的版本来安装相应的pytorch即可。</p>
<p>查看cudatoolkit直接</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
<p>即可看到cudatoolkit版本，Wheel(即pip安装)的形式，可以看到torch版本是1.7.0+cu11.0的样式，其cu11.0就是anaconda的cudatoolkit，如果是conda安装的，那么可以直接看到cudatoolkit是作为一个库独立存在，可以看到其版本。</p>
<p>由此可知，我们完全可以在anaconda中相对应的cudatoolkit，具体的torch版本对应的torchvision、torchaudio和可选的cudatoolkit版本可以在如下官网查看。</p>
<blockquote class="blockquote-center">
            <p><a href="https://github.com/pytorch/vision">torch、torchvision、python的版本对应</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://pytorch.org/get-started/previous-versions/">各个torch版本下及其对应的torchvision、cudatoolkit的安装命令</a></p>

          </blockquote>
<p>由于由第一步知道，cuda最高支持驱动为11.0，那么我们就选用11.0cudatoolkit下的某个toch版本，</p>
<p>在此选用了1.7.0，对应的torchvison可以查上面两个链接，使用wheel安装，命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install torch&#x3D;&#x3D;1.7.0+cu110 torchvision&#x3D;&#x3D;0.8.0+cu110 torchaudio&#x3D;&#x3D;0.7.0 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.html</span><br></pre></td></tr></table></figure>
<p>即安装的cudatoolkit是11.0的，torchvision是其对应版本。</p>
<p>至此安装后即可正常运行。</p>
<p><strong>注:有的时候命令行操作python  a.py可以，但是vscode端运行不可以，是不是因为调用了CUDATOOLKIT，而不是anaconda中的cudatoolkit，导致版本不兼容?</strong></p>
<h2 id="命令行运行py文件"><a href="#命令行运行py文件" class="headerlink" title="命令行运行py文件"></a>命令行运行py文件</h2><p>正常如果不需要配置什么内容的话，直接如下即可运行:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python a.py</span><br></pre></td></tr></table></figure>
<p>但如果有如下这一类的配置文件:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def get_parser():</span><br><span class="line">    &quot;&quot;&quot;Get default arguments.&quot;&quot;&quot;</span><br><span class="line">    parser &#x3D; configargparse.ArgumentParser(</span><br><span class="line">        description&#x3D;&quot;Transfer learning config parser&quot;,</span><br><span class="line">        config_file_parser_class&#x3D;configargparse.YAMLConfigFileParser,</span><br><span class="line">        formatter_class&#x3D;configargparse.ArgumentDefaultsHelpFormatter,</span><br><span class="line">    )</span><br><span class="line">    # general configuration</span><br><span class="line">    parser.add(&quot;--config&quot;, is_config_file&#x3D;True, help&#x3D;&quot;config file path&quot;)</span><br><span class="line">    parser.add(&quot;--seed&quot;, type&#x3D;int, default&#x3D;0)</span><br><span class="line">    parser.add_argument(&#39;--num_workers&#39;, type&#x3D;int, default&#x3D;0)</span><br><span class="line">    </span><br><span class="line">    # network related</span><br><span class="line">    parser.add_argument(&#39;--backbone&#39;, type&#x3D;str, default&#x3D;&#39;resnet50&#39;)</span><br><span class="line">    parser.add_argument(&#39;--use_bottleneck&#39;, type&#x3D;str2bool, default&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # data loading related</span><br><span class="line">    parser.add_argument(&#39;--data_dir&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    parser.add_argument(&#39;--src_domain&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    parser.add_argument(&#39;--tgt_domain&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    </span><br><span class="line">    # training related</span><br><span class="line">    parser.add_argument(&#39;--batch_size&#39;, type&#x3D;int, default&#x3D;32)</span><br><span class="line">    parser.add_argument(&#39;--n_epoch&#39;, type&#x3D;int, default&#x3D;100)</span><br><span class="line">    parser.add_argument(&#39;--early_stop&#39;, type&#x3D;int, default&#x3D;0, help&#x3D;&quot;Early stopping&quot;)</span><br><span class="line">    parser.add_argument(&#39;--epoch_based_training&#39;, type&#x3D;str2bool, default&#x3D;False, help&#x3D;&quot;Epoch-based training &#x2F; Iteration-based training&quot;)</span><br><span class="line">    parser.add_argument(&quot;--n_iter_per_epoch&quot;, type&#x3D;int, default&#x3D;20, help&#x3D;&quot;Used in Iteration-based training&quot;)</span><br><span class="line"></span><br><span class="line">    # optimizer related</span><br><span class="line">    parser.add_argument(&#39;--lr&#39;, type&#x3D;float, default&#x3D;1e-3)</span><br><span class="line">    parser.add_argument(&#39;--momentum&#39;, type&#x3D;float, default&#x3D;0.9)</span><br><span class="line">    parser.add_argument(&#39;--weight_decay&#39;, type&#x3D;float, default&#x3D;5e-4)</span><br><span class="line"></span><br><span class="line">    # learning rate scheduler related</span><br><span class="line">    parser.add_argument(&#39;--lr_gamma&#39;, type&#x3D;float, default&#x3D;0.0003)</span><br><span class="line">    parser.add_argument(&#39;--lr_decay&#39;, type&#x3D;float, default&#x3D;0.75)</span><br><span class="line">    parser.add_argument(&#39;--lr_scheduler&#39;, type&#x3D;str2bool, default&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # transfer related</span><br><span class="line">    parser.add_argument(&#39;--transfer_loss_weight&#39;, type&#x3D;float, default&#x3D;10)</span><br><span class="line">    parser.add_argument(&#39;--transfer_loss&#39;, type&#x3D;str, default&#x3D;&#39;mmd&#39;)</span><br><span class="line">    return parser</span><br><span class="line">  </span><br><span class="line">def main():</span><br><span class="line">    parser &#x3D; get_parser()</span><br></pre></td></tr></table></figure>
<p>就需要额外的配置了:比如如下，上述中无default的，require为true的都要配置。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python main.py --config DAN&#x2F;DAN.yaml --data_dir &#x2F;remote-home&#x2F;lwq&#x2F;transferlearning&#x2F;code&#x2F;DeepDA&#x2F;data18 --src_domain train18 --tgt_domain test18</span><br></pre></td></tr></table></figure>
<h2 id="在jupyter中使用虚拟环境"><a href="#在jupyter中使用虚拟环境" class="headerlink" title="在jupyter中使用虚拟环境"></a>在jupyter中使用虚拟环境</h2><p>创建好新的环境后，如果使用jupyter时想用该环境的话，还需要配置一下：</p>
<p>首先查看当前的所有环境：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda env <span class="built_in">list</span></span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># conda environments:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">base                  *  D:\anzhuang\anaconda</span><br><span class="line">gf                       D:\anzhuang\anaconda\envs\gf</span><br><span class="line">kite                     D:\anzhuang\anaconda\envs\kite</span><br><span class="line">labelme                  D:\anzhuang\anaconda\envs\labelme</span><br><span class="line">mindspore                D:\anzhuang\anaconda\envs\mindspore</span><br><span class="line">new_env                  D:\anzhuang\anaconda\envs\new_env</span><br><span class="line">pycharm                  D:\anzhuang\anaconda\envs\pycharm</span><br><span class="line">pythonProject1           D:\anzhuang\anaconda\envs\pythonProject1</span><br><span class="line">pythonProject2           D:\anzhuang\anaconda\envs\pythonProject2</span><br><span class="line">tensorflow               D:\anzhuang\anaconda\envs\tensorflow</span><br></pre></td></tr></table></figure>
<p>如果我们想在jupyter中使用gf这个环境的话，需要如下操作：</p>
<p>首先激活该环境：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda activate gf</span><br></pre></td></tr></table></figure>
<p>然后安装ipykernel</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda install ipykernel</span><br></pre></td></tr></table></figure>
<p>最后添加对应文件：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">python -m ipykernel install --name gf</span><br></pre></td></tr></table></figure>
<p>此时已完成新环境的添加，如下打开jupyter即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p>注意：创建新环境的时候，激活后，使用jupyter notebook可能提示没有jupyter，那是没有安装jupyter，安装一下即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">conda install jupyter</span><br></pre></td></tr></table></figure>
<h2 id="在jupyter中某个环境kernel-error"><a href="#在jupyter中某个环境kernel-error" class="headerlink" title="在jupyter中某个环境kernel error"></a>在jupyter中某个环境kernel error</h2><blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/weixin_44457768/article/details/106027628">Jupyter notebook中报错，出现kernel error解决方法</a></p>

          </blockquote>
<p>其中jupyter kernelspec list可以查看所有的环境用的kernel路径（json中配置了python编译器位置）</p>
<h2 id="Pycharm中使用anconda中的环境"><a href="#Pycharm中使用anconda中的环境" class="headerlink" title="Pycharm中使用anconda中的环境"></a>Pycharm中使用anconda中的环境</h2><p>file-setting-Project Interpreter-system Interperter处，将python编译器变成anconda的，如果anaconda已加入环境变量中，是可以自动识别出来的</p>
<h2 id="本地打开服务器的tensorboard"><a href="#本地打开服务器的tensorboard" class="headerlink" title="本地打开服务器的tensorboard"></a>本地打开服务器的tensorboard</h2><p>tensorboard —logdir=. —bind_all然后ip:6006就可以在本地打开了</p>
<h2 id="Torch中使用GPU"><a href="#Torch中使用GPU" class="headerlink" title="Torch中使用GPU"></a>Torch中使用GPU</h2><p>代码中，常会发现如下两句:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>上述的”cuda:0”,如果电脑只有单张独立显卡，就是”cuda”。这两行是放在读取数据之前，意思是将模型复制一份到device上，如果GPU存在(同时torch是GPU版本，cudatooltik也匹配)，torch.cuda.is_available()就会判断成立。然后后面读取数据的时候也通过data.to(device)把数据复制一份到GPU上，初始数据复制到了GPU上，中间的数据自然就会存在GPU的显存中,torch类方法中就有这个to方法。</p>
<p>而数据如下即可，g.cuda()如果之前没有特地的设置的，默认是cuda:0,所以如下即可。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    h = g.cuda()</span><br><span class="line">    print(h)</span><br></pre></td></tr></table></figure>
<p>若是多块GPU，模型用到GPU上如下:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Model()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">	model = nn.DataParallel(model，device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="keyword">elif</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model.to(device)</span><br></pre></td></tr></table></figure>
<p>或者如下：(推荐)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,3&#x27;</span></span><br><span class="line">net = torch.nn.DataParallel(model)</span><br></pre></td></tr></table></figure>
<p>数据加载的话，需要如下,但是.cuda默认是第一章卡，所以还需要os.environ[‘CUDA_VISIBLE_DEVICES’] = ‘0,3’这个命令在之前才可以。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">inputs = inputs.cuda()</span><br><span class="line">labels = labels.cuda()</span><br></pre></td></tr></table></figure>
<p>如果多GPU下只用一张卡，还可以如下(不推荐)，如下情况，虽然只指定了一张卡，但是print的结果不变，即不会改变可见显卡，后续还可以用<code>torch.nn.DataParallel(model, device_ids=[1, 2])</code>进行指定，但是必须包含<code>set_device(1)</code>指定的device:1的设备，缺点是仍然会存在占用一些device:0的gpu内存；</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cuda.set_device(<span class="number">1</span>)</span><br><span class="line">print(torch.cuda.device_count()) <span class="comment">#可用GPU数量</span></span><br></pre></td></tr></table></figure>
<p>当然可以通过其他方式制定GPU:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#（1）直接终端中设定 </span></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#（2）python代码中设定：</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICE&#x27;</span>]=<span class="string">&#x27;1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#（3）使用函数set_device</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.set_device(<span class="built_in">id</span>)</span><br></pre></td></tr></table></figure>
<p>把Tensor放到GPU上的方法如下，也可以g.to(device)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    h = g.cuda()</span><br><span class="line">    print(h)</span><br></pre></td></tr></table></figure>
<p>把GPU上的Tensor或者Variable的数据返回到CPU上变成numpy格式(比如绘图的时候需要用到，GPU上的数据没法直接在界面上可视化)：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将变量或者数据移到GPU</span></span><br><span class="line">gpu_info = Variable(torch.randn(<span class="number">3</span>,<span class="number">3</span>)).cuda()</span><br><span class="line"><span class="comment"># 将变量或者数据移到CPU</span></span><br><span class="line">cpu_info = gpu_info.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>注:常用的torch.cuda函数:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.cuda.is_available()<span class="comment">#如果可以cuda驱动gpu，则为true</span></span><br><span class="line">torch.cuda.device_count()<span class="comment">#返回GPU可用数量，2则代表2个</span></span><br><span class="line">torch.cuda.get_device_name()<span class="comment">#返回GPU型号，如GeForce RTX 3090</span></span><br><span class="line">torch.cuda.get_device_properties(<span class="string">&quot;cuda:0&quot;</span>)<span class="comment">#返回GPU性能，显存、线程等等</span></span><br><span class="line">torch.cuda.current_device()<span class="comment">#查看当前GPU使用序号。注意#os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#x27;1,2&#x27;这种指令会改变torch感知设备的编号，使用了1和#2后，torch.cuda.current_device()感知只会是0或1，因为感知是从0开始的。</span></span><br></pre></td></tr></table></figure>
<p>参考链接:<a href="https://blog.csdn.net/shaopeng568/article/details/95205345">(29条消息) Pytorch to（device）_shaopeng568的专栏-CSDN博客</a></p>
<p><a href="https://blog.csdn.net/qq_34243930/article/details/106695877">(29条消息) pytorch之多GPU使用——#CUDA_VISIBLE_DEVICES使用 #torch.nn.DataParallel() #报错解决_夏普通-CSDN博客</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练 - 知乎 (zhihu.com)</a></p>
<h2 id="模型训练中显示启动卷积失败，cuda等等问题。"><a href="#模型训练中显示启动卷积失败，cuda等等问题。" class="headerlink" title="模型训练中显示启动卷积失败，cuda等等问题。"></a>模型训练中显示启动卷积失败，cuda等等问题。</h2><p>os.environ[‘CUDA_VISIBLE_DEVICES’] = ‘/gpu:0’</p>
<p>有一次写代码发现明明写的是：</p>
<p>os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1,2,3,4,5,6,7”</p>
<p>但是却显示只发现了 gpu：0，百思不得其解，百度谷歌都没有发现类似问题，后来检查才发现有一个调用的py文件里写了一句：</p>
<p>os.environ[“CUDA_VISIBLE_DEVICES”] = “0”</p>
<p>删掉之后，可以发现全部的gpu</p>
<h2 id="tf导包失败问题"><a href="#tf导包失败问题" class="headerlink" title="tf导包失败问题"></a>tf导包失败问题</h2><p>tf中，如下导包失败,报错没np_utils这个包，一般这种问题很有可能是版本问题导致包已经到其他仓库里了或者修改了包名等等。这里其实是keras在版本迁移的时候，包的位置迁移了导致导不进来，在__init__.py可以看到。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure>
<p>修改成如下,即多了个python位置，此时导入不报错，但是使用的时候有问题即np_utils.方法都不能用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.python.keras.utils.np_utils</span><br></pre></td></tr></table></figure>
<p>如下即可成功导入并使用</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://blog.csdn.net/qq_40662487/article/details/109902765">(29条消息) 解决ImportError: cannot import name ‘np_utils‘ from ‘tensorflow.keras.utils‘ 的问题_小了白了兔DY的博客-CSDN博客</a></p>
<h2 id="学习网站"><a href="#学习网站" class="headerlink" title="学习网站"></a>学习网站</h2><blockquote class="blockquote-center">
            <p><a href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017496031185408">缪雪峰的官方网站</a></p>

          </blockquote>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>深度学习</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>CNN</tag>
        <tag>深度学习</tag>
        <tag>目标追踪</tag>
      </tags>
  </entry>
  <entry>
    <title>Python之魔法函数</title>
    <url>/2021/07/20/Python%E4%B9%8B%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/</url>
    <content><![CDATA[<p>Python是一个很类似于matlab的语言，都是一种高层解释型语言，区别于C、C++，C、C++则是编译通过后才执行(说明语法都没有错误)，Python执行之前不需要编译链接，直接是一行行执行，将除了缩进的语言都执行一遍，为了防止在读入包/模块时候出现代码执行，出现了__name__的魔法函数，也是Python的一种内置属性，除了__name__，在读faster rcnn的torch源码时候，也会频繁遇到__call__()、 __ len__()、__ init__() 、__ getitem__()等一类魔法函数，特地去搜了搜，网上讲的很好，链接如下:</p>
<a id="more"></a>
<blockquote class="blockquote-center">
            <p><a href="https://www.zhihu.com/question/49136398">if <strong>name</strong> == ‘<strong>main</strong>‘ 如何正确理解? - 知乎 (zhihu.com)</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://zhuanlan.zhihu.com/p/344951719">Python：实例讲解Python中的魔法函数（高级语法） - 知乎 (zhihu.com)</a></p>

          </blockquote>
<p>由于上述链接讲的都很完备，下面只进行简单的总结一下。                            </p>
<h3 id="name"><a href="#name" class="headerlink" title="__name__"></a>__name__</h3><p>大概说一下__name__吧，其本质就是Python的一个内置属性，当自己作为主文件执行时，__name__是”__main__“,而当作为模块时，则__name__就是模块名，自己也特地测试了一个，图如下，其中Write_coe是我将Python参数存储为FPGA的RAM文件格式写的自定义模块函数。</p>
<p><a href="https://imgtu.com/i/WYpEdg"><img src="https://z3.ax1x.com/2021/07/19/WYpEdg.png" alt="WYpEdg.png"></a></p>
<h2 id="魔法函数"><a href="#魔法函数" class="headerlink" title="魔法函数"></a>魔法函数</h2><p>魔法函数是类中的可重写的方法，有助于更灵活方便的使用类。</p>
<h3 id="1、字符串表示：-str-和-repr-的区别"><a href="#1、字符串表示：-str-和-repr-的区别" class="headerlink" title="1、字符串表示：__str__和__repr__的区别"></a>1、<strong>字符串表示：</strong>__str__和__repr__的区别</h3><p>在Python类中，这两个方法的区别主要如下：</p>
<p>print类的时候，首先找__str__方法，若__str__方法未进行重写，则调用__repr__(或者可以说__str__未重写的时候，初始定义和__repr__一样的)</p>
<p>而在交互式窗口直接打实例化类的时候，则调用__repr__方法，其本质就是打印一个str字符串：类名+object at+内存地址。当然__repr__也可以重写。</p>
<h3 id="2、集合、序列相关："><a href="#2、集合、序列相关：" class="headerlink" title="2、集合、序列相关："></a>2、集合、序列相关：</h3><h4 id="2-1、-len"><a href="#2-1、-len" class="headerlink" title="2.1、__len__"></a>2.1、__len__</h4><p>在使用len(某个class)  ，而len的函数本质是将该class作为参数传入，然后调用的是类自身的__len__方法。</p>
<h4 id="2-2、-getitem"><a href="#2-2、-getitem" class="headerlink" title="2.2、__getitem__"></a>2.2、__getitem__</h4><div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>Python</category>
        <category>魔法函数</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>魔法函数</tag>
      </tags>
  </entry>
  <entry>
    <title>基于FPGA的手势识别系统设计</title>
    <url>/2021/07/09/%E5%9F%BA%E4%BA%8EFPGA%E7%9A%84%E6%89%8B%E5%8A%BF%E8%AF%86%E5%88%AB%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1/</url>
    <content><![CDATA[<p>​        之前做图像处理的时候，也没有涉及过太多的深度学习，所以都是用传统算法做的，这个是本科期间做的大创（感谢陈老师帮该项目申请成了省级大创），其实不能说做的多好，因为真的发现图像处理领域很多传统算法被深度学习吊锤，而且现在FPGA加速器有崛起的势头，但是我们还是要惊羡于那些数学家的美妙构思（数学真是最美妙的学科），不多扯了，进入正题。</p>
<a id="more"></a>
<p>注:这里没有提到待卷积窗口的生成(3*3 or 5*5的)，大家可以参考这篇文章</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.cnblogs.com/ninghechuan/p/6789399.html">深刻认识shift_ram IP core</a></p>

          </blockquote>
<p>以后有时间会在加速器设计里讲如何生成待卷积窗口，最近有点忙。。。</p>
<p>2.1 系统介绍<br>该项目通过FPGA驱动摄像头ov7725,通过配置寄存器使得ov7725采集到RGB565格式的数据，由于摄像头工作频率和FPGA工作频率不匹配，所以先存入ram中然后再由FPGA读出摄像头数据，为了进行图像分割，去掉除了手势以外的其他部分，我们对其进行了色域转换，将RGB565格式转为YCBCR格式，通过控制CBCR域值进行提取肤色。然后，滤除图像中的杂波,我们进行分别进行了两次腐蚀操作和两次膨胀操作，为了提高后续识别算法的鲁棒性，我们对其进行了sobel算子边缘化提取，最后，我们通过hu不变矩算法进行识别手势。其系统框图如下<a href="https://imgtu.com/i/RjL0gg"><img src="https://z3.ax1x.com/2021/07/09/RjL0gg.png" alt="RjL0gg.png"></a></p>
<p>2.3图像处理算法模块</p>
<p>此块详细描述</p>
<p>（1）肤色提取</p>
<p>为了进行肤色提取，我们需要先将RGB565格式转为YCbCr域进行识别。</p>
<p>YCbCr是通过有序的三元组来表示的，三元由Y(Luminance)、Cb(Chrominance-Blue)、和Cr(Chrominance-Red)组成，其中Y表示颜色的明亮度和浓度，而Cb和Cr则分别表示颜色的蓝色浓度偏移量和红色浓度偏移量。人的肉眼对由YCbCr色彩空间编码的视频中的Y分量更敏感，而Cb和Cr的微小变化不会引起视觉上的不同，根据该原理，通过对Cb和Cr进行子采样来减小图像的数据量，使得图像对存储需求和传输带宽的要求大大降低，从而达到在完成图像压缩的同时也保证了视觉上几乎没有损失的效果，进而使得图像的输出速度更快，存储更加方便。值得一提的是，如果需要得到灰度图像的话，要将采集到的彩色图像转化为YCbCr后，将Y分量分别送给VGA的R、G、B，即此时R、G、B数值上是一致的(其实由于位数原因会不一致，比如RGB565，G是6位而此时RB只有5位)，这样便得到了灰度图，此处我们没有用到灰度图，所以不需要Y分量，但下述算法依旧会一并提及。</p>
<p>我们配置摄像头采集到的数据是RGB565的格式，官方给出的转化公式是严格的RGB888转为YCbCr888，所以先需要将RGB565转化为RGB888，这个时候我们又遇到了一个有意思的问题，平时像素数据一般RGB565，但很多FPGA开发板支持的VGA不是RGB565的，EGO1就是RGB444的，那么如何将RGB565变为RGB444呢？又如何将RGB565变为RGB888？此处便提到了压缩和量化补偿思想。</p>
<p>对于压缩，主要是一个思维：取高位。如果是RGB565格式想变成RGB444，那么再FPGA中always块只要如下三句即可：</p>
<script type="math/tex; mode=display">
R1<=R[4:1];
            G1<=G[5:2];                                                                            B1<=B[4:1];</script><p>即分别取了原像素点RGB的高四位作为RGB444。</p>
<p>  对于量化补偿，是用在扩充位数的时候用的，比如此处需要的RGB888。以RGB565转RGB888为例。   </p>
<p>16bit RGB565 -&gt; 24bit RGB888的转换（高位补低位）</p>
<p>16bit RGB565：</p>
<script type="math/tex; mode=display">
\\{R4 R3 R2 R1 R0} 

\\{G5 G4 G3 G2 G1 G0}

\\{B4 B3 B2 B1 B0}</script><p>24bit RGB888:</p>
<script type="math/tex; mode=display">
\\{ R4 R3 R2 R1 R0 R2 R1 R0}

\\{ G5 G4 G3 G2 G1 G0 G1 G0}

\\{ B4 B3 B2 B1 B0 B2 B1 B0}</script><p>官方给出的RGB888-&gt;YCbCr888转化公式如下：</p>
<script type="math/tex; mode=display">
Y=0.229R+0.587G+0.114B
\\Cb=0.568(B-Y)+128
\\Cr=0.713(R-Y)+128</script><p>由于FPGA无法实现浮点数运算，所以需要把系数变为整数，我们不妨将Y、Cb和Cr都扩大1024倍，然后所有系数取整，那么变成如下公式：</p>
<p>Y=306R+601G+116B</p>
<p>Cb=-176R-347G+523B+131072</p>
<p>Cr=523R-438G-85B+131072</p>
<p>这样便可以得到整型的运算，最后得到的结果右移10位即可，但为了时序的科学严谨性，我们不应该一次在always块中算出Y、Cb、Cr，因为一个关系式中涉及到三次乘法和两次加法，越多的运算量就越可能导致时序延时错乱，此处或许不会有问题，但不在一个块中用太复杂的运算式是一种好的习惯，我们应该选择业界普遍使用的流水线做法，将乘法在一个always块里实现，在另一个always块中实现加法。</p>
<p>在转换为YCBCR后，通过限制CBCR阈值，可以提取出肤色，在FPGA中设定的初始值肤色范围为：</p>
<script type="math/tex; mode=display">
77<Cb<127
\\133<Cr<173</script><p>这是一个经典的人的肤色阈值，但是为了个别人肤色差别需要调节，FPGA上给出了调节按键。最终在该范围内的像素使其为黑色，否则为白色，那么VGA上只显示出一只手。</p>
<p>(2)腐蚀</p>
<p>腐蚀是一种消除边界点，使边界向内部收缩的过程，可以用来消除小且无意义的物体。由于在第一步手势提取的可能出现杂波，那么为了消除这些小黑点杂波，我们进行两次腐蚀算法便可以将其消除。</p>
<p>简单来说，腐蚀操作需要用3×3的结构元素，扫描图像的每一个像素点，用结构元素与其覆盖的二值图像做“与”操作，如果全为1，结果图像的该像素是1，否则为0。结果会使二值图像小一圈，消除像素杂波。算法原理如下：</p>
<script type="math/tex; mode=display">
\begin{matrix}
   P1 &= & P11 & \& & P12 & \& & P13 \\
   P2 &= & P21 & \& & P22 & \& & P23 \\
   P3 &= & P31 & \& & P32 & \& & P33 \\
   P &= & P1 & \& & P2 & \& & P3 \\
  \end{matrix}\tag{2.4}</script><p>从原理上来说，其是比较简单的，但是FPGA实现时，我们需要考虑如何得到这个3<em>3的矩阵，因为FPGA扫描像素点是一个一个进行的，一行有640个数据，如何得到三行中的三个数据呢？这个时候我们需要用到FPGA中shift ram IP核，可以说这个IP核是为了构建矩阵量身定制，其是一个只有一行的循环移位的IP核，那么我们需要两个这样的IP核，进行循环移位得到3</em>3的矩阵，然后进行腐蚀滤波。</p>
<p>（3）膨胀</p>
<p>膨胀与腐蚀效果相反，是将与物体接触的所有背景点合并到该物体中，使边界向外部扩张的过程，由于在腐蚀的时候，是为了消除杂波，但不可避免的减小了有效的手势区域，那么我们如何来恢复被消除的手势区域，那么此时便用到了膨胀算法。</p>
<p>膨胀算法思维核腐蚀思维类似，都需要构建3*3的矩阵，用3×3的结构元素，扫描图像的每一个像素点，用结构元素与其覆盖的二值图像做“或”操作，如果全为0，结果图像的该像素是0，否则为1。结果会使二值图像扩大一圈。算法原理如下：</p>
<script type="math/tex; mode=display">
\begin{matrix}
   P1 &= & P11 & | & P12 & | & P13 \\
   P2 &= & P21 & | & P22 & | & P23 \\
   P3 &= & P31 & | & P32 & | & P33 \\
   P &= & P1 & | & P2 & | & P3 \\
  \end{matrix}\tag{2.4}</script><p>在其实现过程中，依旧需要shift ram IP核，然后得到3*3矩阵，然后用流水线法实现膨胀操作，恢复手势区域。</p>
<p><strong>（4）Sobel边缘化</strong></p>
<p> 此步我们进行Sobel算子边缘化提取，为什么进行Sobel化呢？Sobel化边缘提取可以提取出手势的边缘而不是整只手，这样是为了提高后续的hu不变矩识别算法的稳定性。</p>
<p>Sobel边缘检测的核心在于像素矩阵的卷积，卷积对于数字图像处理非常重要，很多图像处理算法都是做卷积来实现的。卷积运算的本质就是对制定的图像区域的像素值进行加权求和的过程，其计算过程为图像区域中的每个像素值分别与卷积模板的每个元素对应相乘，将卷积的结果作求和运算，运算到的和就是卷积运算的结果。</p>
<p>3×3的窗口M与卷积模板C 的卷积运算如下：</p>
<script type="math/tex; mode=display">
M= \left[
 \begin{matrix}
   M1 & M2 & M3 \\
   M4 & M5 & M6 \\
   M7 & M8 & M9
  \end{matrix}
  \right] \tag{2.4}\ \ \ \ \ \ \ \
  C= \left[
 \begin{matrix}
   C1 & C2 & C3 \\
   C4 & C5 & C6 \\
   C7 & C8 & C9
  \end{matrix}
  \right]
  \\M5'=M1*C1+M2*C2+M3*C3+M4*C4+M5*C5+\\M6*C6+M7*C7+M8*C8+M9*C9</script><p>G_x和G_y是Sobel的卷积因子，将这两个因子和原始图像做如下卷积，其中A表示原视图像。</p>
<script type="math/tex; mode=display">
G_x=\left[
 \begin{matrix}
   M1 & M2 & M3 \\
   M4 & M5 & M6 \\
   M7 & M8 & M9
  \end{matrix}
  \right] *A\tag{2.4}\ \ \ \ \ \ \
  G_y=\left[
 \begin{matrix}
   M1 & M2 & M3 \\
   M4 & M5 & M6 \\
   M7 & M8 & M9
  \end{matrix}
  \right] *A</script><p>得到图像中的每一个点的横向纵向梯度G_x、G_y。最后通过如下公式来计算该点总体梯度的大小。</p>
<script type="math/tex; mode=display">
G=\sqrt{(G^2_x+G^2_y)}</script><p>我们此时还需要设定一个阈值，该如果算出的G大于设定的阈值，那么认为此处是边缘处，使其为黑色，否则认为不是边缘，使其为白色。</p>
<p>上述是算法原理，很显然，这里也用到了3*3矩阵，那么又需要shift ram IP核，所以如果需要学习图像滤波等处理，shift ram需要熟练掌握。</p>
<p><strong>（5）手势识别算法：Hu不变矩</strong></p>
<p>对于一个提取出来的手势，我们需要有固定且唯一的特征来对其进行记录，且该特征不会受到手势的大小，旋转，平移而变化，且鲁棒性较好，所以此处引入hu不变矩算法，下面进行原理介绍:</p>
<p>1、普通矩（也叫p+q阶不变矩），和p+q中心矩的定义</p>
<p>对于像素分布为f(x,y)的图像，其(p+q)阶矩定义为：</p>
<script type="math/tex; mode=display">
m_{pq}=\int\int{x^py^qf(x,y)dxdy}\tag{5.1}</script><p>(p+q)阶中心矩定义为：</p>
<script type="math/tex; mode=display">
\mu_{pq}=\int\int{(x-x_0)^p(y-y_0)^qf(x,y)dxdy}\tag{5.2}</script><p>其中矩心(x_0,y_0)为：</p>
<script type="math/tex; mode=display">
x_0=\frac{m_{10}}{m_{00}}\ \ \ \ \ \ y_0=\frac{m_{01}}{m_{00}}</script><p>上述都是在连续量上引出的，但是FPGA只能存储离散量，得到的是离散的320<em> 240或者640 </em> 480的离散图像，那么下面引入适用于离散图像的hu不变矩：</p>
<p>对于数字图像，离散化得到，公式如下：</p>
<script type="math/tex; mode=display">
m_{pq}=\sum^M_{x=1}\sum^N_{y=1}x^py^qf(x,y)</script><p>式中p、q=0，1，2….</p>
<p>直接用普通矩或中心矩进行特征表示，不能使特征同时具有平移、旋转和比例不变性，所以我们下面进行归一化使得手势平移、旋转核比例不变性。</p>
<p>2.归一化中心矩定义</p>
<p>当图像发生变化时，m_pq也发生变化，而\mu_pq则具有平移不变性但对旋转依然敏感。</p>
<p>归一化中心矩：</p>
<script type="math/tex; mode=display">
y_{pq}=\frac{\mu_{pq}}{\mu^r_{00}}\\
其中r=\frac{p+q+2}{2},p+q=2,3...</script><p>如果利用归一化中心矩,则特征不仅具有平移不变性，而且还具有比例不变性。</p>
<p>至此我们得到了最终可以应用的不变矩，为了说明其又平移、寻转、放缩不变性，下面我们进行对各个性质进行证明：</p>
<p>（1）中心矩对于f(x,y)的平移具有不变性：</p>
<p>假如新的坐标(x’,y’)</p>
<script type="math/tex; mode=display">
x'=x+\alpha\\
y'=y+\beta</script><p>其中alpha和beta是常数，通过简单的变量代换，可以发现最终的常数会消去，得到f(x,y)和f(x’,y’)的中心矩是相同的。</p>
<p>（2）中心矩对于缩放具有不变性：</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
   x' \\
   y'
  \end{matrix}
  \right]
  = 
  \left[
 \begin{matrix}
   \alpha & 0\\
   0 & \alpha  
  \end{matrix}
  \right] *
  \left[
 \begin{matrix}
   x \\
   y
  \end{matrix}
  \right] \tag{2.4}</script><p>alpha是个常数，(x’,y’)可以看作是(x,y)分别乘以系数alpha得到，对于每一个alpha系数有公式</p>
<script type="math/tex; mode=display">
a'_{pq}=\alpha^{p+q}a_{pq}</script><p>因为alpha是个常数，那么变换前后的中心矩有这样的关系：</p>
<script type="math/tex; mode=display">
\mu'_{pq}=\alpha^{p+q+2}\mu_{pq}</script><p>最后可以得到：</p>
<script type="math/tex; mode=display">
\frac{\mu'_{pq}}{(\mu')^{\frac{p+q}{2}}+1}=\frac{\mu_{pq}}{\mu^{\frac{p+q}{2}}+1}</script><p>式中p+q=2,3…</p>
<p>这也可以称为相似不变矩性。</p>
<p>（3）中心矩对于旋转具有不变性：</p>
<script type="math/tex; mode=display">
\left[
 \begin{matrix}
   x' \\
   y'
  \end{matrix}
  \right]
  = 
  \left[
 \begin{matrix}
   cos\theta & sin\theta \\
   -sin\theta & -cos\theta  
  \end{matrix}
  \right] *
  \left[
 \begin{matrix}
   x \\
   y
  \end{matrix}
  \right] \tag{2.5}</script><p>旋转矩阵的模是1。</p>
<script type="math/tex; mode=display">
J  = 
  \left[
 \begin{matrix}
   cos\theta & sin\theta \\
   -sin\theta & cos\theta  
  \end{matrix}
  \right] =
    \pm1\tag{2.5}</script><p>将(8)和(9)式与公式(2)结合，也可以得到mu’_pq与mu_pq的关系，称为正交不变性。</p>
<p>HU矩利用二阶和三阶归一化中心矩构造了7个不变矩，他们在连续图像条件下可保持平移、缩放和旋转不变，具体定义如下：</p>
<script type="math/tex; mode=display">
I_1=y_{20}+y_{02}\\
I_2=(y_{20}-y_{02})^2+4y^2_{11}\\
I_3=(y_{30}-3y_{12})^2+(3y_{21}-y_{03})^2\\
I_4=(y_{30}+y_{12})^2+(y_{21}+y_{03})^2\\
I_5=(y_{30}-3y_{12})(y_{30}+y_{12})[(y_{30}+y_{12})^2-3(y_{21}+y_{03})^2]\\
+(3y_{21}-y_{03})(y_{21}+y_{03})[3(y_{30}+y_{12})^2-(y_{21}+y_{03})^2]\\
I_6=(y_{20}-y_{02})[(y_{30}+y_{12})^2-(y_{21}+y_{03})^2]\\
+4y_{11}(y_{30}+y_{12})(y_{21}+y_{03})\\
I_7=(3y_{21}-y_{03})(y_{30}+y_{12})[(y_{30}+y_{12})^2-3(y_{21}+y_{03})^2]\\
+(3y_{12}-y_{03})(y_{21}+y_{03})[3(y_{30}+y_{12})^2-(y_{21}+y_{03})^2]\\</script><p>上述共有七个不变矩，如果只需要识别数十个手势的话，只需要实现前两个不变矩即可，在FPGA实现时，需要注意的是，由于算法中有除法运算，需要调用FPAG的除法器IP核，此外，但不得不说，FPGA在处理复杂的算法方面有劣势，一个几十位的除法器延时了二十几个时钟。此外，在上述的特征值中，其特征值数值较小，是比较小的小数，所以过程中最好放大2的20次方左右这样得到整数。</p>
<p><strong>（6）识别实现</strong></p>
<p>在识别之前，先用是个reg寄存器存储十个hu不变矩特征值，这十个值可以是自己做的十个不同手势得到的hu不变矩结果，识别的时候，将识别的手势hu结果和存储的十个hu特征值对比，最靠近谁即认为识别的结果就是该手势，当然，也得设置一个阈值，同时还有误差小于这个阈值，防止图片中没有手势的时候都误识别出结果。</p>
<p><strong>（7）展示</strong></p>
<p>最终采集的图片格式是320*240的(EGO1中没有DDR，存不下640 * 480的图片)，结果如下：</p>
<p><a href="https://imgtu.com/i/RjLsDs"><img src="https://z3.ax1x.com/2021/07/09/RjLsDs.png" alt="RjLsDs.png"></a></p>
<p>下面展示四个手势被处理后的效果：</p>
<p>手势一：</p>
<p><a href="https://imgtu.com/i/RjLruj"><img src="https://z3.ax1x.com/2021/07/09/RjLruj.png" alt="RjLruj.png"></a></p>
<p>手势二：</p>
<p><a href="https://imgtu.com/i/RjLBvQ"><img src="https://z3.ax1x.com/2021/07/09/RjLBvQ.png" alt="RjLBvQ.png"></a></p>
<p>手势三：</p>
<p><a href="https://imgtu.com/i/RjLw8S"><img src="https://z3.ax1x.com/2021/07/09/RjLw8S.png" alt="RjLw8S.png"></a></p>
<p>手势四：</p>
<p><a href="https://imgtu.com/i/RjLybn"><img src="https://z3.ax1x.com/2021/07/09/RjLybn.png" alt="RjLybn.png"></a></p>
<p>经过灰度化二值化膨胀腐蚀边缘化后提取的效果如下(由于摄像头插上板卡的时候斜了90度，所以VGA显示的时候也斜了90度)：</p>
<p>手势一：</p>
<p><a href="https://imgtu.com/i/RjLcEq"><img src="https://z3.ax1x.com/2021/07/09/RjLcEq.png" alt="RjLcEq.png"></a></p>
<p>手势二：</p>
<p><a href="https://imgtu.com/i/RjLgU0"><img src="https://z3.ax1x.com/2021/07/09/RjLgU0.png" alt="RjLgU0.png"></a></p>
<p>手势三：</p>
<p><a href="https://imgtu.com/i/RjL25V"><img src="https://z3.ax1x.com/2021/07/09/RjL25V.png" alt="RjL25V.png"></a></p>
<p>手势四:</p>
<p><a href="https://imgtu.com/i/RjLWCT"><img src="https://z3.ax1x.com/2021/07/09/RjLWCT.png" alt="RjLWCT.png"></a></p>
<p>经过Hu不变矩后便可以得到不同的特征值，结果如下：</p>
<p>手势一:</p>
<p><a href="https://imgtu.com/i/RjLh2F"><img src="https://z3.ax1x.com/2021/07/09/RjLh2F.png" alt="RjLh2F.png"></a></p>
<p>更多的就不放了，会拖慢博客速度，哈哈哈。</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>

]]></content>
      <categories>
        <category>FPGA</category>
        <category>图像处理</category>
      </categories>
      <tags>
        <tag>FPGA</tag>
        <tag>图像处理</tag>
        <tag>手势识别</tag>
      </tags>
  </entry>
  <entry>
    <title>jupyter lab实现跨文件的函数调用</title>
    <url>/2021/04/10/jupyter%20lab%E5%AE%9E%E7%8E%B0%E8%B7%A8%E6%96%87%E4%BB%B6%E7%9A%84%E5%87%BD%E6%95%B0%E8%B0%83%E7%94%A8/</url>
    <content><![CDATA[<p>在python各类编译器中，jupyter notebook真的是非常非常棒的调试工具，但是jupyter notebook在大工程方面却是逊于pycharm的，其虽然调试很方便，但是却不是像pycharm一样打开一整个文件夹，所以其团队又开发了jupyter lab来弥补缺陷，而jupyter lab的代码补齐真的又一言难尽，由于jupyter lab本身并不是完善，用了kite工具辅助代码补齐，即使操作完全没问题，kite也可能不工作，工作了也发现kite速度真的超慢又浪费CPU</p>
<a id="more"></a>
<p>算了，毕竟jupyter lab已经是一大进步了，平时我们写代码，模块化是非常重要的，所以经常会分文件进行写函数，然后跨文件调用，python中跨文件调用很简单，如果想在文件A中调用文件B中的max函数，如下即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># 待引用的py文件B路径加到了搜索列表里</span></span><br><span class="line">sys.path.append(<span class="string">r&quot;D:\python\test&quot;</span>) </span><br><span class="line"><span class="keyword">import</span> B</span><br><span class="line">B.<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure>
<p>或者如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="comment"># 待引用的py文件路径加到了搜索列表里</span></span><br><span class="line">sys.path.append(<span class="string">r&quot;D:\python\test&quot;</span>) </span><br><span class="line">form B <span class="keyword">import</span> <span class="built_in">max</span></span><br><span class="line"><span class="built_in">max</span>()</span><br></pre></td></tr></table></figure>
<p>A和B若是在同一路径下，可以无需添加路径。</p>
<p>但上述的方法是用来py文件跨文件调用的，而jupyter中的文件不是py而是ipynb文件，那么上述这个方法就没法在jupyter中用了，官方给出了一个解决方法，先在jupyter中建立一个文件，写入如下代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># In[ ]:</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io, os,sys,types</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> get_ipython</span><br><span class="line"><span class="keyword">from</span> nbformat <span class="keyword">import</span> read</span><br><span class="line"><span class="keyword">from</span> IPython.core.interactiveshell <span class="keyword">import</span> InteractiveShell</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NotebookFinder</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Module finder that locates Jupyter Notebooks&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.loaders = &#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">find_module</span>(<span class="params">self, fullname, path=<span class="literal">None</span></span>):</span></span><br><span class="line">        nb_path = find_notebook(fullname, path)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nb_path:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        key = path</span><br><span class="line">        <span class="keyword">if</span> path:</span><br><span class="line">            <span class="comment"># lists aren&#x27;t hashable</span></span><br><span class="line">            key = os.path.sep.join(path)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.loaders:</span><br><span class="line">            self.loaders[key] = NotebookLoader(path)</span><br><span class="line">        <span class="keyword">return</span> self.loaders[key]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_notebook</span>(<span class="params">fullname, path=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;find a notebook, given its fully qualified name and an optional path</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    This turns &quot;foo.bar&quot; into &quot;foo/bar.ipynb&quot;</span></span><br><span class="line"><span class="string">    and tries turning &quot;Foo_Bar&quot; into &quot;Foo Bar&quot; if Foo_Bar</span></span><br><span class="line"><span class="string">    does not exist.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    name = fullname.rsplit(<span class="string">&#x27;.&#x27;</span>, <span class="number">1</span>)[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> path:</span><br><span class="line">        path = [<span class="string">&#x27;&#x27;</span>]</span><br><span class="line">    <span class="keyword">for</span> d <span class="keyword">in</span> path:</span><br><span class="line">        nb_path = os.path.join(d, name + <span class="string">&quot;.ipynb&quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(nb_path):</span><br><span class="line">            <span class="keyword">return</span> nb_path</span><br><span class="line">        <span class="comment"># let import Notebook_Name find &quot;Notebook Name.ipynb&quot;</span></span><br><span class="line">        nb_path = nb_path.replace(<span class="string">&quot;_&quot;</span>, <span class="string">&quot; &quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(nb_path):</span><br><span class="line">            <span class="keyword">return</span> nb_path</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NotebookLoader</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Module Loader for Jupyter Notebooks&quot;&quot;&quot;</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, path=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.shell = InteractiveShell.instance()</span><br><span class="line">        self.path = path</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load_module</span>(<span class="params">self, fullname</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;import a notebook as a module&quot;&quot;&quot;</span></span><br><span class="line">        path = find_notebook(fullname, self.path)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;importing Jupyter notebook from %s&quot;</span> % path)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># load the notebook object</span></span><br><span class="line">        <span class="keyword">with</span> io.<span class="built_in">open</span>(path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            nb = read(f, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment"># create the module and add it to sys.modules</span></span><br><span class="line">        <span class="comment"># if name in sys.modules:</span></span><br><span class="line">        <span class="comment">#    return sys.modules[name]</span></span><br><span class="line">        mod = types.ModuleType(fullname)</span><br><span class="line">        mod.__file__ = path</span><br><span class="line">        mod.__loader__ = self</span><br><span class="line">        mod.__dict__[<span class="string">&#x27;get_ipython&#x27;</span>] = get_ipython</span><br><span class="line">        sys.modules[fullname] = mod</span><br><span class="line"></span><br><span class="line">        <span class="comment"># extra work to ensure that magics that would affect the user_ns</span></span><br><span class="line">        <span class="comment"># actually affect the notebook module&#x27;s ns</span></span><br><span class="line">        save_user_ns = self.shell.user_ns</span><br><span class="line">        self.shell.user_ns = mod.__dict__</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">          <span class="keyword">for</span> cell <span class="keyword">in</span> nb.cells:</span><br><span class="line">            <span class="keyword">if</span> cell.cell_type == <span class="string">&#x27;code&#x27;</span>:</span><br><span class="line">                <span class="comment"># transform the input to executable Python</span></span><br><span class="line">                code = self.shell.input_transformer_manager.transform_cell(cell.source)</span><br><span class="line">                <span class="comment"># run the code in themodule</span></span><br><span class="line">                exec(code, mod.__dict__)</span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            self.shell.user_ns = save_user_ns</span><br><span class="line">        <span class="keyword">return</span> mod</span><br><span class="line">sys.meta_path.append(NotebookFinder())</span><br></pre></td></tr></table></figure>
<p>上述代码中try后的for在写入文件中的时候，编译器可能会提醒for的缩进有问题，会进行标红，不用管，缩进是没问题的。</p>
<p>然后我们给这个文件另存为后缀为py的文件，在jupyter notebook中进行如下另存为。</p>
<p>千万不要直接rename，直接rename是有问题的，所以不要直接rename。另存为py文件后，把这个文件放到和文件A同一个路径下，那么我们就可以如下在文件A中调用B中的函数了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> 文件名 <span class="comment">#刚刚另存为py的那个文件名</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">r&quot;D:\python\test&quot;</span>) <span class="comment"># 待引用的py文件B路径加到了搜索列表里</span></span><br><span class="line"><span class="keyword">import</span> B</span><br><span class="line">B.<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure>
<p>如果这时候提醒你另存为的那个文件找不到，那么就需要看看你的另存为py的那个文件名名字是否是<strong>大写字母开头</strong>，如果是小写开头也可能出问题(真的就巨坑)。</p>
<p>然后在运行就不会提醒说找不到那个py文件了。但是有可能有如下这样报错</p>
<p>说B文件中没有这个函数，这个一般是新建文件的时候在不同虚拟环境下创建的问题，如果是jupyter lab，如下修改即可</p>
<p>如果是notebook，在命令行中修改。</p>
<p>最后运行完全没问题，如果我们继续在B中写其他函数，或者修改原来的max函数，在A中运行却得不到想要的结果，这个就是A中对B进行了缓存导致的，所以需要shut down这个kernel（即文件A），然后再打开运行即可。</p>
<p>深深体会到jupyter为什么在大工程方面被pycharm锤了，文件调用以及打开文件夹等功能真的需要改进，但不得不说jupyter 调试超棒</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>jupyter</category>
      </categories>
      <tags>
        <tag>jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title>一个有意思的面试题</title>
    <url>/2021/03/27/%E4%B8%80%E4%B8%AA%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
    <content><![CDATA[<p>前两天室友面试深信服的时候，遇到了个有意思的题目，要求用c或者c++求解如下题目：用户输入一个可以包含加减乘除的数学表达式，你给这个表达式加上任意多的括号，使得这个表达式结果最大。</p>
<a id="more"></a>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>来看一下这个题目，比如用户输入了如下表达式</p>
<script type="math/tex; mode=display">
1+2*3+5\tag{1.1}</script><p>那么我们随便加些括号</p>
<script type="math/tex; mode=display">
(1+2)*(3+5)\tag{1.2}</script><p>显然加上括号后比初始的结果要大，我们需要做的就是加些括号使得原始表达式最大。</p>
<p>如果需要比较结果，第一想到的就是枚举出所有的可能性，由于括号个数和位置都是不定的，我们如何编程来找出加上括号的所有可能情况呢？</p>
<p>我们知道一个左括号必然对应一个右括号，且用括号括住一个数是无意义的，比如((3))*2这种的括号就是无意义的，所以就是说一个括号括住至少两个数才是有意义的。看一下式(1.1)，那么我们就可以知道，1的左侧最多有三个有意义的左括号，因为1的右边只有三个数，三个左括号对应三个右括号，1的右侧最多有0个有意义的右括号，因为1左边没有其他数了，同理2的左侧有意义的括号数为2,2的右侧为1,3的左侧为1，右侧为2,5的左侧为0，右侧为3。</p>
<p>我们定义1、2、3、5的左右侧有意义括号数为a0、a1、b0、b1、c0、c1、d0、d1(这里为了方便大家看，定义的很多变量，真正写代码的时候当然是用数组啦)，那么这些量符合如下约束：</p>
<script type="math/tex; mode=display">
a0+b0+c0+d0=a1+b1+c1+d1
\\a0+b0+c0+d0    <=    3
\\a0<=3\ \ \ \ \ a1<=0
\\b0<=2\ \ \ \ \ b1<=1
\\c0<=1\ \ \ \ \ c1<=2
\\d0<=0\ \ \ \ \ d1<=3\tag{1.3}
\\其都属于自然数</script><p>那么我们通过求解出符合条件的值即可知道如何添加括号。</p>
<h2 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h2><p>显然式(1.3)应该是属于一个整型规划问题，那么lingo、matlab、R、python等等甚至可以通过函数或者可视化工具求解，C则需要自己实现整型规划问题的算法，如何求解整型规划问题网上教程很多，在此不做详解。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>上述提出的解法只是自己个人的一点分析，里面得到的括号添加方式是可能有重复的，如((1+2*3+5))最外面的两个括号和一个括号、无括号的情况下是一样的，所以此法并非最优解法，欢迎大家可以提出更好的解法。</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>优化</category>
        <category>趣味题目</category>
      </categories>
      <tags>
        <tag>博客搭建</tag>
        <tag>整型规划</tag>
      </tags>
  </entry>
  <entry>
    <title>数据量化</title>
    <url>/2021/03/16/%E6%95%B0%E6%8D%AE%E9%87%8F%E5%8C%96/</url>
    <content><![CDATA[<p>在训练神经网络时候，参数存储是float类型，服务器或者电脑内存都较大，但是对于部署到硬件加速的时候，片上内存则非常珍贵，往往是不够用的，且硬件端需要存储的是整型，所以我们需要对参数进行量化。</p>
<a id="more"></a>
<h2 id="量化方法"><a href="#量化方法" class="headerlink" title="量化方法"></a>量化方法</h2><p>对于很多研究已经表明，深度学习的参数用16bit进行量化的时候，精度损失很小(甚至一些情况下可以用10bit/8bit进行量化)。对于参数值x，量化公式如下</p>
<script type="math/tex; mode=display">
q(x)=floor(x/scale+zero)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (1)</script><p>这里举例介绍一下scale和zero含义，比如神经网络中float32类型的参数值都在[0,2]之间，那么我们需要将其量化为int8的话,就是说我们想把[0,2]内的值缩放到[0,255]，那么原始值在[0,2]之间的x，放缩到[0,255]之间为x’应该符合如下关系：</p>
<script type="math/tex; mode=display">
\frac{x}{2}=\frac{x'}{255}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2)</script><p>分母代表区域界限大小：2=2-0,255=255-0。那么x‘就等于:</p>
<script type="math/tex; mode=display">
x'=\frac{x}{\frac{2}{255}}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (3)</script><p>那么scale就是2/255,zero是0，所以说scale由原始范围和量化后范围决定。那么zero是用来干嘛的呢？上面是说float32的[0,2]量化为int8，int8是8位的，但是深度学习中参数是正负都有的，所以只有一个sscale一个参数无法确保量化到[0,255]这个固定范围,如果float32是在[-1,1]之间，那么通过上面的式子可以依旧确定scale是2/255，只有一个scale一个参数只能量化到[-127.5,127.5].而zero呢？zero是作为一个偏置，为了使得量化到固定范围，如果我们想量化到[0,255]范围那么zero值为:</p>
<script type="math/tex; mode=display">
zero=x'_{max}-\frac{x_{max}}{scale}=127.5</script><p>为什么式(1)要有一个floor进行截断呢，是由于量化后是要进行取整，则简单进行截断即可，此时量化范围为[0,255]。</p>
<h2 id="解量化"><a href="#解量化" class="headerlink" title="解量化"></a>解量化</h2><p>在数值被量化后，就已经不是本身的值了，所以在每次卷积后需要进行解量化。</p>
<blockquote class="blockquote-center">
            <p>值得一提的是，对于回归类问题，是一定要进行解量化的，比如预测下一阶段某一动物的体重。但是对于分类问题，最终是比较相对大小，所以无需进行解量化，但是如果分类时为了不用解量化，记得不能使用zero，因为zero会导致初始参数的正负值和量化后的正负值不同</p>

          </blockquote>
<p>由于自己做的主要是分类问题，所以都没有用到解量化，不作详解。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>综上，对于分类问题，可以只用一个scale值进行简单的量化，也无需解量化。更多的量化类介绍可以参考如下链接</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/qq_38798425/article/details/107423892">基于FPGA的卷积神经网络实现（五）数据量化（1）</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://zhuanlan.zhihu.com/p/64744154">神经网络量化简介</a></p>

          </blockquote>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>]]></content>
      <categories>
        <category>FPGA</category>
      </categories>
      <tags>
        <tag>FPGA</tag>
        <tag>CNN</tag>
        <tag>加速器</tag>
      </tags>
  </entry>
  <entry>
    <title>latex转word</title>
    <url>/2021/01/24/latex%E8%BD%ACword/</url>
    <content><![CDATA[<p>现在写论文几乎都是在用latex，但是在latex中，只能生成pdf，无法生成word会给我们造成不便，网上有Tex2word等方法，但那些方法是很多年前的，用过发现也不太好用甚至不能用。所以就找到了pandoc这个很棒的工具，它可以将文档在 Markdown、LaTeX、reStructuredText、HTML、Word docx 等多种标记格式之间相互转换，并支持输出 PDF、EPUB、HTML 幻灯片等多种格式。</p>
<a id="more"></a>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><p>对于已经安装了anaconda的，pandoc已经被集成到anaconda中了，可以直接使用，如果没有anaconda的，可以在<a href="https://pandoc.org/installing.html">pandoc</a>官网下载。首先在cmd命令窗口中，将路径转到latex目录下，使用方法如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd D:\latex\latex_example1</span><br></pre></td></tr></table></figure>
<p>将上述路径改为自己文件所在路径即可。</p>
<p>然后输入如下命令，其中example.tex改为你需要转换的tex文件，example.doc是输出的doc文件名</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pandoc example.tex -o example.doc</span><br></pre></td></tr></table></figure>
<p>除了可以进行tex到word的转换，还可以在Markdown、latex、HTML、Word等之间转换，后缀名变成相应的格式即可。更多可以参考下述：</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.jianshu.com/p/6ba04f669d0b">Pandoc 安装与使用</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://segmentfault.com/a/1190000021698926">Pandoc——Pandoc安装、使用、快速上手</a></p>

          </blockquote>
<h2 id="更新1"><a href="#更新1" class="headerlink" title="更新1"></a>更新1</h2><p>论文里面一般有很多张图片，但是上述命令无法转换图片，看了网上的用resource命令然后出现一堆错误，其实pandoc —help就发现他们的命令用的是错误的，不明白为什么那么多人在说什么版本问题。。。</p>
<p>命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pandoc example.tex --resource-path&#x3D;图片文件夹路径 -o example.doc</span><br></pre></td></tr></table></figure>
<p>如相对路径如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pandoc example.tex --resource-path&#x3D;Img -o example.doc</span><br></pre></td></tr></table></figure>
<p>也可以用绝对路径</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pandoc example.tex --resource-path&#x3D;D:\tex\Img -o example.doc</span><br></pre></td></tr></table></figure>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>







]]></content>
      <categories>
        <category>latex</category>
      </categories>
      <tags>
        <tag>latex</tag>
        <tag>word</tag>
      </tags>
  </entry>
  <entry>
    <title>博客搭建和规划</title>
    <url>/2021/01/19/hello-world/</url>
    <content><![CDATA[<p>欢迎来到<a href="http://gezhilai.com/">我的博客</a>! 这是我的第一篇博客. 受到疫情影响，今年都无法留校，但早早的回家也不能荒废了时间，就参考了一些文章搭建了这个博客，一是为了记下自己学习的过程，二也是可以和大家分享知识。下面主要说一说博客搭建参考的文章和未来博客准备发的东西。</p>
<a id="more"></a>
<h2 id="博客搭建"><a href="#博客搭建" class="headerlink" title="博客搭建"></a>博客搭建</h2><h3 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h3><p>搭建过程中，参考了如下几篇文章</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.jianshu.com/p/39562a0d8eb6">可能是最详细的 Hexo + GitHub Pages 搭建博客的教程</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a href="https://zhuanlan.zhihu.com/p/26625249">GitHub+Hexo 搭建个人网站详细教程</a></p>

          </blockquote>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>在后续优化中，强烈推荐下面这个博主</p>
<blockquote class="blockquote-center">
            <p><a href="https://tding.top/">小丁的个人博客</a></p>

          </blockquote>
<p>如果具体操作还有些问题，可以顺带看一下下面的视频，</p>
<blockquote class="blockquote-center">
            <p><a href="https://www.bilibili.com/video/BV16W411t7mq?p=21">使用Hexo博客搭建的个人博客，使用Next主题来进行优化改造</a></p>

          </blockquote>
<h3 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h3><p>1、公式无法显示问题参考如下博客，</p>
<blockquote class="blockquote-center">
            <p><a href="https://blog.csdn.net/weixin_44489823/article/details/105028860">hexo next主题解决无法显示数学公式</a></p>

          </blockquote>
<p>其中mathjax开启可以在markdown顶部写如下命令进行开启显示数学公式</p>
<blockquote class="blockquote-center">
            <p>mathjax: true #显示数学公式</p>

          </blockquote>
<p>2、更新博客后，登录网站后发现没有新内容，是由于浏览器缓存功能导致，在浏览器设置—隐私中清除一下缓存或者等待几分钟即可</p>
<h2 id="博客规划"><a href="#博客规划" class="headerlink" title="博客规划"></a>博客规划</h2><p>在后续中，主要会写一写以前做过和学过的东西，包括FPGA、FPGA图像处理、深度学习、机器学习以及西瓜书、花书和凸优化等书的学习分享。当然必然不会仅限于此，但自己也不知道以后还会学些什么，所以暂且写这些吧，假期期间会把上述几本书仔细读一读。</p>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>







]]></content>
      <categories>
        <category>Hexo</category>
        <category>搭建</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
</search>
