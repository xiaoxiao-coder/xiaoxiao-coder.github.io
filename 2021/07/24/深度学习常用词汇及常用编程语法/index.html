<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="Dc-vXTJ_Z0o0sirD51XHawJ1YZQdyEgKKbOXaI7Mo7E">
  <meta name="baidu-site-verification" content="k7ID4DXsv2">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"gezhilai.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="之前说要更新一下本科期间做的一些东西，然后再讲解一下加速器如何在FPGA上搭建，结果太忙了。。。。木得时间，后面有时间慢慢补上。 最近读了faster rcnn和yolo的源码，里面用到的一些函数自己不常用或者用法新奇或者有时候自己忘掉的，而且最先读faster rcnn，two-stage的源码的确比较复杂，原理很快看懂，读源码花了几天才搞明白，写篇博客不断更新记录一下遇到的一些函数和用法，防止">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习常用词汇及常用编程语法--不断更新中">
<meta property="og:url" content="http://gezhilai.com/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/index.html">
<meta property="og:site_name" content="Apers&#39; Blog">
<meta property="og:description" content="之前说要更新一下本科期间做的一些东西，然后再讲解一下加速器如何在FPGA上搭建，结果太忙了。。。。木得时间，后面有时间慢慢补上。 最近读了faster rcnn和yolo的源码，里面用到的一些函数自己不常用或者用法新奇或者有时候自己忘掉的，而且最先读faster rcnn，two-stage的源码的确比较复杂，原理很快看懂，读源码花了几天才搞明白，写篇博客不断更新记录一下遇到的一些函数和用法，防止">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2021-07-23T19:52:32.000Z">
<meta property="article:modified_time" content="2021-12-22T06:22:12.862Z">
<meta property="article:author" content="Apers">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="目标追踪">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://gezhilai.com/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>深度学习常用词汇及常用编程语法--不断更新中 | Apers' Blog</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?e7dd5726174410a42696b9a0189c1ed0# <app_id>";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Apers' Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">茶亦醉人何必酒,书能香我不须花</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">31</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">25</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">21</span></a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-fw fa-calendar"></i>日程表</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-commonweal">

    <a href="/404/" rel="section"><i class="fa fa-fw fa-heartbeat"></i>公益 404</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links" rel="section"><i class="fa fa-fw fa-link"></i>友链</a>

  </li>
        <li class="menu-item menu-item-movies">

    <a href="/movies/" rel="section"><i class="fa fa-fw fa-film"></i>观影</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-fw fa-book"></i>阅读</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://gezhilai.com/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg#QQ1.jpg">
      <meta itemprop="name" content="Apers">
      <meta itemprop="description" content="若有诗书藏于心,岁月从不败美人">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Apers' Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          深度学习常用词汇及常用编程语法--不断更新中
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
	
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-23 19:52:32" itemprop="dateCreated datePublished" datetime="2021-07-23T19:52:32Z">2021-07-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-22 06:22:12" itemprop="dateModified" datetime="2021-12-22T06:22:12Z">2021-12-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a>
                </span>
            </span>

          
            <span id="/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/" class="post-meta-item leancloud_visitors" data-flag-title="深度学习常用词汇及常用编程语法--不断更新中" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="leancloud-visitors-count"></span>
            </span>
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>30k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>27 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>之前说要更新一下本科期间做的一些东西，然后再讲解一下加速器如何在FPGA上搭建，结果太忙了。。。。木得时间，后面有时间慢慢补上。</p>
<p>最近读了faster rcnn和yolo的源码，里面用到的一些函数自己不常用或者用法新奇或者有时候自己忘掉的，而且最先读faster rcnn，two-stage的源码的确比较复杂，原理很快看懂，读源码花了几天才搞明白，写篇博客不断更新记录一下遇到的一些函数和用法，防止遗忘，后续手机端查看也方便。跑faster rcnn深有感触，这玩意不是给普通人玩的，backbone用mobilenet v2情况下debug一下用10s多，内存耗掉四个g，要是restnet50就更夸张了，还是老老实实等服务器吧。。。(更新：拿到了，4块3090)</p>
<a id="more"></a>
<h2 id="常用名词"><a href="#常用名词" class="headerlink" title="常用名词"></a>常用名词</h2><p>backbone:主干，object detection中提取特征的cnn网络简称backbone</p>
<p>bounding box ：边界框，object detection中框住物体的边界框</p>
<p>ground truth:目标的真实位置</p>
<p>logits:意思是输入时候不需要sigmoid概率化</p>
<h2 id="简称"><a href="#简称" class="headerlink" title="简称"></a>简称</h2><p>fpn:feature pyramid network 图像金字塔网络，多个遍历的窗口</p>
<p>rpn：region proposal network 区域提名网络</p>
<h2 id="Pytorch-amp-Numpy基础原理类"><a href="#Pytorch-amp-Numpy基础原理类" class="headerlink" title="Pytorch &amp; Numpy基础原理类"></a>Pytorch &amp; Numpy基础原理类</h2><p>别人讲的太好了，所以有些就直接放链接了。</p>
<p>dir(类)可以查看类中的方法和属性，比如range(10)打印不可视化，可以dir(range(10)),可以看到有str和repr方法，但其实这两个打印的都不可视化，可以用list强制转换就可以打印了(一些自定义类中，可以自己重写__len__，这样就可以使用len()函数了)。</p>
<h3 id="1、广播机制。"><a href="#1、广播机制。" class="headerlink" title="1、广播机制。"></a>1、广播机制。</h3><p>广播机制主要是在两个tensor/array进行运算时候，维度不一时候用到的机制，由于广播机制元素是从内往外扩充的，所以两个tensor/array之间的维度大小从内到外要一致，不一致的两个tensor/array至少有一个维度为1或者有一个压根没那个维度。</p>
<p>具体介绍的链接如下：</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/60365398">2个规则弄懂numpy的broadcast广播机制 - 知乎 (zhihu.com)</a></p>

          </blockquote>
<h3 id="2、存储和视图。"><a href="#2、存储和视图。" class="headerlink" title="2、存储和视图。"></a>2、存储和视图。</h3><p>torch中经常出现共享内存，只是改变视图和stride，而不分配新的内存空间，比如转置、expand等，<strong>可以用tensor.clone()来拷贝一份得到新的内存空间</strong>，共享内存很大程度是为了应对深度学习超大的参数量，这样不需要新的内存空间，也不需要变动内存存储顺序，可以起到节约空间和加速运行的效果。具体介绍如下：</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.360doc.com/content/19/1130/10/32196507_876476008.shtml">由浅入深地带你了解分析张量</a></p>

          </blockquote>
<p>torch中可以有很多用.来进行调用的方法，可以帮我们查看一些信息，显然就是因为torch底层大多数是以面向对象的类封装的，所以arrya用点调用方法显然比tensor少。</p>
<p>tensor.size(),可以得到其shape</p>
<p>tensor.len()，这个真没有，其实不是没有，只不过用的是tensor.<strong>len</strong>而已，可以用len(tensor)，len内部会调用tensor.<strong>len</strong>,</p>
<h3 id="善用？查看函数"><a href="#善用？查看函数" class="headerlink" title="善用？查看函数"></a>善用？查看函数</h3><h2 id="Pytorch-amp-Numpy"><a href="#Pytorch-amp-Numpy" class="headerlink" title="Pytorch &amp; Numpy"></a>Pytorch &amp; Numpy</h2><p>以下函数都在Python3中得到验证</p>
<p><strong>0、tensor生成</strong></p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.randn((2,3))#生成2行3列的正态分布随机数</span><br><span class="line">b&#x3D;torch.randint(3,10,(4,4))#Returns a tensor filled with random integers generated uniformly between :attr:&#96;low&#96; (inclusive) and :attr:&#96;high&#96; (exclusive).均匀分布，产生low-high的整数，只填一个数默认作为high，low默认是0</span><br><span class="line">c&#x3D;torch.randperm(5)#Returns a random permutation of integers from &#96;&#96;0&#96;&#96; to &#96;&#96;n - 1&#96;&#96;,返回一个0到n-1的随机排序.可以用于随机采样样本，比如采样前百分之70。</span><br><span class="line">d&#x3D;torch.rand((2,3))#2行3列的[0,1)的均匀分布，uniform distribution</span><br><span class="line">e&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a,&quot;\n&quot;,b,&quot;\n&quot;,c,&quot;\n&quot;,d,&quot;\n&quot;,e)</span><br></pre></td></tr></table></figure>
<p>输入如下，其中torch.randint的三个参数含义分别是low，high和size(即shape)，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 0.8787,  0.5344,  0.3134],</span><br><span class="line">       [ 0.9224, -1.2589,  1.7708]]) </span><br><span class="line">tensor([[9, 5, 4, 4],</span><br><span class="line">       [3, 7, 4, 8],</span><br><span class="line">       [8, 6, 9, 9],</span><br><span class="line">       [9, 8, 5, 8]]) </span><br><span class="line">tensor([1, 4, 3, 2, 0]) </span><br><span class="line">tensor([[0.1877, 0.8270, 0.3400],</span><br><span class="line">       [0.2405, 0.6493, 0.7478]]) </span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">       [4., 5., 6.]])</span><br></pre></td></tr></table></figure>
<p><strong>1、zip函数</strong></p>
<p>python自带函数 作用：将np/tensor/dict/list等可迭代对象元素组合，这个元素是指第0维的元素。在目标追踪网络中常出现，用于拼接x、y、w、h等等。其格式：zip``(iterable1,iterable2, …)，即输入的是可迭代对象。</p>
<p>例子</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;[1,2,3]</span><br><span class="line">b&#x3D;[4,5,6]</span><br><span class="line">c&#x3D;zip(a,b)</span><br><span class="line">print(c,type(c))</span><br><span class="line">print(list(c))</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>结果如下，直接print其实调用的是魔法函数<strong>str</strong>，由于未重写该方法，所以其和<strong>repr</strong>方法输出是一致，就是类名 + object at + 地址。而通过list函数，可以zip类的结果变成可视化的list列表，列表中每个元素都是拼接的值，<strong>值得注意的是，在list(c)后，不知道list函数中调用了c自身的某种方法，但这种方法改变了c的某些元素值，使得list(c)再list(c)时候，得到的是一个空列表，所以这个c传递更类似于引用传递，改变了c自身。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;zip object at 0x000001B48B796200&gt; &lt;class &#39;zip&#39;&gt;</span><br><span class="line">[(1, 4), (2, 5), (3, 6)]</span><br></pre></td></tr></table></figure>
<p>上述是可迭代对象list，下面测试一下可迭代对象dict：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#v1,v2,v3可是是任何可迭代对象，如：字符串、列表、元祖、字典</span><br><span class="line">v1 &#x3D; &#123; 1 : 11 , 2 : 22 &#125; #此处可迭代对象为字典</span><br><span class="line">v2 &#x3D; &#123; 3 : 33 , 4 : 44 &#125;</span><br><span class="line">v3 &#x3D; &#123; 5 : 55 , 6 : 66 &#125;</span><br><span class="line"> </span><br><span class="line">v &#x3D; zip (v1,v2,v3)   #压缩</span><br><span class="line">print ( list (v))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出，对于字典，压缩组合是对于索引号的。(通过索引号索引值，用法和np类似)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(1, 3, 5), (2, 4, 6)]</span><br></pre></td></tr></table></figure>
<p>通过zip(*压缩组合元素)可以进行解压会原来样式（未必是完全一样，因为压缩的时候可能就不完整）。例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(list(zip(*zip (v1,v2,v3))))</span><br></pre></td></tr></table></figure>
<p>结果如下，<strong>为什么不直接list(zip(*v))呢？上面说过了，经过list后就会改变原值，会变成空元素，压缩得到的也是空的。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(1, 2), (3, 4), (5, 6)]</span><br></pre></td></tr></table></figure>
<p>zip由于是组合，list后可以看出，其是可以变成list列表的，而list是Iterable的，即可迭代对象，自然可以用for，而list中元素都是一个个元组，元组单个元素赋值给单个变量，可以通过逗号进行(要么一个变量直接接受的赋值是一个元组，要么变量个数就是元组内元素个数，否则报错)，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;[1,2,3,4]</span><br><span class="line">b&#x3D;[4,5,6,7]</span><br><span class="line">c&#x3D;[1,3,5,7]</span><br><span class="line">for i in zip(a,b):</span><br><span class="line">    print(i)</span><br><span class="line">for i,j in zip(a,b):</span><br><span class="line">    print(i,j)</span><br><span class="line">for i,j,k in zip(a,b,c):</span><br><span class="line">    print(i,j,k)</span><br><span class="line">(1, 4)</span><br><span class="line">(2, 5)</span><br><span class="line">(3, 6)</span><br><span class="line">(4, 7)</span><br><span class="line">1 4</span><br><span class="line">2 5</span><br><span class="line">3 6</span><br><span class="line">4 7</span><br><span class="line">1 4 1</span><br><span class="line">2 5 3</span><br><span class="line">3 6 5</span><br><span class="line">4 7 7</span><br></pre></td></tr></table></figure>
<p><strong>1.1、元组赋值方式</strong></p>
<p>元组赋值方式，要么直接赋值给一个变量，那个变量就是接收到一个元组，即那个变量也是元组方式，要么接收值的变量个数和元组内元素数量一样（否则报错），变量之间用逗号隔开。</p>
<p><strong>2、for循环使用方法</strong></p>
<p>for常用功能两个：循环遍历和列表生成式 (其实列表生成式本质也是循环遍历，都用到了<strong>iter</strong>和<strong>next</strong>方法，即迭代器生成和不断获取下一个元素方法)。</p>
<p>本节看完后可以继续看迭代器扩展—生成器等等：<a href="http://gezhilai.com/2021/12/20/算法精粹-一/">算法精粹(一) | Apers’ Blog (gezhilai.com)</a></p>
<p>2.1、循环遍历</p>
<p>循环遍历常用的 for i in range(start,end,stride)：不用细讲(需要注意的是，python语法中需要起始终止位置的，似乎从来不包括end的，比如切片，和verilog区别度显然)。此处主要想讲的是循环遍历的原理，for循环中，起始本质是使用了<strong>iter</strong>制造迭代器，然后用<strong>next</strong>方法不断顺序的读取下一个数据，不可回退，直至最后无元素可读，此刻抛出StopIteration 异常(这个异常估计就是try except处理了)，然后退出。下面介绍一下迭代器等。</p>
<p>2.1.1、Iterable</p>
<p>Iterable即可迭代对象，循环遍历的对象必须是可迭代对象，其是一个元素个数明确且可迭代的对象(可迭代即可遍历)，常用的有list、np、tensor、str、tuple、st、dict等</p>
<p>可迭代对象自带<strong>iter</strong>方法，通过<strong>iter</strong>即可得到迭代器Iterator(迭代器)，比如a是个list，则可以通过a.<strong>lter</strong>调用迭代器方法，不过其实这个方法不需要我们调用，for的时候会自行调用。这里可以调用一下试试，例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;np.zeros((1,2))</span><br><span class="line">print(a.__iter__)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下：其中method-wrapper的意思是”包装的方法”，后面就是数据类型 + object(物体，对象)+at+地址。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;method-wrapper &#39;__iter__&#39; of numpy.ndarray object at 0x000001B48BF1D0D0&gt;</span><br></pre></td></tr></table></figure>
<p>2.1.2、Iterator</p>
<p>Iterator即迭代器，Iterable对象通过<strong>iter</strong>方法可以得到Iterator(注意得到Iterator并不是返回，需要得到返回Iterator类型的，用iter()强制转换)，Iterable即可迭代对象是一个元素个数已知且可遍历的对象，通过<strong>iter</strong>方法就得到了个数未知但依旧可遍历的对象Iterator，然后通过<strong>next</strong>进行遍历Iterator，<strong>next</strong>方法不断往下取数据，无数据取出时抛出StopIteration 异常停止迭代。</p>
<p>其实可以看出Iterator和Iterable很像，其实Iterable调用<strong>iter</strong>方法得到的Iterator，是一个继承Iterable的子类</p>
<p>2.1.3、判别Iterable和Iterator</p>
<p>判断一个变量的类型，可以通过isinstance()或者type()函数，但两者有所区别，type不认为子类是和父类是同一个类型，但isinstance则认为子类属于父类的类型，父类不属于子类的类型。推荐使用isinstance，因为平时往往认为子类和父类是一个类别的。(instance中是子类属于父类，父类不是子类，针对父类的函数，子类都可以传进去，还可以正确运行：看缪雪峰的面向对象章，这就是类的多态，)</p>
<p>isinstance例子如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;1</span><br><span class="line">print(isinstance(a,int),isinstance(a,str),isinstance(a,dict),isinstance(a,(int,str,dict,list)))</span><br></pre></td></tr></table></figure>
<p>结果如下，判别属于某个类别，isinstance第二个元素可以是元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True False False True</span><br></pre></td></tr></table></figure>
<p>在看一下isinstance和type区别，如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">class A():</span><br><span class="line">    pass</span><br><span class="line">class B(A):</span><br><span class="line">    pass</span><br><span class="line">print(isinstance(A(),A),isinstance(B(),A),type(B())&#x3D;&#x3D;A)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">True True False</span><br></pre></td></tr></table></figure>
<p>下面看一下迭代器的类别：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">from collections import Iterator,Iterable</span><br><span class="line">a&#x3D;np.zeros((1,2))</span><br><span class="line">print(isinstance(a,Iterable))</span><br><span class="line">print(isinstance(a,Iterator))</span><br><span class="line">print(len(a),a.__len__())</span><br></pre></td></tr></table></figure>
<p>结果如下，np数组是可迭代对象，可以看出父类不属于子类；同时前面也提过，可迭代对象可以有长度属性，其实len()的本质就是调用了Iterable中的<strong>len</strong>方法，不过不加括号的话只会显示方法+类+object+at+地址（具体原因不知）。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br><span class="line">1 1</span><br></pre></td></tr></table></figure>
<p>下面可以看出子类输入父类，<strong>通过iter函数即可强制将变量转成迭代器</strong>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a1&#x3D;iter(a)#iter转换成迭代器</span><br><span class="line">print(isinstance(a1,Iterable))</span><br><span class="line">print(isinstance(a1,Iterator))</span><br><span class="line">print(len(a1))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出a1既属于可迭代对象，也属于迭代器，即子类既是属于子类也属于父类，用len的时候，报错了，因为迭代器长度是未知的，无法用len方法，只能用<strong>next</strong>方法。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">True</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-163-dff46c0da30e&gt; in &lt;module&gt;</span><br><span class="line">      2 print(isinstance(a1,Iterable))</span><br><span class="line">      3 print(isinstance(a1,Iterator))</span><br><span class="line">----&gt; 4 print(len(a1))</span><br><span class="line"></span><br><span class="line">TypeError: object of type &#39;iterator&#39; has no len()</span><br></pre></td></tr></table></figure>
<p><strong>记住需要返回迭代器得用iter(),需要返回取迭代器值的，用next,直接调用<strong>iter</strong>和<strong>next</strong>没有想要的返回值。</strong></p>
<p>下面看一下迭代器利用next取值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a2&#x3D;np.arange(0,10,1).reshape(2,-1)</span><br><span class="line">print(a2)</span><br><span class="line">a2&#x3D;iter(a2)</span><br><span class="line">print(next(a2))</span><br><span class="line">print(next(a2))</span><br><span class="line">print(next(a2))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出，通过next方法不断往下取值，当没有值可取时，抛出StopIteration</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[0 1 2 3 4]</span><br><span class="line"> [5 6 7 8 9]]</span><br><span class="line">[0 1 2 3 4]</span><br><span class="line">[5 6 7 8 9]</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">StopIteration                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-182-473194238b9f&gt; in &lt;module&gt;</span><br><span class="line">      4 print(next(a2))</span><br><span class="line">      5 print(next(a2))</span><br><span class="line">----&gt; 6 print(next(a2))</span><br><span class="line"></span><br><span class="line">StopIteration: </span><br></pre></td></tr></table></figure>
<p>2.1.4 for本质</p>
<p>for的本质就是Iterator通过不断调用next()实现的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;np.arange(0,5,1)</span><br><span class="line">for i in a:</span><br><span class="line">    pass</span><br></pre></td></tr></table></figure>
<p>这个for循环等价于如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a1&#x3D;iter(a)</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        i&#x3D;next(a1)</span><br><span class="line">    except StopIteration:</span><br><span class="line">        break</span><br></pre></td></tr></table></figure>
<p>之前说过了，next是不可以回退的，所以一个Iterator只能遍历一次即用不了了，但Iterable是可以多次遍历的，所以每次for时都用到<strong>iter</strong>，每次<strong>iter</strong>都会得到一个新的Iterator，再next就好了。</p>
<p>2.2、列表生成式、集合生成式、字典生成式</p>
<p>列表生成式如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;np.arange(4)</span><br><span class="line">b&#x3D;[i for i in a]</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0, 1, 2, 3]</span><br></pre></td></tr></table></figure>
<p>字典生成式如下，其中items()方法是输入字典类型，返回list形式的可迭代对象。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#d &#x3D; &#123;key: value for (key, value) in iterable&#125;</span><br><span class="line">d1 &#x3D; &#123;&#39;x&#39;: 1, &#39;y&#39;: 2, &#39;z&#39;: 3&#125;</span><br><span class="line">d2 &#x3D; &#123;k: v for (k, v) in d1.items()&#125;</span><br><span class="line">print(d2)</span><br><span class="line">print(d1.items())</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&#39;x&#39;: 1, &#39;y&#39;: 2, &#39;z&#39;: 3&#125;</span><br><span class="line">dict_items([(&#39;x&#39;, 1), (&#39;y&#39;, 2), (&#39;z&#39;, 3)])</span><br></pre></td></tr></table></figure>
<p>集合生成式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#集合生成式</span><br><span class="line">s1&#x3D;&#123;x for x in range(10)&#125;</span><br><span class="line">print(s1)#集合无序性？所以没法切片</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;0, 1, 2, 3, 4, 5, 6, 7, 8, 9&#125;</span><br></pre></td></tr></table></figure>
<p>谈到了各类生成式，却没说元组生成式，貌似按照上述的做法，在括号内进行for得到的就是元组生成式，然而其实不是这样的，其实得到的是Generator。下面讲一下生成器</p>
<p>2.2.1、Generator生成器</p>
<p>先来看一下用法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#生成器</span><br><span class="line">a&#x3D;(i**2 for i in [1,2,3,4])</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出显然得到的不是一个元组，而是一个generator</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;generator object &lt;genexpr&gt; at 0x000001B48B769970&gt;</span><br></pre></td></tr></table></figure>
<p>生成器和迭代器很像，我们用next试试：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">print(next(a))</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">9</span><br><span class="line">16</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">StopIteration                             Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-194-2251f521890f&gt; in &lt;module&gt;</span><br><span class="line">      3 print(next(a))</span><br><span class="line">      4 print(next(a))</span><br><span class="line">----&gt; 5 print(next(a))</span><br><span class="line"></span><br><span class="line">StopIteration: </span><br></pre></td></tr></table></figure>
<p>似乎这个和迭代器没有区别呀，似乎就是含有1,4,9,16的迭代器。</p>
<p>其实可以理解为生成器Generator就是特殊的迭代器，只不过其生成器一种迭代遍历过程中才计算的迭代器，就是说它存储的是1,2,3,4，在迭代过程中，才进行了平方操作，即next取值的时候进行了计算，可以理解为：生成器的元素在访问前不会生成，只有当访问时才会生成；如果继续向后访问，那么当前的元素会销毁，这个也可以理解，毕竟next是不可以回头的，之前的数据没有意义，所以销毁节约内存。<strong>而生成器的一种生成方式是将列表生成式改为小括号包裹。</strong></p>
<p>下面谈一下生成器的本质(引用了)：</p>
<ul>
<li>生成器本质上是一个函数</li>
<li><strong>当一个生成器被调用时，它返回一个生成器对象，而不用执行该函数。 当第一次调用 <code>next()</code>方法时，函数向下执行，如果遇到yield则返回 <code>yield 后面的</code>值。 再次调用<code>next()</code>方法时，函数从上次结束的位置继续向下执行，如果遇到yield则返回 <code>yield 后面的</code>值。</strong></li>
<li>可以使用yield来定义一个生成器。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;\n----使用yield生成generator-------&quot;)</span><br><span class="line">def ge():</span><br><span class="line">    print(&quot;第一次yield&quot;)</span><br><span class="line">    yield 1</span><br><span class="line">    print(&quot;第二次yield&quot;)</span><br><span class="line">    yield 2</span><br><span class="line">    print(&quot;第三次yield&quot;)</span><br><span class="line">    yield 3</span><br><span class="line">print(type(o))</span><br><span class="line">o &#x3D; ge()</span><br><span class="line">print(next(o))</span><br><span class="line">print(next(o))</span><br><span class="line">print(next(o))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出o是一个生成器。即生成器其实是可以通过yield关键字来得到。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">----使用yield生成generator-------</span><br><span class="line">&lt;class &#39;generator&#39;&gt;</span><br><span class="line">第一次yield</span><br><span class="line">1</span><br><span class="line">第二次yield</span><br><span class="line">2</span><br><span class="line">第三次yield</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<ul>
<li>生成器本质上是一个函数，如果想要获取这个函数的返回值，我们需要使用异常捕获来获取这个返回值：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def fib(max):</span><br><span class="line">    n,a,b &#x3D; 0,0,1</span><br><span class="line">    while n &lt;max:</span><br><span class="line">        yield b</span><br><span class="line">        a,b &#x3D;b,a+b</span><br><span class="line">        n &#x3D; n+1</span><br><span class="line">    return &#39;done&#39;</span><br><span class="line"></span><br><span class="line">print(&quot;\n-----尝试获得函数返回值------&quot;)</span><br><span class="line">gg&#x3D;fib(6)</span><br><span class="line">while True:</span><br><span class="line">    try:</span><br><span class="line">        x&#x3D;next(gg)</span><br><span class="line">        print(&quot;g:&quot;,x)</span><br><span class="line">    except StopIteration as e:</span><br><span class="line">        print(&#39;返回值等于:&#39;,e.value)</span><br><span class="line">        break</span><br><span class="line">-----尝试获得函数返回值------</span><br><span class="line">g: 1</span><br><span class="line">g: 1</span><br><span class="line">g: 2</span><br><span class="line">g: 3</span><br><span class="line">g: 5</span><br><span class="line">g: 8</span><br><span class="line">返回值等于: done</span><br></pre></td></tr></table></figure>
<ul>
<li>既可以使用next()来迭代生成器，也可以使用for来迭代：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def ge():</span><br><span class="line">    print(&quot;第一次yield&quot;)</span><br><span class="line">    yield 1</span><br><span class="line">    print(&quot;第二次yield&quot;)</span><br><span class="line">    yield 2</span><br><span class="line">    print(&quot;第三次yield&quot;)</span><br><span class="line">    yield 3</span><br><span class="line">o &#x3D; ge()</span><br><span class="line"></span><br><span class="line">print(&quot;\n---迭代generator的方法--------&quot;)</span><br><span class="line">for x in o:</span><br><span class="line">    print(x)#相当于进入到generator函数中，执行下去并得到返回值</span><br><span class="line">---迭代generator的方法--------</span><br><span class="line">第一次yield</span><br><span class="line">1</span><br><span class="line">第二次yield</span><br><span class="line">2</span><br><span class="line">第三次yield</span><br><span class="line">3</span><br></pre></td></tr></table></figure>
<p>至此for结束，参考链接如下:</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/53664886">Python笔记整理 迭代器和生成器</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/progor/p/8414550.html">字典生成式、集合生成式、生成器</a></p>

          </blockquote>
<p><strong>3、拼接函数和分割函数、增加元素等等</strong>。</p>
<p>3.1、torch.stack会增加维度，传入时候需要是<strong>tensor组成的元组或列表，普通列表和元组不行</strong>，如(a,a)或[a,a]，<strong>扩充的维度如果是n，那么就是对n-1维度下的对应位置的元素拼接起来</strong>，然后加个维度。(np里也有stack,用法略有不同)</p>
<p>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.tensor([[[1,2,3],[4,5,6]]])</span><br><span class="line">b&#x3D;torch.stack((a,a),dim&#x3D;3)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>结果如下，dim=3即是在第二维度后面进行扩充，将第二维度下面的元素对应位置组起来加个维度。值得注意的是，torch中习惯用dim，而np习惯用axis。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[1, 1],</span><br><span class="line">          [2, 2],</span><br><span class="line">          [3, 3]],</span><br><span class="line"></span><br><span class="line">         [[4, 4],</span><br><span class="line">          [5, 5],</span><br><span class="line">          [6, 6]]]])</span><br></pre></td></tr></table></figure>
<p>如果dim=2，结果如下，即是在第一维度后面加一个维度，则是将第一维度下的元素对应位置拼接起来组成一个维度。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[[1, 2, 3],</span><br><span class="line">          [1, 2, 3]],</span><br><span class="line"></span><br><span class="line">         [[4, 5, 6],</span><br><span class="line">          [4, 5, 6]]]])</span><br></pre></td></tr></table></figure>
<p>append</p>
<p>3.2、torch.cat</p>
<p>cat不增加维度，传入时候需要是<strong>tensor组成的元组或列表，普通列表和元组不行</strong>，例子:cat([a,b,c],dim=1)。对于维度n，cat只是对该维度内元素进行<strong>顺序拼接，不增加维度。</strong></p>
<p>如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([1,2,3])</span><br><span class="line">b&#x3D;torch.Tensor([1,2,3])</span><br><span class="line">c&#x3D;torch.cat([a,b],dim&#x3D;0)</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br></pre></td></tr></table></figure>
<p>由于cat是顺序拼接，其实上述的这个结果很容易用其他方式实现，比如如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(torch.Tensor(np.append(np.array(a),np.array(b))))</span><br><span class="line">print(torch.Tensor([list(a),list(b)]).reshape(-1))</span><br></pre></td></tr></table></figure>
<p>结果如下,<strong>可以看出通过append或者reshape都可以实现，但是torch中好像没有append函数，所以就先用np作为中介了。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br><span class="line">tensor([1., 2., 3., 1., 2., 3.])</span><br></pre></td></tr></table></figure>
<p>在torch中有cat，numpy中自然有对应的函数，其就是<strong>np.concatenate((a,b),axis = 1)</strong>,功能和torch.cat一致，与stack的区别仅在于没有扩充维度罢了。最典型的维度变换，cifar-10读进来的图片是batch*3*32*32,即一个数据有3072个点，前1024个为r，后1024个为g，最后1024个为b，所以读入后需要reshape(32,32,3),不能是reshape(3,32,32)，因为numpy的reshape是按顺序来的，这样就错了。reshape(32,32,3)后，我们希望可以imshow观测，imshow需要的格式是(32,32,3)这个3代表rgb，所以我们在得到a=img.reshape(3,32,32)后，先进行a0=a[0,:,:],a1=a[1,:,:],a1=a[2,:,:],拆开3个通道，再b0=a0.reshape(32,32,1),b1=a1.reshape(32,32,1),b2=a2.reshape(32,32,1),之所以多弄一个维度是为了后续拼接，然后np.concatenate((b0,b1,b2),axis=2)即可得到(32,32,3)的形状。</p>
<p>3.3、split函数</p>
<p>第二个参数是表明其在第几个维度上分割，第一个元素代表分割的步长，函数返回的是元组类型，元组中元素是tensor类型。比如faster rcnn中，a的shape代表[batch_size,所有预测特征层anchors总数]，为了把每个预测特征层分开，就可以如下操作，假设第一个预测特征层anchor总数为3，第二个预测特征层anchors是2；此法适合之前拼接时候记住个数的，然后在分割开。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">a&#x3D;torch.tensor([[1,2,3,4,5],[6,7,8,9,10]])</span><br><span class="line">b&#x3D;[3,2]</span><br><span class="line">c&#x3D;a.split(b,1)</span><br><span class="line">print(a,&quot;\n&quot;,c)</span><br></pre></td></tr></table></figure>
<p>输出如下，说明分割是形参传递，并非引用，不改变原始a。返回的结果是元组形式。元组和list，tensor，np都是可迭代格式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ 1,  2,  3,  4,  5],</span><br><span class="line">        [ 6,  7,  8,  9, 10]]) </span><br><span class="line"> (tensor([[1, 2, 3],</span><br><span class="line">        [6, 7, 8]]), tensor([[ 4,  5],</span><br><span class="line">        [ 9, 10]]))</span><br></pre></td></tr></table></figure>
<p><strong>torch中split和numpy不一样，numpy中split是按照下标切的，如下的[0,1]就是按照下标0及其之前切成一个，0到1之间切成一个，剩下的切作为一个</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(np.split(np.array([[1,2,3],[4,5,6]]),[0,1]))</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出其得到的是列表，而torch是元组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[array([], shape&#x3D;(0, 3), dtype&#x3D;int32), array([[1, 2, 3]]), array([[4, 5, 6]])]</span><br></pre></td></tr></table></figure>
<p>3.4、分割 tensor.unbind</p>
<p>tensor.unbind，比如[[1,2,3],[4,5,6]],则a.unbind(1)即在维度1上分割，得到[1,4],[2,5],[3,6]，其是把设定维度上每个元素分割开，得到一个元组。</p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a.unbind(0),&quot;\n&quot;,a.unbind(1))</span><br><span class="line">(tensor([1., 2., 3.]), tensor([4., 5., 6.])) </span><br><span class="line">(tensor([1., 4.]), tensor([2., 5.]), tensor([3., 6.]))</span><br></pre></td></tr></table></figure>
<p><strong>4.维度变化</strong></p>
<p>1、tensor.fltten（array的用法不一样）</p>
<p>flatten(0,-2)<strong>即代表从第0维度一直拉平到倒数第二维度截止，拉平的元素以倒数第二个维度的元素为一个整体</strong>，即只有倒数第二个维度的元素作为总体进行拉平，拉平应该是这样格式[[1,2,3],[4,5,6],[7,8,9]]。</p>
<p>flatten(1)代表从1维度一直拉平到最后。</p>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(a)</span><br><span class="line">print(a.flatten(0,-1))</span><br><span class="line">print(a.flatten(0,-2))</span><br><span class="line">print(a.flatten(0))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br><span class="line">tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br></pre></td></tr></table></figure>
<p>4.2、reshape</p>
<p>用于改变维度，常用-1进行自动填充某个维度。tensor.reshape((0维度大小，1维度大小。。。))。其可以用于不连续空间的维度调整</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.randn((2,3))</span><br><span class="line">print(a)</span><br><span class="line">print(a.reshape((3,2)))</span><br><span class="line">tensor([[-0.4904,  0.7570, -0.5010],</span><br><span class="line">        [ 0.7374,  2.9273, -2.4853]])</span><br><span class="line">tensor([[-0.4904,  0.7570],</span><br><span class="line">        [-0.5010,  0.7374],</span><br><span class="line">        [ 2.9273, -2.4853]])</span><br></pre></td></tr></table></figure>
<p>4.3、Tensor.view</p>
<p>view也可以用于改变维度，和reshape类似，但是有区别，<strong>和reshape的区别是view要内存连续存储，reshape可以不连续</strong>。<strong>需要注意的是，array的view是用来改变dtype和type的，用法不一样，可以用array.view？来查看函数的help。</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">print(b.view(-1))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([1., 2., 3., 4., 5., 6.])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.],</span><br><span class="line">        [5., 6.]])</span><br></pre></td></tr></table></figure>
<p>4.3、tensor.expand 和 tensor.expanda_as</p>
<p>expand（）函数的功能是用来扩展张量中某维数据的尺寸，它返回输入张量在某维扩展为更大尺寸后的张量。其扩展维度的本质是和广播机制一致(详见本文的广播机制)，即维度从后往前和扩展成的维度对比，必须要完全一致，不一致的必须是该tensor原始维度为1(广播机制是任意一个为1即可)。<strong>值得注意的是，扩展张量不会分配新的内存，只是在存在的张量上创建一个新的视图，其原始tensor和处理后的tensor是共享内存的。关于视图和存储，详见本文的tensor存储和视图原理</strong></p>
<p>使用如下：tensor.expand((第0维度大小，第1维度大小，第2维度大小。。。))例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1],[4]])</span><br><span class="line">b&#x3D;a.expand(2,3)</span><br><span class="line">print(a,&quot;\n&quot;,b)</span><br></pre></td></tr></table></figure>
<p>结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1.],</span><br><span class="line">        [4.]]) </span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [4., 4., 4.]])</span><br></pre></td></tr></table></figure>
<p>在来看一下内存共享，证明通过expand得到的b只是一个新的视图，不是一个新的存储：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">b[1,1]&#x3D;5</span><br><span class="line">print(a,&quot;\n&quot;,b)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出b变了一个值，导致一行全变了，a也变了，可以看出b只是一个新视图，对应新的stride而已，而没有新的空间，如果需要新的空间，可以使用<strong>b=tensor.copy()</strong>得到的b就是新存储。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1.],</span><br><span class="line">        [5.]]) </span><br><span class="line">tensor([[1., 1., 1.],</span><br><span class="line">        [5., 5., 5.]])</span><br></pre></td></tr></table></figure>
<p>对于expand_as其实作用和expand是一致的，只是expand需要的参数直接是shape，而expand_as需要的参数直接是tensor变量，就可以将变量变为传入tensor维度一样了。例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3,4],[5,6,7,8]])</span><br><span class="line">b&#x3D;torch.Tensor([5,6,7,8])</span><br><span class="line">print(b.expand_as(a))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5., 6., 7., 8.],</span><br><span class="line">        [5., 6., 7., 8.]])</span><br></pre></td></tr></table></figure>
<p>4.4 None—增加维度（可以用于np和tensor中，list不可以，list可以先转换在转回来）</p>
<p>None在i维度之后出现，其实就是给第i维度的元素增加一个维度，就是加个[]。在0维度上是None，就是给np数组总体多加个括号作为维度，比如a[:,None]就是在第0维度后加上一个维度，a[None]就是在最外面加上一个维度。。</p>
<p>如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;np.zeros((4))</span><br><span class="line">b&#x3D;np.ones((3))</span><br><span class="line">c&#x3D;np.ones((4,5))</span><br><span class="line">a&#x3D;a[:,None]</span><br><span class="line">print(a)</span><br><span class="line">b&#x3D;b[None,:]</span><br><span class="line">print(b)</span><br><span class="line">c&#x3D;c[:,None]</span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure>
<p>得到的结果如下，在0维度后加None，其实就是给原来0维度的元素加个维度，在0维度之前加None，就总给np数组总体加个维度。注意：变量c有两个维度，所以中间加个None，应该是c[:,None,:]，如果None后还有维度，但是我们没写，默认是：，如果写了，则服从切片原理。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[[0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]</span><br><span class="line"> [0.]]</span><br><span class="line">[[1. 1. 1.]]</span><br><span class="line">[[[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]</span><br><span class="line"></span><br><span class="line"> [[1. 1. 1. 1. 1.]]]</span><br></pre></td></tr></table></figure>
<p>None除了增加维度外，还可以用于测试一个变量是否被定义，比如我们需要一个length变量来存储输入数据的长度，如果length没有定义，那么就丢出错误或者定义一个。例子如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">if a is None:</span><br><span class="line">    print(&quot;a 不存在&quot;)</span><br><span class="line">    raise ValueError(&quot;a should not be None when box_predictor &quot;</span><br><span class="line">                                 &quot;is not specified&quot;)#圆括号隐式转换，加一个括号中，可以应对一行语句太长，分为多行。</span><br><span class="line">else:</span><br><span class="line">    print(&quot;a 是存在的&quot;)</span><br><span class="line">if length is None:</span><br><span class="line">    raise ValueError(&quot;length should not be None when box_predictor &quot;</span><br><span class="line">                                 &quot;is not specified&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;length 是存在的&quot;)</span><br></pre></td></tr></table></figure>
<p>结果如下，a是存在的，所以执行的是else语句，而length是不存在的，所以我们认为抛出一个错误。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a 是存在的</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">NameError                                 Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-21-e7b9afa88565&gt; in &lt;module&gt;</span><br><span class="line">      5 else:</span><br><span class="line">      6     print(&quot;a 是存在的&quot;)</span><br><span class="line">----&gt; 7 if length is None:</span><br><span class="line">      8     raise ValueError(&quot;length should not be None when box_predictor &quot;</span><br><span class="line">      9                                  &quot;is not specified&quot;)#圆括号隐式转换，加一个括号中，可以应对一行语句太长，分为多行。   </span><br><span class="line"></span><br><span class="line">NameError: name &#39;length&#39; is not defined</span><br></pre></td></tr></table></figure>
<p>4.4.1、not</p>
<p>由于上面None说到了可以判别一个量是否被定义，这里就要说一下not，而not可以用于判断一个变量是否为空。首先看一下not的本质，其本质就是对布尔值进行取反，而空变量的布尔属性是False的(python中False和True首字母都大写)，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;[]</span><br><span class="line">print(bool(a))</span><br><span class="line">b&#x3D;[0,2,3]</span><br><span class="line">print(bool(b))</span><br></pre></td></tr></table></figure>
<p>结果如下，可以看出空变量的布尔属性是False，而非空变量的布尔属性是True。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">False</span><br><span class="line">True</span><br></pre></td></tr></table></figure>
<p>所以可以通过如下语句判断一个量是否空的:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if not a:</span><br><span class="line">    print(&quot;a 是空的&quot;)</span><br></pre></td></tr></table></figure>
<p>其中not的功能可以理解为将布尔值取反。</p>
<p>4.5、tensor.permute</p>
<p>permute有排序、置换的意思，tensor.permute（1,0）就是把原来维度1的数量放到维度0上，把原来维度0的数量放到维度1上。比如a.permute(3,0,2,1)就是把原本shape：[a,b,c,d]变成[d,a,c,b]。其是用于维度交换的。</p>
<p>测试如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3],[4,5,6]])</span><br><span class="line">b&#x3D;a.permute(1,0)</span><br><span class="line">print(b)</span><br><span class="line">b[1,1]&#x3D;0</span><br><span class="line">print(b,&quot;\n&quot;,a)</span><br></pre></td></tr></table></figure>
<p>输出如下，可以看出，其实permute置换只是得到一个新的视图，而没有新的存储空间，其和a是共享的，这个也符合torch中节约空间和加速的出发点。可以用tensor.copy()开辟一段新存储空间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 4.],</span><br><span class="line">        [2., 5.],</span><br><span class="line">        [3., 6.]])</span><br><span class="line">tensor([[1., 4.],</span><br><span class="line">        [2., 0.],</span><br><span class="line">        [3., 6.]]) </span><br><span class="line"> tensor([[1., 2., 3.],</span><br><span class="line">        [4., 0., 6.]])</span><br></pre></td></tr></table></figure>
<p>4、unsqueeze</p>
<p>可以增加一个维度，和stack比较类似，只是stack有一个拼接的过程。其增加维度方式和stack类似。在a=[[1,2,3],[4,5,6]]的时候，a[:,0]显然会造成降维，可以a[:,0].unsqueeze(1)这样就可以保持也是二维的，增加的维度是维度1，即在维度0后面增加，其实对于unsqueeze增加维度，传入参数是n，就是给原tensor的n-1后加个维度，也就是给n-1维度的元素加个维度。</p>
<p>例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.randperm(10).reshape((2,5))</span><br><span class="line">print(a)</span><br><span class="line">print(a.unsqueeze(0),&quot;\n&quot;,a.unsqueeze(1),&quot;\n&quot;,a.unsqueeze(2))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5, 3, 2, 9, 1],</span><br><span class="line">        [4, 0, 7, 6, 8]])</span><br><span class="line">tensor([[[5, 3, 2, 9, 1],</span><br><span class="line">         [4, 0, 7, 6, 8]]]) </span><br><span class="line">tensor([[[5, 3, 2, 9, 1]],</span><br><span class="line"></span><br><span class="line">        [[4, 0, 7, 6, 8]]]) </span><br><span class="line">tensor([[[5],</span><br><span class="line">         [3],</span><br><span class="line">         [2],</span><br><span class="line">         [9],</span><br><span class="line">         [1]],</span><br><span class="line"></span><br><span class="line">        [[4],</span><br><span class="line">         [0],</span><br><span class="line">         [7],</span><br><span class="line">         [6],</span><br><span class="line">         [8]]])</span><br></pre></td></tr></table></figure>
<p>squeeze是去除维度的，其只能去除维度值为1的维度，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(a.unsqueeze(0).squeeze(0))</span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[5, 3, 2, 9, 1],</span><br><span class="line">        [4, 0, 7, 6, 8]])</span><br></pre></td></tr></table></figure>
<p><strong>5.排序函数torch.topk()</strong></p>
<p>torch.topk()，如名字般，是为了求tensor的某个维度的前k大或前k小的值（还有index）。其具体用法如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">topk(input, k, dim&#x3D;None, largest&#x3D;True, sorted&#x3D;True, *, out&#x3D;None) -&gt; (Tensor, LongTensor)</span><br><span class="line">input--tensor数据</span><br><span class="line">k--指定k值</span><br><span class="line">dim--指定维度</span><br><span class="line">largest--默认是 True，则从大到小排序，False则从小到大排序。</span><br><span class="line">sorted--默认是 True，即返回是按照顺序排好的</span><br></pre></td></tr></table></figure>
<p>例如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.Tensor([[1,2,3,4],[5,6,7,8]])</span><br><span class="line">b&#x3D;torch.topk(a,3,dim&#x3D;1)</span><br><span class="line">print(b)</span><br></pre></td></tr></table></figure>
<p>输出结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.return_types.topk(</span><br><span class="line">values&#x3D;tensor([[4., 3., 2.],</span><br><span class="line">        [8., 7., 6.]]),</span><br><span class="line">indices&#x3D;tensor([[3, 2, 1],</span><br><span class="line">        [3, 2, 1]]))</span><br></pre></td></tr></table></figure>
<p>6、四舍五入，小数取舍，上下限设置等等</p>
<p>torch变量的方法clamp，设置下限为0，小于0的自动设置为0，可以用于切片时，但由非法(负数)的情况。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">matched_idxs.clamp(min&#x3D;0)</span><br></pre></td></tr></table></figure>
<p>round函数，使用:a.round()，a是torch变量，该函数是对a进行四舍五入。</p>
<p>还有floor，ceil</p>
<p>7、返回坐标信息torch.where和nonzero</p>
<p>其返回的是元组，例子如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.tensor([1,2,3,4,5,0,1,0])</span><br><span class="line">print(torch.where(torch.eq(a,0)))</span><br></pre></td></tr></table></figure>
<p>得到如下结果,即元组内才是torch变量，torch.where(torch.eq(a,0))[0]才能得到torch.tensor([5,7])</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([5, 7]),)</span><br></pre></td></tr></table></figure>
<p>之所以这样是因为where返回的元组结果是这样的([第0维度坐标]，[第一维度坐标],[第二维度坐标]，)。所以只有一个维度的时候元组也是([第0维度坐标]，)。所以需要片选0才能得到结果。</p>
<p>再例如这样</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a&#x3D;torch.tensor([[1,2,3,4,5,0,1,0]])</span><br><span class="line">print(torch.where(torch.eq(a,0)))</span><br></pre></td></tr></table></figure>
<p>结果就是这样了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor([0, 0]), tensor([5, 7]))</span><br></pre></td></tr></table></figure>
<p>这时候坐标每个维度是分开的，如果想组合到一起，可以通过torch.stack函数来进行。</p>
<p>8、相等np.equal(对应元素相同)和np.array_equal(完全相同)。torch.equal和np.equal一致。</p>
<p>9、打乱顺序，随机采样，torch.randperm()函数，通过打乱顺序，切片取前n个就可以当做是随机采样了。</p>
<p>10、tensor.max(dim=1)</p>
<p>比如a=torch.tensor([[1,8,3],[4,5,6]]),则a.max(dim=0)就是[4,8,6],dim=1则就是为[8,6]</p>
<p>11、meshgrid函数，可以简单理解为网格划线</p>
<p>20、isinstance(a,(list,tuple))这个就是判断是否是list或者tuple类型。非的时候，这样表示：not isinstance()</p>
<p>21、torch.full((100,),0)即是得到100个元素的矩阵，都填充为0</p>
<p>100、不要改动框架中# tpye的语句，因为#代表注释符，但如果# type就是代表类型说明符，随便改动就会导致模型错误。</p>
<p>101、arange，np.arange(100)，生成0-99，np.arange(95,100),生成95-99</p>
<p>102、转置arrya.T。</p>
<p>103、语音处理相关库，图像处理相关库，文件读取相关库os</p>
<p>105、array[0,…]的三个省略号代表所有维度</p>
<p>106、range(start,end,stride)</p>
<p>只写一个数，默认start为0，stride为1</p>
<p><strong>107、图像存储问题</strong></p>
<p><strong>用minist数据集的时候，读入的图片都是1*32*32的，直接变成32<em>32的话，然后imshow(),或者imshow(img,cmap=’gray_r’)都是没有问题的，minist的像素数据很奇特，其中有负数，如果直接将该32\</em>32的数据保存，得到的是一个近乎全黑的图(测试了一下，那些保存的时候都是简单的进行取整，然后保存，然而minist的数据很多负数，其他正数也很小，也就3这样，255才是白色，3可以认为是黑色了)，但是为什么imshow却没问题呢，应该是imshow显示的时候，将负数默认为0了，然后最大值缩放到255，其他等比例放大(或者是总体加上了一个正数，使得最小的负数加上后恰好为0，然后等比例放缩)，所以观测没问题，但是那些PIL等等函数保存成jpg却成了黑图的原因(PIL这些图看来是保存的时候简单取整，读入的时候却会自动缩放为0-255)。那么我们保存的时候，先将负数变为0，然后等比例放大。img*(img&gt;0))*255/np.max(img),这样即可，值得注意的是，np中的sum和max无论输入多少维度的，得到的只有一个值。</strong></p>
<p>注：array都代表np变量，即np.array();tensor都代表tensor变量，即torch.Tensor</p>
<p>np里面习惯用axis，比如array.max函数，split（array,下标的list or 等分，dim）函数。split没有array.split格式。np中有很多都没有array.函数的格式，比如array.append(array)就不可以，得np.append(array1,array2)，而list却是可以的。</p>
<p><strong>np中的sum和max无论输入多少维度的，得到的只有一个值。</strong></p>
<p>而torch中喜欢用dim</p>
<p>函数+？可以得到函数介绍，比如torch.stack?，记得函数后面不要加括号。</p>
<p>verilog中负数不能比较大小，坑爹呀</p>
<p>json</p>
<p>torch中迭代器输入的数据，(batchsize，维度，行，列)，label是(batchsize)，输入的数据类型是torch.float,分类的label需要是int64(交叉熵函数需要这个数据类型)，int64在torch中是torch.long,一般定义数据的时候，可以直接指定dtype，也可以定义好变量a后，a.long这样修改dtype。对于一些机器学习的分类算法，比如sklearn的svm或者随机森林等等，输入(行，列)，一行是一个数据，label是(行数)</p>
<h2 id="文件读取"><a href="#文件读取" class="headerlink" title="文件读取"></a>文件读取</h2><p>np.loadtxt(“a.txt”,dtype = np.complex128),这就是以复数的形式读取文件，否则默认形式是float，float是实数域的，自然不可以读复数域的。</p>
<h2 id="库安装"><a href="#库安装" class="headerlink" title="库安装"></a>库安装</h2><p>指定镜像地址的命令如下</p>
<p>pip install -i 镜像地址 包名</p>
<p>例如： pip install -i <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a> numpy</p>
<p>国内镜像地址：</p>
<p>清华：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pypi.tuna.tsinghua.edu.cn/simple">https://pypi.tuna.tsinghua.edu.cn/simple</a></p>
<p>阿里云：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://mirrors.aliyun.com/pypi/simple/">http://mirrors.aliyun.com/pypi/simple/</a></p>
<p>中国科技大学 <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pypi.mirrors.ustc.edu.cn/simple/">https://pypi.mirrors.ustc.edu.cn/simple/</a></p>
<p>gdal库的安装：</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/nima1994/article/details/79207805/">python gdal安装与简单使用</a></p>

          </blockquote>
<p>华为的mindspore框架安装：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mindspore python=3.7.5</span><br><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple mindspore</span><br></pre></td></tr></table></figure>
<p>之所以新建一个环境，是因为mindspore所需要的python环境是3.7及以上的，若3.7之下的版本，是装不成功的。</p>
<p>cv2安装：</p>
<figure class="highlight txt"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -i https://pypi.tuna.tsinghua.edu.cn/simple opencv-python</span><br></pre></td></tr></table></figure>
<h2 id="远程服务器登录报错，账号创建问题-usr-bin-xauth-error-timeout-in-locking-authority-file-home-gezhilai-Xauthority"><a href="#远程服务器登录报错，账号创建问题-usr-bin-xauth-error-timeout-in-locking-authority-file-home-gezhilai-Xauthority" class="headerlink" title="远程服务器登录报错，账号创建问题:/usr/bin/xauth: error/timeout in locking authority file /home/gezhilai/.Xauthority"></a>远程服务器登录报错，账号创建问题:/usr/bin/xauth: error/timeout in locking authority file /home/gezhilai/.Xauthority</h2><p>解决如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir &#x2F;home&#x2F;gezhilai</span><br><span class="line">sudo chown gezhilai:gezhilai -R &#x2F;home&#x2F;gezhilai</span><br><span class="line">sudo usermod -s &#x2F;bin&#x2F;bash gezhilai</span><br></pre></td></tr></table></figure>
<p>参考链接:<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/dong_liuqi/article/details/108842873">(29条消息) Liunx创建新用户登录异常：/usr/bin/xauth: error/timeout in locking authority file /home/liuqidong/.Xauthority_dong_liuqi的博客-CSDN博客</a></p>

          </blockquote></p>
<h2 id="anaconda中新建环境"><a href="#anaconda中新建环境" class="headerlink" title="anaconda中新建环境"></a>anaconda中新建环境</h2><p>命令如下：</p>
<p>conda create —name=labelme python=3.6</p>
<p>代表创建了一个新的环境，环境名称叫labelme，使用的是python版本是3.6.</p>
<p>可以通过conda env list查看我们所创建的所有环境，通过conda activate labelme来激活labelme环境。</p>
<p>若conda激活环境失败(很明显的就是都不在base环境中)，Your shell has not been properly configured to use ‘conda activate’。出现此报错的原因是因为之前的虚拟环境没有退出<code>source deactivate</code>，所以需要重新进入虚拟环境： <code>source activate</code>。</p>
<p>退出虚拟环境：<code>source deactivate</code>或<code>conda deactivate</code></p>
<p>参考链接如下：</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/88c52724b016">conda激活环境失败</a></p>

          </blockquote>
<h2 id="批量安装包"><a href="#批量安装包" class="headerlink" title="批量安装包"></a>批量安装包</h2><p>如果我们在git上需要下载一个代码，使用git clone即可把代码下载到服务器端。有些人会提供批量安装的包的txt文件，所以此时cd到该txt文件路径下:</p>
<p>进行如下即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r a.txt</span><br></pre></td></tr></table></figure>
<h2 id="RuntimeError-CUDA-error-no-kernel-image-is-available-for-execution-on-the-device报错"><a href="#RuntimeError-CUDA-error-no-kernel-image-is-available-for-execution-on-the-device报错" class="headerlink" title="RuntimeError: CUDA error: no kernel image is available for execution on the device报错"></a>RuntimeError: CUDA error: no kernel image is available for execution on the device报错</h2><p>torch运行时候出现了该错误，此时是由于torch的版本和cuda驱动的版本不兼容，导致torch无法使用cuda，从而无法使用GPU报错。以个人理解，torch之所以要和cuda匹配，是因为torch中一些语法使用了GPU时候，用来cuda语法编程，从而要和安装的cuda平台相匹配。(就像你用python2.0的编程，却用3.0的解释器，会出错，当然，也可能没有出错，毕竟每一代还是有很多内容相同)，这里报错时候使用的torch是1.8.1，cuda是10.1的，网上没有查到1.8.1的对应版本，出问题了，我们将torch卸载进行重装。</p>
<p>1、首先如下命令，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure>
<p>结果第一行如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVIDIA-SMI 455.23.04    Driver Version: 455.23.04    CUDA Version: 11.1   </span><br></pre></td></tr></table></figure>
<p>可以看出，该服务器的RTX3090的配套最高驱动是11.1。</p>
<p>2、查看如今的CUDA驱动版本</p>
<p>可以使用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;usr&#x2F;local&#x2F;cuda&#x2F;version.txt</span><br></pre></td></tr></table></figure>
<p>查看当前的CUDA运行版本。</p>
<p>CUDA运行API低于GPU支持驱动是可以的。因为CUDA分为CUDA Drvier API和CUDA Runtime API，所谓的Driver API其实是当前GPU底层支持的最高CUDA命令版本，而CUDA Runtime API是服务器上安装的实际版本，CUDA是向下兼容的，也就是说NVDIA设计3090是基于cuda11.1设计的，但是cuda11.1是含括了11.1以前的所有命令，所以cuda10的驱动命令都是适用的，用cuda10来做一些加速也是可以的，当然11.1cuda可能对其他很多东西做了更好的优化。理论上来说，nvdia-smi的型号是GPU支持的最高驱动版本，你安装更高的Driver API，它就看不懂里面的一些命令了。我们安装的时候自然是对应最好。</p>
<p>3、知道如上结果，就可以按照第2步的CUDA驱动版本安装对应的torch了，其中安装的时候torch、torchvision、torchaudio、cudatoolkit需要对应。</p>
<p>这里需要说明一下cudatoolkit，cat /usr/local/cuda/version.txt得到的结果是平时我们所说的CUDA版本，其是在CUDA Toolkit工具包中，所以该CUDA版本也是CUDA Toolkit工具包的版本。</p>
<p>但是装了Anaconda之后Anaconda也会提供一个cudatoolkit工具包，同样包含了CUDA的运行API，可以用来替代官方CUDA的CUDA Toolkit。这也就是为什么有时候我们通过nvcc-V查看的cuda版本很低(比如7.5)，但是能成功运行cuda9.0的pytorch的原因。因为在安装完anaconda后，运行pytorch代码就会使用anaconda的cudatoolkit，而忽视官方的CUDA Toolkit，所以我们只需要根据anaconda的cudatoolkit包的版本来安装相应的pytorch即可。</p>
<p>查看cudatoolkit直接</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
<p>即可看到cudatoolkit版本，Wheel(即pip安装)的形式，可以看到torch版本是1.7.0+cu11.0的样式，其cu11.0就是anaconda的cudatoolkit，如果是conda安装的，那么可以直接看到cudatoolkit是作为一个库独立存在，可以看到其版本。</p>
<p>由此可知，我们完全可以在anaconda中相对应的cudatoolkit，具体的torch版本对应的torchvision、torchaudio和可选的cudatoolkit版本可以在如下官网查看。</p>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/pytorch/vision">torch、torchvision、python的版本对应</a></p>

          </blockquote>
<blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/get-started/previous-versions/">各个torch版本下及其对应的torchvision、cudatoolkit的安装命令</a></p>

          </blockquote>
<p>由于由第一步知道，cuda最高支持驱动为11.0，那么我们就选用11.0cudatoolkit下的某个toch版本，</p>
<p>在此选用了1.7.0，对应的torchvison可以查上面两个链接，使用wheel安装，命令如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch&#x3D;&#x3D;1.7.0+cu110 torchvision&#x3D;&#x3D;0.8.0+cu110 torchaudio&#x3D;&#x3D;0.7.0 -f https:&#x2F;&#x2F;download.pytorch.org&#x2F;whl&#x2F;torch_stable.html</span><br></pre></td></tr></table></figure>
<p>即安装的cudatoolkit是11.0的，torchvision是其对应版本。</p>
<p>至此安装后即可正常运行。</p>
<p><strong>注:有的时候命令行操作python  a.py可以，但是vscode端运行不可以，是不是因为调用了CUDATOOLKIT，而不是anaconda中的cudatoolkit，导致版本不兼容?</strong></p>
<h2 id="命令行运行py文件"><a href="#命令行运行py文件" class="headerlink" title="命令行运行py文件"></a>命令行运行py文件</h2><p>正常如果不需要配置什么内容的话，直接如下即可运行:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python a.py</span><br></pre></td></tr></table></figure>
<p>但如果有如下这一类的配置文件:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def get_parser():</span><br><span class="line">    &quot;&quot;&quot;Get default arguments.&quot;&quot;&quot;</span><br><span class="line">    parser &#x3D; configargparse.ArgumentParser(</span><br><span class="line">        description&#x3D;&quot;Transfer learning config parser&quot;,</span><br><span class="line">        config_file_parser_class&#x3D;configargparse.YAMLConfigFileParser,</span><br><span class="line">        formatter_class&#x3D;configargparse.ArgumentDefaultsHelpFormatter,</span><br><span class="line">    )</span><br><span class="line">    # general configuration</span><br><span class="line">    parser.add(&quot;--config&quot;, is_config_file&#x3D;True, help&#x3D;&quot;config file path&quot;)</span><br><span class="line">    parser.add(&quot;--seed&quot;, type&#x3D;int, default&#x3D;0)</span><br><span class="line">    parser.add_argument(&#39;--num_workers&#39;, type&#x3D;int, default&#x3D;0)</span><br><span class="line">    </span><br><span class="line">    # network related</span><br><span class="line">    parser.add_argument(&#39;--backbone&#39;, type&#x3D;str, default&#x3D;&#39;resnet50&#39;)</span><br><span class="line">    parser.add_argument(&#39;--use_bottleneck&#39;, type&#x3D;str2bool, default&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # data loading related</span><br><span class="line">    parser.add_argument(&#39;--data_dir&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    parser.add_argument(&#39;--src_domain&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    parser.add_argument(&#39;--tgt_domain&#39;, type&#x3D;str, required&#x3D;True)</span><br><span class="line">    </span><br><span class="line">    # training related</span><br><span class="line">    parser.add_argument(&#39;--batch_size&#39;, type&#x3D;int, default&#x3D;32)</span><br><span class="line">    parser.add_argument(&#39;--n_epoch&#39;, type&#x3D;int, default&#x3D;100)</span><br><span class="line">    parser.add_argument(&#39;--early_stop&#39;, type&#x3D;int, default&#x3D;0, help&#x3D;&quot;Early stopping&quot;)</span><br><span class="line">    parser.add_argument(&#39;--epoch_based_training&#39;, type&#x3D;str2bool, default&#x3D;False, help&#x3D;&quot;Epoch-based training &#x2F; Iteration-based training&quot;)</span><br><span class="line">    parser.add_argument(&quot;--n_iter_per_epoch&quot;, type&#x3D;int, default&#x3D;20, help&#x3D;&quot;Used in Iteration-based training&quot;)</span><br><span class="line"></span><br><span class="line">    # optimizer related</span><br><span class="line">    parser.add_argument(&#39;--lr&#39;, type&#x3D;float, default&#x3D;1e-3)</span><br><span class="line">    parser.add_argument(&#39;--momentum&#39;, type&#x3D;float, default&#x3D;0.9)</span><br><span class="line">    parser.add_argument(&#39;--weight_decay&#39;, type&#x3D;float, default&#x3D;5e-4)</span><br><span class="line"></span><br><span class="line">    # learning rate scheduler related</span><br><span class="line">    parser.add_argument(&#39;--lr_gamma&#39;, type&#x3D;float, default&#x3D;0.0003)</span><br><span class="line">    parser.add_argument(&#39;--lr_decay&#39;, type&#x3D;float, default&#x3D;0.75)</span><br><span class="line">    parser.add_argument(&#39;--lr_scheduler&#39;, type&#x3D;str2bool, default&#x3D;True)</span><br><span class="line"></span><br><span class="line">    # transfer related</span><br><span class="line">    parser.add_argument(&#39;--transfer_loss_weight&#39;, type&#x3D;float, default&#x3D;10)</span><br><span class="line">    parser.add_argument(&#39;--transfer_loss&#39;, type&#x3D;str, default&#x3D;&#39;mmd&#39;)</span><br><span class="line">    return parser</span><br><span class="line">  </span><br><span class="line">def main():</span><br><span class="line">    parser &#x3D; get_parser()</span><br></pre></td></tr></table></figure>
<p>就需要额外的配置了:比如如下，上述中无default的，require为true的都要配置。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python main.py --config DAN&#x2F;DAN.yaml --data_dir &#x2F;remote-home&#x2F;lwq&#x2F;transferlearning&#x2F;code&#x2F;DeepDA&#x2F;data18 --src_domain train18 --tgt_domain test18</span><br></pre></td></tr></table></figure>
<h2 id="在jupyter中使用虚拟环境"><a href="#在jupyter中使用虚拟环境" class="headerlink" title="在jupyter中使用虚拟环境"></a>在jupyter中使用虚拟环境</h2><p>创建好新的环境后，如果使用jupyter时想用该环境的话，还需要配置一下：</p>
<p>首先查看当前的所有环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env <span class="built_in">list</span></span><br></pre></td></tr></table></figure>
<p>输出如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># conda environments:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">base                  *  D:\anzhuang\anaconda</span><br><span class="line">gf                       D:\anzhuang\anaconda\envs\gf</span><br><span class="line">kite                     D:\anzhuang\anaconda\envs\kite</span><br><span class="line">labelme                  D:\anzhuang\anaconda\envs\labelme</span><br><span class="line">mindspore                D:\anzhuang\anaconda\envs\mindspore</span><br><span class="line">new_env                  D:\anzhuang\anaconda\envs\new_env</span><br><span class="line">pycharm                  D:\anzhuang\anaconda\envs\pycharm</span><br><span class="line">pythonProject1           D:\anzhuang\anaconda\envs\pythonProject1</span><br><span class="line">pythonProject2           D:\anzhuang\anaconda\envs\pythonProject2</span><br><span class="line">tensorflow               D:\anzhuang\anaconda\envs\tensorflow</span><br></pre></td></tr></table></figure>
<p>如果我们想在jupyter中使用gf这个环境的话，需要如下操作：</p>
<p>首先激活该环境：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate gf</span><br></pre></td></tr></table></figure>
<p>然后安装ipykernel</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install ipykernel</span><br></pre></td></tr></table></figure>
<p>最后添加对应文件：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -m ipykernel install --name gf</span><br></pre></td></tr></table></figure>
<p>此时已完成新环境的添加，如下打开jupyter即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jupyter notebook</span><br></pre></td></tr></table></figure>
<p>注意：创建新环境的时候，激活后，使用jupyter notebook可能提示没有jupyter，那是没有安装jupyter，安装一下即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install jupyter</span><br></pre></td></tr></table></figure>
<h2 id="在jupyter中某个环境kernel-error"><a href="#在jupyter中某个环境kernel-error" class="headerlink" title="在jupyter中某个环境kernel error"></a>在jupyter中某个环境kernel error</h2><blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_44457768/article/details/106027628">Jupyter notebook中报错，出现kernel error解决方法</a></p>

          </blockquote>
<p>其中jupyter kernelspec list可以查看所有的环境用的kernel路径（json中配置了python编译器位置）</p>
<h2 id="Pycharm中使用anconda中的环境"><a href="#Pycharm中使用anconda中的环境" class="headerlink" title="Pycharm中使用anconda中的环境"></a>Pycharm中使用anconda中的环境</h2><p>file-setting-Project Interpreter-system Interperter处，将python编译器变成anconda的，如果anaconda已加入环境变量中，是可以自动识别出来的</p>
<h2 id="本地打开服务器的tensorboard"><a href="#本地打开服务器的tensorboard" class="headerlink" title="本地打开服务器的tensorboard"></a>本地打开服务器的tensorboard</h2><p>tensorboard —logdir=. —bind_all然后ip:6006就可以在本地打开了</p>
<h2 id="Torch中使用GPU"><a href="#Torch中使用GPU" class="headerlink" title="Torch中使用GPU"></a>Torch中使用GPU</h2><p>代码中，常会发现如下两句:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">model.to(device)</span><br></pre></td></tr></table></figure>
<p>上述的”cuda:0”,如果电脑只有单张独立显卡，就是”cuda”。这两行是放在读取数据之前，意思是将模型复制一份到device上，如果GPU存在(同时torch是GPU版本，cudatooltik也匹配)，torch.cuda.is_available()就会判断成立。然后后面读取数据的时候也通过data.to(device)把数据复制一份到GPU上，初始数据复制到了GPU上，中间的数据自然就会存在GPU的显存中,torch类方法中就有这个to方法。</p>
<p>而数据如下即可，g.cuda()如果之前没有特地的设置的，默认是cuda:0,所以如下即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    h = g.cuda()</span><br><span class="line">    print(h)</span><br></pre></td></tr></table></figure>
<p>若是多块GPU，模型用到GPU上如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = Model()</span><br><span class="line"><span class="keyword">if</span> torch.cuda.device_count() &gt; <span class="number">1</span>:</span><br><span class="line">	model = nn.DataParallel(model，device_ids=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="keyword">elif</span>:</span><br><span class="line">    device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line">    model.to(device)</span><br></pre></td></tr></table></figure>
<p>或者如下：(推荐)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICES&#x27;</span>] = <span class="string">&#x27;0,3&#x27;</span></span><br><span class="line">net = torch.nn.DataParallel(model)</span><br></pre></td></tr></table></figure>
<p>数据加载的话，需要如下,但是.cuda默认是第一章卡，所以还需要os.environ[‘CUDA_VISIBLE_DEVICES’] = ‘0,3’这个命令在之前才可以。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = inputs.cuda()</span><br><span class="line">labels = labels.cuda()</span><br></pre></td></tr></table></figure>
<p>如果多GPU下只用一张卡，还可以如下(不推荐)，如下情况，虽然只指定了一张卡，但是print的结果不变，即不会改变可见显卡，后续还可以用<code>torch.nn.DataParallel(model, device_ids=[1, 2])</code>进行指定，但是必须包含<code>set_device(1)</code>指定的device:1的设备，缺点是仍然会存在占用一些device:0的gpu内存；</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.set_device(<span class="number">1</span>)</span><br><span class="line">print(torch.cuda.device_count()) <span class="comment">#可用GPU数量</span></span><br></pre></td></tr></table></figure>
<p>当然可以通过其他方式制定GPU:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#（1）直接终端中设定 </span></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#（2）python代码中设定：</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;CUDA_VISIBLE_DEVICE&#x27;</span>]=<span class="string">&#x27;1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#（3）使用函数set_device</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">torch.cuda.set_device(<span class="built_in">id</span>)</span><br></pre></td></tr></table></figure>
<p>把Tensor放到GPU上的方法如下，也可以g.to(device)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    h = g.cuda()</span><br><span class="line">    print(h)</span><br></pre></td></tr></table></figure>
<p>把GPU上的Tensor或者Variable的数据返回到CPU上变成numpy格式(比如绘图的时候需要用到，GPU上的数据没法直接在界面上可视化)：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将变量或者数据移到GPU</span></span><br><span class="line">gpu_info = Variable(torch.randn(<span class="number">3</span>,<span class="number">3</span>)).cuda()</span><br><span class="line"><span class="comment"># 将变量或者数据移到CPU</span></span><br><span class="line">cpu_info = gpu_info.cpu().numpy()</span><br></pre></td></tr></table></figure>
<p>注:常用的torch.cuda函数:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.cuda.is_available()<span class="comment">#如果可以cuda驱动gpu，则为true</span></span><br><span class="line">torch.cuda.device_count()<span class="comment">#返回GPU可用数量，2则代表2个</span></span><br><span class="line">torch.cuda.get_device_name()<span class="comment">#返回GPU型号，如GeForce RTX 3090</span></span><br><span class="line">torch.cuda.get_device_properties(<span class="string">&quot;cuda:0&quot;</span>)<span class="comment">#返回GPU性能，显存、线程等等</span></span><br><span class="line">torch.cuda.current_device()<span class="comment">#查看当前GPU使用序号。注意#os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &#x27;1,2&#x27;这种指令会改变torch感知设备的编号，使用了1和#2后，torch.cuda.current_device()感知只会是0或1，因为感知是从0开始的。</span></span><br></pre></td></tr></table></figure>
<p>参考链接:<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/shaopeng568/article/details/95205345">(29条消息) Pytorch to（device）_shaopeng568的专栏-CSDN博客</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/qq_34243930/article/details/106695877">(29条消息) pytorch之多GPU使用——#CUDA_VISIBLE_DEVICES使用 #torch.nn.DataParallel() #报错解决_夏普通-CSDN博客</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/86441879">pytorch多gpu并行训练 - 知乎 (zhihu.com)</a></p>
<h2 id="模型训练中显示启动卷积失败，cuda等等问题。"><a href="#模型训练中显示启动卷积失败，cuda等等问题。" class="headerlink" title="模型训练中显示启动卷积失败，cuda等等问题。"></a>模型训练中显示启动卷积失败，cuda等等问题。</h2><p>os.environ[‘CUDA_VISIBLE_DEVICES’] = ‘/gpu:0’</p>
<p>有一次写代码发现明明写的是：</p>
<p>os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1,2,3,4,5,6,7”</p>
<p>但是却显示只发现了 gpu：0，百思不得其解，百度谷歌都没有发现类似问题，后来检查才发现有一个调用的py文件里写了一句：</p>
<p>os.environ[“CUDA_VISIBLE_DEVICES”] = “0”</p>
<p>删掉之后，可以发现全部的gpu</p>
<h2 id="tf导包失败问题"><a href="#tf导包失败问题" class="headerlink" title="tf导包失败问题"></a>tf导包失败问题</h2><p>tf中，如下导包失败,报错没np_utils这个包，一般这种问题很有可能是版本问题导致包已经到其他仓库里了或者修改了包名等等。这里其实是keras在版本迁移的时候，包的位置迁移了导致导不进来，在__init__.py可以看到。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure>
<p>修改成如下,即多了个python位置，此时导入不报错，但是使用的时候有问题即np_utils.方法都不能用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.python.keras.utils.np_utils</span><br></pre></td></tr></table></figure>
<p>如下即可成功导入并使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.keras.utils <span class="keyword">import</span> np_utils</span><br></pre></td></tr></table></figure>
<p>参考：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/qq_40662487/article/details/109902765">(29条消息) 解决ImportError: cannot import name ‘np_utils‘ from ‘tensorflow.keras.utils‘ 的问题_小了白了兔DY的博客-CSDN博客</a></p>
<h2 id="学习网站"><a href="#学习网站" class="headerlink" title="学习网站"></a>学习网站</h2><blockquote class="blockquote-center">
            <p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.liaoxuefeng.com/wiki/1016959663602400/1017496031185408">缪雪峰的官方网站</a></p>

          </blockquote>
<div>
    
        <div style="text-align:center;color: #ccc;font-size:25px;">- - - - - - - - - - - - - - 本文结束啦，感谢您的观看 - - - - - - - - - - - - - -</div>
    
</div>
    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\07\20\Python之魔法函数\" rel="bookmark">Python之魔法函数</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\03\16\数据量化\" rel="bookmark">数据量化</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="\2021\11\28\Torch类具体说明-以剪枝模型为例\" rel="bookmark">Torch类具体说明--以剪枝模型为例</a></div>
    </li>
  </ul>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>Apers
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://gezhilai.com/2021/07/24/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E7%94%A8%E8%AF%8D%E6%B1%87%E5%8F%8A%E5%B8%B8%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E6%B3%95/" title="深度学习常用词汇及常用编程语法--不断更新中">http://gezhilai.com/2021/07/24/深度学习常用词汇及常用编程语法/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"><i class="fa fa-ship"></i> Python</a>
              <a href="/tags/CNN/" rel="tag"><i class="fa fa-ship"></i> CNN</a>
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-ship"></i> 深度学习</a>
              <a href="/tags/%E7%9B%AE%E6%A0%87%E8%BF%BD%E8%B8%AA/" rel="tag"><i class="fa fa-ship"></i> 目标追踪</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/20/Python%E4%B9%8B%E9%AD%94%E6%B3%95%E5%87%BD%E6%95%B0/" rel="prev" title="Python之魔法函数">
      <i class="fa fa-chevron-left"></i> Python之魔法函数
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/26/%E5%9F%BA%E4%BA%8EFPGA%E7%9A%84%E6%97%A0%E4%BA%BA%E6%9C%BA%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F%E5%AE%9E%E7%8E%B0/" rel="next" title="基于FPGA的无人机目标跟踪系统实现">
      基于FPGA的无人机目标跟踪系统实现 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B8%B8%E7%94%A8%E5%90%8D%E8%AF%8D"><span class="nav-number">1.</span> <span class="nav-text">常用名词</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AE%80%E7%A7%B0"><span class="nav-number">2.</span> <span class="nav-text">简称</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pytorch-amp-Numpy%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86%E7%B1%BB"><span class="nav-number">3.</span> <span class="nav-text">Pytorch &amp; Numpy基础原理类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E3%80%81%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6%E3%80%82"><span class="nav-number">3.1.</span> <span class="nav-text">1、广播机制。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2%E3%80%81%E5%AD%98%E5%82%A8%E5%92%8C%E8%A7%86%E5%9B%BE%E3%80%82"><span class="nav-number">3.2.</span> <span class="nav-text">2、存储和视图。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%96%84%E7%94%A8%EF%BC%9F%E6%9F%A5%E7%9C%8B%E5%87%BD%E6%95%B0"><span class="nav-number">3.3.</span> <span class="nav-text">善用？查看函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pytorch-amp-Numpy"><span class="nav-number">4.</span> <span class="nav-text">Pytorch &amp; Numpy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E4%BB%B6%E8%AF%BB%E5%8F%96"><span class="nav-number">5.</span> <span class="nav-text">文件读取</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%BA%93%E5%AE%89%E8%A3%85"><span class="nav-number">6.</span> <span class="nav-text">库安装</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9C%E7%A8%8B%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%99%BB%E5%BD%95%E6%8A%A5%E9%94%99%EF%BC%8C%E8%B4%A6%E5%8F%B7%E5%88%9B%E5%BB%BA%E9%97%AE%E9%A2%98-usr-bin-xauth-error-timeout-in-locking-authority-file-home-gezhilai-Xauthority"><span class="nav-number">7.</span> <span class="nav-text">远程服务器登录报错，账号创建问题:&#x2F;usr&#x2F;bin&#x2F;xauth: error&#x2F;timeout in locking authority file &#x2F;home&#x2F;gezhilai&#x2F;.Xauthority</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#anaconda%E4%B8%AD%E6%96%B0%E5%BB%BA%E7%8E%AF%E5%A2%83"><span class="nav-number">8.</span> <span class="nav-text">anaconda中新建环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%B9%E9%87%8F%E5%AE%89%E8%A3%85%E5%8C%85"><span class="nav-number">9.</span> <span class="nav-text">批量安装包</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RuntimeError-CUDA-error-no-kernel-image-is-available-for-execution-on-the-device%E6%8A%A5%E9%94%99"><span class="nav-number">10.</span> <span class="nav-text">RuntimeError: CUDA error: no kernel image is available for execution on the device报错</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%BF%90%E8%A1%8Cpy%E6%96%87%E4%BB%B6"><span class="nav-number">11.</span> <span class="nav-text">命令行运行py文件</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8jupyter%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83"><span class="nav-number">12.</span> <span class="nav-text">在jupyter中使用虚拟环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9C%A8jupyter%E4%B8%AD%E6%9F%90%E4%B8%AA%E7%8E%AF%E5%A2%83kernel-error"><span class="nav-number">13.</span> <span class="nav-text">在jupyter中某个环境kernel error</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pycharm%E4%B8%AD%E4%BD%BF%E7%94%A8anconda%E4%B8%AD%E7%9A%84%E7%8E%AF%E5%A2%83"><span class="nav-number">14.</span> <span class="nav-text">Pycharm中使用anconda中的环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E6%89%93%E5%BC%80%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84tensorboard"><span class="nav-number">15.</span> <span class="nav-text">本地打开服务器的tensorboard</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Torch%E4%B8%AD%E4%BD%BF%E7%94%A8GPU"><span class="nav-number">16.</span> <span class="nav-text">Torch中使用GPU</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%98%BE%E7%A4%BA%E5%90%AF%E5%8A%A8%E5%8D%B7%E7%A7%AF%E5%A4%B1%E8%B4%A5%EF%BC%8Ccuda%E7%AD%89%E7%AD%89%E9%97%AE%E9%A2%98%E3%80%82"><span class="nav-number">17.</span> <span class="nav-text">模型训练中显示启动卷积失败，cuda等等问题。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tf%E5%AF%BC%E5%8C%85%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98"><span class="nav-number">18.</span> <span class="nav-text">tf导包失败问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%AB%99"><span class="nav-number">19.</span> <span class="nav-text">学习网站</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Apers"
      src="/images/avatar.jpg#QQ1.jpg">
  <p class="site-author-name" itemprop="name">Apers</p>
  <div class="site-description" itemprop="description">若有诗书藏于心,岁月从不败美人</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">21</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/xiaoxiao-coder" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;xiaoxiao-coder" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:xiaoxiaocoder1@gmail.com" title="E-Mail → mailto:xiaoxiaocoder1@gmail.com" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://plus.google.com/yourname" title="Google → https:&#x2F;&#x2F;plus.google.com&#x2F;yourname" rel="noopener external nofollow noreferrer" target="_blank"><i class="fa fa-fw fa-google"></i>Google</a>
      </span>
  </div>


<div style="">
  <canvas id="canvas" style="width:60%;">当前浏览器不支持canvas，请更换浏览器后再试</canvas>
</div>
<script>
(function(){

   var digit=
    [
        [
            [0,0,1,1,1,0,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,0,1,1,0],
            [0,0,1,1,1,0,0]
        ],//0
        [
            [0,0,0,1,1,0,0],
            [0,1,1,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [1,1,1,1,1,1,1]
        ],//1
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,1,1],
            [1,1,1,1,1,1,1]
        ],//2
        [
            [1,1,1,1,1,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//3
        [
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,0],
            [0,0,1,1,1,1,0],
            [0,1,1,0,1,1,0],
            [1,1,0,0,1,1,0],
            [1,1,1,1,1,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,1,1]
        ],//4
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,1,1,1,1,0],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//5
        [
            [0,0,0,0,1,1,0],
            [0,0,1,1,0,0,0],
            [0,1,1,0,0,0,0],
            [1,1,0,0,0,0,0],
            [1,1,0,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//6
        [
            [1,1,1,1,1,1,1],
            [1,1,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,0,0,1,1,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0],
            [0,0,1,1,0,0,0]
        ],//7
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,1,1,0]
        ],//8
        [
            [0,1,1,1,1,1,0],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [1,1,0,0,0,1,1],
            [0,1,1,1,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,0,1,1],
            [0,0,0,0,1,1,0],
            [0,0,0,1,1,0,0],
            [0,1,1,0,0,0,0]
        ],//9
        [
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0],
            [0,0,0,0,0,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,1,1,1,0,0],
            [0,0,0,0,0,0,0]
        ]//:
    ];

var canvas = document.getElementById('canvas');

if(canvas.getContext){
    var cxt = canvas.getContext('2d');
    //声明canvas的宽高
    var H = 100,W = 700;
    canvas.height = H;
    canvas.width = W;
    cxt.fillStyle = '#f00';
    cxt.fillRect(10,10,50,50);

    //存储时间数据
    var data = [];
    //存储运动的小球
    var balls = [];
    //设置粒子半径
    var R = canvas.height/20-1;
    (function(){
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        //存储时间数字，由十位小时、个位小时、冒号、十位分钟、个位分钟、冒号、十位秒钟、个位秒钟这7个数字组成
        data.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
    })();

    /*生成点阵数字*/
    function renderDigit(index,num){
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    cxt.beginPath();
                    cxt.arc(14*(R+2)*index + j*2*(R+1)+(R+1),i*2*(R+1)+(R+1),R,0,2*Math.PI);
                    cxt.closePath();
                    cxt.fill();
                }
            }
        }
    }

    /*更新时钟*/
    function updateDigitTime(){
        var changeNumArray = [];
        var temp = /(\d)(\d):(\d)(\d):(\d)(\d)/.exec(new Date());
        var NewData = [];
        NewData.push(temp[1],temp[2],10,temp[3],temp[4],10,temp[5],temp[6]);
        for(var i = data.length-1; i >=0 ; i--){
            //时间发生变化
            if(NewData[i] !== data[i]){
                //将变化的数字值和在data数组中的索引存储在changeNumArray数组中
                changeNumArray.push(i+'_'+(Number(data[i])+1)%10);
            }
        }
        //增加小球
        for(var i = 0; i< changeNumArray.length; i++){
            addBalls.apply(this,changeNumArray[i].split('_'));
        }
        data = NewData.concat();
    }

    /*更新小球状态*/
    function updateBalls(){
        for(var i = 0; i < balls.length; i++){
            balls[i].stepY += balls[i].disY;
            balls[i].x += balls[i].stepX;
            balls[i].y += balls[i].stepY;
            if(balls[i].x > W + R || balls[i].y > H + R){
                balls.splice(i,1);
                i--;
            }
        }
    }

    /*增加要运动的小球*/
    function addBalls(index,num){
        var numArray = [1,2,3];
        var colorArray =  ["#3BE","#09C","#A6C","#93C","#9C0","#690","#FB3","#F80","#F44","#C00"];
        for(var i = 0; i < digit[num].length; i++){
            for(var j = 0; j < digit[num][i].length; j++){
                if(digit[num][i][j] == 1){
                    var ball = {
                        x:14*(R+2)*index + j*2*(R+1)+(R+1),
                        y:i*2*(R+1)+(R+1),
                        stepX:Math.floor(Math.random() * 4 -2),
                        stepY:-2*numArray[Math.floor(Math.random()*numArray.length)],
                        color:colorArray[Math.floor(Math.random()*colorArray.length)],
                        disY:1
                    };
                    balls.push(ball);
                }
            }
        }
    }

    /*渲染*/
    function render(){
        //重置画布宽度，达到清空画布的效果
        canvas.height = 100;
        //渲染时钟
        for(var i = 0; i < data.length; i++){
            renderDigit(i,data[i]);
        }
        //渲染小球
        for(var i = 0; i < balls.length; i++){
            cxt.beginPath();
            cxt.arc(balls[i].x,balls[i].y,R,0,2*Math.PI);
            cxt.fillStyle = balls[i].color;
            cxt.closePath();
            cxt.fill();
        }
    }

    clearInterval(oTimer);
    var oTimer = setInterval(function(){
        //更新时钟
        updateDigitTime();
        //更新小球状态
        updateBalls();
        //渲染
        render();
    },50);
}

})();
</script>
      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Apers</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">122k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:51</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener external nofollow noreferrer" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/valine@1/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'mLlhk74loQanT3wsKkyBMxve-gzGzoHsz',
      appKey     : 'gie2FNStD4ro7YKzTQYJag4u',
      placeholder: "留下你的痕迹吧!",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : 'zh-cn' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>



<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div class="moon-menu-item" onclick="back2bottom()">
      <i class='fa fa-chevron-down'></i>    </div>
    
    <div class="moon-menu-item" onclick="back2top()">
      <i class='fa fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
